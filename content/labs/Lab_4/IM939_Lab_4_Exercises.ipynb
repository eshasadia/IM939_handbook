{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Wine dataset\n",
    "\n",
    "As with previous exercises, fill in the question marks with the correct code.\n",
    "\n",
    "Last week you were introduced to the [wine dataset](https://archive.ics.uci.edu/ml/datasets/wine+quality). We have 10 input variables and 1 output variables.\n",
    "\n",
    "Input variables (based on physicochemical tests):\n",
    "\n",
    "1. fixed acidity\n",
    "2. volatile acidity\n",
    "3. citric acid\n",
    "4. residual sugar\n",
    "5. chlorides\n",
    "6. free sulfur dioxide\n",
    "7. total sulfur dioxide\n",
    "8. density\n",
    "9. pH\n",
    "10. sulphates\n",
    "11. alcohol\n",
    "\n",
    "Output variable (based on sensory data):\n",
    "\n",
    "12. quality (score between 0 and 10)\n",
    "\n",
    "I suggest we look at two broad questions with this dataset:\n",
    "\n",
    "1. Will dimension reduction reveal variable groupings? Think back to how we interpreted the loadings in the crime dataset.\n",
    "2. What does clustering the wines well us?\n",
    "\n",
    "## Load data and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sn?\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PC?\n",
    "from sklearn.decomposition import S????ePCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "df = pd.read_excel('data/winequality-red_v2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "df.h??d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "# May take a while depending on your computer\n",
    "# feel free not to run this\n",
    "sns.pair????(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation\n",
    "\n",
    "Before you carry out any operation, you might want to perform some normalisation. This will ensure that some of the assumptions that the algorithms are making are met and also the results are not biased/determined by the different value ranges and variation ranges inherent in the data. \n",
    "\n",
    "Do try out the following steps **without** normalisation first and then come back to this, normalise the data and see the differences it makes using a **normalised** copy of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# first save the column names, we will create a new dataset with the scaled data\n",
    "col_names = df.columns\n",
    "\n",
    "# This is the normalization function. \n",
    "# We are using the MinMaxScaler which brings all the data between 0 and 1.\n",
    "# Make use of other transformations offered by scikitlearn, experiment, note changes. \n",
    "\n",
    "# The last column of the data contains the \"quality\" labels/scores, we don't want to normalize them \n",
    "# as they is sort of the \"dependent (or \"target\") variable and there is meaning in these scores. \n",
    "# Let's normalize the first 11 columns which are our \"independent\" columns.\n",
    "scaled_df =  pd.DataFrame(MinMaxScaler().fit_transform(df.iloc[:, 0:11]))\n",
    "\n",
    "# now we want to add the \"quality\" values back in. We'll need them.\n",
    "scaled_df = scaled_df.join(df.iloc[:, 11:12])\n",
    "\n",
    "# now we name the columns with the original column names. We do this because MinMaxScaler \n",
    "# produces a data frame with no column names (don't ask me why..)\n",
    "scaled_df.columns = col_names\n",
    "\n",
    "# let's have a look at what the data is looking like:\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note:** The rest of the code will continue to use the **non-normalised** version of the data. For now, just carry on with that and try running the operations with the non-normalised version. Once you are through and/or somewhere in the middle, try them out with the **normalised** data. See what this will change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 2\n",
    " \n",
    "pca = PCA(n_??????????=n_components)\n",
    "df_pca = pca.fit(df?iloc[:, 0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "df_pca_vals = df_pca.???_transform(df.iloc[:, 0:11])\n",
    "df['c1'] = [item[0] for item in df_pca_????]\n",
    "df['c2'] = [item[1] for item in df_pca_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "sns.scatterplot(data = df, x = ?, y = ?, hue = 'quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "print(df.columns)\n",
    "df_pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about other dimension reduction methods?\n",
    "\n",
    "## SparcePCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "s_pca = SparsePCA(n_components=n_components)\n",
    "df_s_pca = s_pca.fit(df.????[:, 0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "df_s_pca_vals = s_pca.fit_?????????(df.iloc[:, 0:11])\n",
    "df['c1 spca'] = [item[0] for item in df_s_pca_vals]\n",
    "df['c2 spca'] = [item[1] for item in df_s_pca_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "sns.scatterplot(data = df, x = 'c1 spca', y = 'c2 spca', hue = 'quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "tsne_model = TSNE(n_components=n_components)\n",
    "df_tsne = tsne_model.fit(df.iloc[:, 0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "df_tsne_vals = tsne_model.fit_transform(df.iloc[:, 0:11])\n",
    "df['c1 tsne'] = [item[0] for item in ??_tsne_vals]\n",
    "df['c2 tsne'] = [item[1] for item in df_tsne_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "# This plot does not look right\n",
    "# I am not sure why.\n",
    "sns.scatterplot(data = ??, x = 'c1 tsne', y = 'c1 tsne', hue = 'quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks concerning - there is a straight line. It looks like something in the above code might not be correct.\n",
    "\n",
    "Can you find out what that might be? \n",
    "\n",
    "**Hint:** think about when you would get a straight line in a scatterplot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you fixed the error above, you will notice a different structure to the ones you observed in the PCA runs. There isn't really a clear next step which of these projections one should adopt. \n",
    "\n",
    "For now, we will use PCA components. PCA would be a good choice if the interpretability of the components is important to us. Since PCA is a linear projection method, the components carry the weights of each raw feature which enable us to make inferences about the axes. However, if we are more interested in finding structures and identify groups of similar items, t-SNE might be a better projection to use since it emphasises proximity but the axes don't mean much since the layout is formed stochastically (fancy speak for saying that there is randomness in the algorithm and the layout will be different each time your run it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "data = {'columns' : df.iloc[:, 0:11].columns,\n",
    "        'component 1' : df_pca.components_[0],\n",
    "        'component 2' : df_pca.components_[1]}\n",
    "\n",
    "\n",
    "loadings = pd.?????????(data)\n",
    "loadings_sorted = loadings.sort_values(by=['component 1'], ascending=False)\n",
    "loadings_sorted.iloc[1:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "loadings_sorted = loadings.sort_values(by=['component 2'], ascending=False)\n",
    "loadings_sorted.iloc[1:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    ????? = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(df[['c1', 'c2']])\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(ks, inertias, '-o', color='black')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "k_means_3 = KMeans(n_clusters = 3, init = 'random')\n",
    "k_means_3.fit(df[['c1', 'c2']])\n",
    "df['Three clusters'] = pd.Series(k_means_3.???????(df[['c1', 'c2']].values), index = df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "sns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Three clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider:\n",
    "\n",
    "* Is that useful? \n",
    "* What might it mean?\n",
    "\n",
    "Outside of this session go back to normalising the data and try out different methods for normalisation as well (e.g., centering around the mean), clustering the raw data (and not the projections from PCA), trying to get tSNE working or using different numbers of components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
