{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Working with multiple models\n",
    "\n",
    "In this notebook, we will look at the London Borough data that we already encountered when we worked with the London Borough profiles and the Borough Cards that we utilised in the session.\n",
    "\n",
    "You can have a look at the data through this online tool provided by GLA: <https://data.london.gov.uk/london-area-profiles/>\n",
    "\n",
    "However, unlike the data that we explored during the session, this data set has several features (i.e., columns or variables), 76 of them to be precise.\n",
    "\n",
    "What we would like to do in this notebook is to make use of a dimension reduction algorithm -- Multidimensional Scaling -- to help us create various different \"spaces\". Each of these space will be a different way of \"seeing this data\" and if we adopt the language from Scott Page [-@pageWhyManyModelThinkers2018], they will have different \"attentions\". \n",
    "\n",
    "What the following exercise will do is to walk you through the variables of this data set through a few visualisations. It will then create a few different projections and will give them some names. What we expect you to do is to create your own projections and try to interpret them. \n",
    "\n",
    "If you want to be reminded of what MDS is, you can have a look at the slides from last week. In a super tiny nutshell, MDS tries to create a space where \"real\" distances in the data are preserved as much as possible while \"projecting\" the data elements on a lower dimensional space. For instance, the following is from the slide deck:\n",
    "\n",
    "![An MDS plot of cities.](figs/mds.png){width=800}\n",
    "\n",
    "What we see here are a few cities that would normally \"exist\" in our 3-dimensional world and the distances between them would normally be on this spherical coordinate system. But we create here is a 2D map and the distances between cities are preserved as much as possible. Near cities in the world are closer, and the further ones are further but not as accurate as it is in the world. The dimensions here carry no real meaning it is the distances that will \"tell a story\" (if there is one).\n",
    "\n",
    "OK, let's get on with the data now.\n",
    "\n",
    "## Data exploration and wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('data/london-borough-profilesV3.xlsx', engine = 'openpyxl')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of different features. We also have really odd `NaN` values such as `x` and `not avail`. We can try and get rid of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnumber(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except:\n",
    "        if (len(x) > 1) & (\"not avail\" not in x):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "# apply isnumber function to every element\n",
    "df = df[df.applymap(isnumber)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks much cleaner. The missing values are all `NaN` now. This will help us fill them in and/or address them in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only numeric columns\n",
    "numericColumns = df._get_numeric_data()\n",
    "numericColumns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above piece of code is throwing a lot of features out for not being a numeric column. The resulting frame has 41 features, however, we would expect more.\n",
    "# upon a bit of debugging, we found out that --  df._get_numeric_data() -- a Pandas function -- has a minor bug in inferring which columns are numeric when the first value of a column is a missing value, i.e., 'NaN' value. \n",
    "# This meant that some columns that are numeric were removed from the dataset even though they are numeric. \n",
    "\n",
    "\n",
    "# this next piece of code is addressing that now and also fills in the missing values with the mean() value of a column\n",
    "\n",
    "for column_name, column in df.items():\n",
    "    try:\n",
    "        df[column_name] = df[column_name].fillna(df[column_name].mean())\n",
    "    except:\n",
    "        print(\"Column:\", column_name, \" is not a numeric column\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only numeric columns\n",
    "numericColumns = df._get_numeric_data()\n",
    "numericColumns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 69 columns. Looks much better than 41, we can move on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: callout-warning\n",
    "\n",
    "Reflect here whether the `mean()` replacements for missing values taht we did above is sensible and/or would work for each and every column here. You might need more sophisticated methods for filling in some of the missing values, e.g., using a model-based approach to 'predict' a value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "# keep place names and store them in a variable\n",
    "placeNames = df[\"Area/INDICATOR\"]\n",
    "\n",
    "# if we hadn't done it, we could have filled in the missing values also here.\n",
    "# numericColumns = numericColumns.fillna(numericColumns.mean())\n",
    "\n",
    "# let's centralize the data\n",
    "numericColumns -= numericColumns.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure everything looks ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericColumns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot out our many dimension space by uncommenting the code below (also note down how long does this take)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#sns_plot = sns.pairplot(numericColumns)\n",
    "#sns_plot.savefig(\"figs/output.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this takes quite a while (around 10 minutes), this is the image that would result from uncommenting and running the code above.\n",
    "\n",
    "::: {.column-page}\n",
    "![](figs/output.png)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension reduction will help us here!\n",
    "\n",
    "## Multidimensional scaling\n",
    "\n",
    "We could apply various different types of dimension reduction here. We are specifically going to capture the dissimilarity in the data using [multidimensional scaling](https://scikit-learn.org/stable/modules/manifold.html#multidimensional-scaling). We will need a distance matrix to start here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "# Here, we compute the euclidean distances between the columns by passing the same data twice\n",
    "# the resulting data matrix will now have the pairwise distances between the boroughs.\n",
    "# CAUTION: note that we are now building a distance matrix in a high-dimensional data space\n",
    "# remember the Curse of Dimensionality -- we need to be cautious with the distance values\n",
    "distMatrix = euclidean_distances(numericColumns, numericColumns)\n",
    "\n",
    "# for instance, typing distMatrix.shape on the console gives:\n",
    "# Out[115]: (38, 38) # i.e., the number of rows\n",
    "\n",
    "# first we generate an MDS object and extract the projections\n",
    "mds = manifold.MDS(n_components = 2, max_iter=3000, n_init=1, dissimilarity=\"precomputed\", normalized_stress=False)\n",
    "Y = mds.fit_transform(distMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret what is happening, let us plot the boroughs on the projected two dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: page\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 15)\n",
    "plt.suptitle('MDS on only London boroughs')\n",
    "ax.scatter(Y[:, 0], Y[:, 1], c=\"#D06B36\", s = 100, alpha = 0.8, linewidth=0)\n",
    "\n",
    "for i, txt in enumerate(placeNames):\n",
    "    ax.annotate(txt, (Y[:, 0][i],Y[:, 1][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are projecting all the numeric variables, so it is difficult to get a sense of what these components represent and how to interpret the visualisation. We may want to project only a subset of features. \n",
    "\n",
    "::: callout-important\n",
    "\n",
    "### Feature selection\n",
    "\n",
    "Feature selection is not straightforward and it is always a decision made by us informed by something (i.e. measures, literature --refer to the slides from last week). Below we will be _embedding_ (machine learners' word for projection) two different sets of features for you to compare.\n",
    "\n",
    ":::\n",
    "\n",
    "### Feature selection: Happiness\n",
    "\n",
    "In the example below, we are selecting happiness metrics. Pulling these out of our data and carrying out more multidimensional scaling can help us see how the boroughs differ in happiness.\n",
    "\n",
    "::: callout-note\n",
    "\n",
    "\n",
    "\n",
    "The decision of selecting the features `Life satisfaction score 2012-13 (out of 10)`, `Worthwhileness score 2012-13 (out of 10)` and `Happiness score 2012-13 (out of 10)` is not based on any numerical decision, but it is based on the semantics of the variables. In other words, these three variables provide different perspectives to describe how happy people are in the different neighbourhoods.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: page\n",
    "\n",
    "# get the data columns relating to emotions and feelings\n",
    "dataOnEmotions = numericColumns[[\"Life satisfaction score 2012-13 (out of 10)\", \"Worthwhileness score 2012-13 (out of 10)\",\"Happiness score 2012-13 (out of 10)\"]]\n",
    "\n",
    "# a new distance matrix to represent \"emotional distance\"s\n",
    "distMatrix2 = euclidean_distances(dataOnEmotions, dataOnEmotions)\n",
    "\n",
    "# compute a new \"embedding\" (machine learners' word for projection)\n",
    "Y2 = mds.fit_transform(distMatrix2)\n",
    "\n",
    "# let's look at the results\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 15)\n",
    "plt.suptitle('An \\\"emotional\\\" look at London boroughs')\n",
    "ax.scatter(Y2[:, 0], Y2[:, 1], c=\"#D06B36\", s = 100, alpha = 0.8, linewidth=0)\n",
    "\n",
    "for i, txt in enumerate(placeNames):\n",
    "    ax.annotate(txt, (Y2[:, 0][i],Y2[:, 1][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location of the different boroughs on the 2 dimensional multidimensional scaling space from the happiness metrics is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fixed = Y2.copy()\n",
    "print(results_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_fixed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to look at if the general happiness rating captures the position of the boroughs. To do this, we need to assign colours based on the binned happiness score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "colorMappingValuesHappiness = np.asarray(dataOnEmotions[[\"Life satisfaction score 2012-13 (out of 10)\"]]).flatten()\n",
    "colorMappingValuesHappiness.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "colorMappingValuesHappiness\n",
    "#c = colorMappingValuesCrime, cmap = plt.cm.Greens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot this. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: page\n",
    "\n",
    "# let's look at the results\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 15)\n",
    "plt.suptitle('An \\\"emotional\\\" look at London boroughs')\n",
    "#ax.scatter(results_fixed[:, 0], results_fixed[:, 1], c = colorMappingValuesHappiness, cmap='viridis')\n",
    "plt.scatter(results_fixed[:, 0], results_fixed[:, 1], c = colorMappingValuesHappiness, s = 100, cmap=plt.cm.Greens)\n",
    "\n",
    "for i, txt in enumerate(placeNames):\n",
    "    ax.annotate(txt, (results_fixed[:, 0][i],results_fixed[:, 1][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection: diversity\n",
    "\n",
    "Similarly, we are now selecting features based on diveristy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: page\n",
    "\n",
    "# get the data columns relating to indicators that we think are related to \"diversity\" in one way or the other\n",
    "dataOnDiversity = numericColumns[[\"Proportion of population aged 0-15, 2013\", \"Proportion of population of working-age, 2013\", \"Proportion of population aged 65 and over, 2013\", \"% of population from BAME groups (2013)\", \"% people aged 3+ whose main language is not English (2011 census)\"]]\n",
    "\n",
    "# a new distance matrix to represent distances in \"diversity\"\n",
    "distMatrix3 = euclidean_distances(dataOnDiversity, dataOnDiversity)\n",
    "\n",
    "mds = manifold.MDS(n_components = 2, max_iter=3000, n_init=1, dissimilarity=\"precomputed\", normalized_stress = False)\n",
    "Y = mds.fit_transform(distMatrix3)\n",
    "\n",
    "# Visualising the data.\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 15)\n",
    "plt.suptitle('A \\\"diversity\\\" look at London boroughs')\n",
    "ax.scatter(Y[:, 0], Y[:, 1], s = 100, c = colorMappingValuesHappiness, cmap=plt.cm.Greens)\n",
    "\n",
    "for i, txt in enumerate(placeNames):\n",
    "    ax.annotate(txt, (Y[:, 0][i],Y[:, 1][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## It is now your turn!\n",
    "\n",
    "\n",
    "::: callout-caution\n",
    "\n",
    "### It is your turn now\n",
    "\n",
    "**First task:**\n",
    "\n",
    "This looks very different to the one above on \"emotion\" related variables. Our job now is to relate these two projections to one another. Do you see similarities? Do you see clusters of boroughs? Can you reflect on how you can relate and combine these two maps conceptually?\n",
    "\n",
    "**Second task:**\n",
    "\n",
    "Can you think of and then generate other maps that you can produce with this data? Have a look at the variables once again and try to produce new \"perspectives\" to the data and see what they have to say. \n",
    "\n",
    "Also think of visualisations to help you here, can you colour them with a different variable? What would that change?\n",
    "\n",
    ":::\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
