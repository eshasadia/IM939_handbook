[
  {
    "objectID": "content/about/teaching_staff.html",
    "href": "content/about/teaching_staff.html",
    "title": "Teaching Staff",
    "section": "",
    "text": "Cagatay Turkay\nProfessor, Module Convener\nMy research falls under the broad area that can be referred to as Visual Data Science and focuses on designing visualisations, interactions and computational methods to enable an effective combination of human and machine capabilities to facilitate data-intensive problem solving.\nI have a special interest in working on problems where high-dimensional, spatio-temporal, heterogenous and large datasets are used in answering questions with data.",
    "crumbs": [
      "About",
      "Teaching Staff"
    ]
  },
  {
    "objectID": "content/about/teaching_staff.html#sec-staff-all",
    "href": "content/about/teaching_staff.html#sec-staff-all",
    "title": "Teaching Staff",
    "section": "Present and former staff",
    "text": "Present and former staff\nThis module has also been taught by the following people in the past (in alphabetical order):\n\nBusola Oronti, Teaching Assistant (2022-Present)\nCagatay Turkay, Professor (202X-Present)\nCarlos Cámara-Menoyo, Senior Research Software Engineer (2022-Present)\nJames Tripp, Senior Research Software Engineer (2020-2022)\nKavin Narasimhan, Assistant Professor (2023-Present)\nMaria Petrescu, Teaching Assistant (XX-XXX)\nYulu Pi, Teaching Assistant (2022-2023)\nZofia Bednarowska-Michaiel, Teaching Fellow (2021-2022)\n\nWe would like to thank them all for their contributions to the module, some of which are also reflected in these materials.",
    "crumbs": [
      "About",
      "Teaching Staff"
    ]
  },
  {
    "objectID": "content/about/im939.html",
    "href": "content/about/im939.html",
    "title": "About the module",
    "section": "",
    "text": "What this module is about?\nThis module introduces students to the fundamental techniques, concepts and contemporary discussions across the broad field of data science. With data and data related artefacts becoming ubiquitous in all aspects of social life, data science gains access to new sources of data, is taken up across an expanding range of research fields and disciplines, and increasingly engages with societal challenges. The module provides an advanced introduction to the theoretical and scientific frameworks of data science, and to the fundamental techniques for working with data using appropriate procedures, algorithms and visualisation. Students learn how to critically approach data and data-driven artefacts, and engage with and critically reflect on contemporary discussions around the practice of data science, its compatibility with different analytics frameworks and disciplinary, and its relation to on-going digital transformations of society. As well as lectures discussing the theoretical, scientific and ethical frameworks of data science, the module features coding labs and workshops that expose students to the practice of working effectively with data, algorithms, and analytical techniques, as well as providing a platform for reflective and critical discussions on data science practices, resulting data artefacts and how they can be interpreted, actioned and influence society.",
    "crumbs": [
      "About",
      "About the module"
    ]
  },
  {
    "objectID": "content/about/im939.html#what-does-this-module-aim-to-achieve",
    "href": "content/about/im939.html#what-does-this-module-aim-to-achieve",
    "title": "About the module",
    "section": "What does this module aim to achieve?",
    "text": "What does this module aim to achieve?\nIn this module, students gain both formal knowledge and practical experience of the theoretical, scientific and ethical frameworks underpinning data science and critically reflect on the scope and impact of these frameworks. Lectures will provide a grounded understanding of the theoretical and scientific frameworks underpinning data science. In workshops, students gain experience of the fundamentals of the practice of data science, and through seminars they will be exposed to academic debates in data studies and related fields about the changing role of data science in society as seen in, for instance, the increasing use of data artefacts in policy and decision making in governmental bodies and businesses, how scientific discoveries are made and communicated, or how (in)equalities and power (im)balances are surfacing in uses of data. The module aims to build the required skills to apply data science techniques and algorithms within and across analytics frameworks developed in different disciplines. The module aims to cultivate a holistic data science practice which reviews the whole data science process critically and inquisitively, and handles problems through a user-centred thinking. This practice also embraces critical reflection about the data, algorithms, and data artefacts, as well as the ethical, societal, and cultural implications of data science broadly conceived.",
    "crumbs": [
      "About",
      "About the module"
    ]
  },
  {
    "objectID": "content/about/im939.html#learning-outcomes",
    "href": "content/about/im939.html#learning-outcomes",
    "title": "About the module",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nDemonstrate an in-depth understanding of the theoretical underpinnings, scientific and ethical frameworks of data science as applied across disciplines\nDemonstrate a critical understanding of the role that data and data intensive practices play in research, industry and the wider society\nDemonstrate an understanding of the workings and the practicalities of the data science process\nApply and evaluate data science techniques and tools for particular scenarios and argue their suitability\nDemonstrate an ability to critique any resulting data artefacts, such as data-informed decisions to data-driven models, including from a user-centred perspective\nDevelop and demonstrate an understanding of the societal, ethical, and cultural implications of advances in and applications of data science",
    "crumbs": [
      "About",
      "About the module"
    ]
  },
  {
    "objectID": "content/about/im939.html#teaching-timetable",
    "href": "content/about/im939.html#teaching-timetable",
    "title": "About the module",
    "section": "Teaching timetable",
    "text": "Teaching timetable\n\nQ & A sessions based on lecture content, weeks 1-5 & 7-8\n\nWhole group Q & A at LIB2: Wednesdays 10am – 11am\n\nLab sessions Weeks 1-5 & 7-10\n\nLab group 1: Fridays 10am-12pm, in-person, at JX2.02\nLab Group 2: Fridays 2pm-4pm, in-person, at FAB5.03\n\n\nFLIPPED CLASSROOM: IM939 broadly follows a flipped classroom model. This means that the in-person classes will be mostly used for discussions, Q&As, hands-on activities and for “doing” data science. There will be less time dedicated to traditional lectures during the in-person sessions. Instead, pre-recorded lecture videos will be provided as learning material.\nLecture materials such as slides and reading lists, and pre-recorded lecture videos that introduce the week’s content will be made available each week. You are expected to go through these material before the in-person sessions. We will use the Q&A sessions to go over any open questions and to reflect collectively on the week’s material. In most weeks, we will start with a brief recap of what is discussed in the pre-recorded lecture materials.",
    "crumbs": [
      "About",
      "About the module"
    ]
  },
  {
    "objectID": "content/about/im939.html#assessment",
    "href": "content/about/im939.html#assessment",
    "title": "About the module",
    "section": "Assessment",
    "text": "Assessment\nThe assessments will be individual based and will involve two components: a critical review and a data-driven essay. The critical review will involve students approaching a selected Data Science project through a critical lens covered during the lectures. The short report will expect students to engage with the related literature and reflect on the decisions made by the researchers of the project. Within the second component, the data-driven essay, students will report on a data science project that they carried on a chosen question and appropriate data set. The essay will be reporting on the data science process from initiation to evaluation to reflection while engaging with the relevant literature in the domain. These essays vary in length, depending on the number of CATS a student wishes to complete.\n\n\n\n\n\n\n\n\n\n\n15-CATS\n20 CATS\n30 CATS\n\n\n\n\nCritical Review\n(1000 words) – 40%\n(1250 words) – 40%\n(1500 words) – 40%\n\n\nFinal Essay\n(1500 words) – 60%\n(2000 words) – 60%\n(3000 words) – 60%",
    "crumbs": [
      "About",
      "About the module"
    ]
  },
  {
    "objectID": "content/about/im939.html#illustrative-bibliography",
    "href": "content/about/im939.html#illustrative-bibliography",
    "title": "About the module",
    "section": "Illustrative Bibliography",
    "text": "Illustrative Bibliography\n\nData science as a scientific practice : Dhar, Vasant. “Data science and prediction.” Communications of the ACM, 56.12 (2013): 64-73.\nIliadis, A. and Russo, F., 2016. Critical data studies: An introduction. Big Data & Society, 3(2), p.2053951716674238.\nGinsberg, Jeremy, et al. “Detecting influenza epidemics using search engine query data.” Nature (2008)\nKandel, Sean, et al. “Enterprise data analysis and visualization: An interview study.” Visualization and Computer Graphics, IEEE Transactions on 18.12 (2012): 2917-2926.\nOsborne, Jason. “Notes on the use of data transformations.” Practical Assessment, Research & Evaluation 8.6 (2002): 1-8.\nOsborne, Jason W., and Amy Overbay. “The power of outliers (and why researchers should always check for them).” Practical assessment, research & evaluation 9.6 (2004): 1- 12.\nGuyon, Isabelle, and André Elisseeff. “An introduction to variable and feature selection.” The Journal of Machine Learning Research 3 (2003): 1157-1182.\nRingnér, Markus (2008). “What is principal component analysis?”. Nature biotechnology (1087-0156), 26 (3), p. 303.\nJaworska, Natalia, and Angelina Chupetlovska-Anastasova. “A review of multidimensional scaling (MDS) and its utility in various psychological domains.” Tutorials in Quantitative Methods for Psychology 5.1 (2009): 1-10.\nKohavi, A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, Intl. Jnt. Conf. AI\nWhite, Douglas R., and Stephen P. Borgatti. “Betweenness centrality measures for directed graphs.” Social Networks 16.4 (1994): 335-346.\nHeer, Jeffrey, and Ben Shneiderman. “Interactive dynamics for visual analysis.” Queue 10.2 (2012): 30.\nRuckenstein, M. and Schüll, N.D., 2017. The datafication of health. Annual Review of Anthropology, 46, pp.261-278.\nPink, S., Ruckenstein, M., Willim, R. and Duque, M., 2018. Broken data: Conceptualising data in an emerging world. Big Data & Society, 5(1), p.2053951717753228.",
    "crumbs": [
      "About",
      "About the module"
    ]
  },
  {
    "objectID": "content/sessions/session-01.html",
    "href": "content/sessions/session-01.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Highlights of the lecture\nThis week we start by an introduction where we look at how the module is operating and discussing the basic objectives and definitions of the module.\nSome of the key concepts you should remember from this week are …",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-01.html#highlights-of-the-lecture",
    "href": "content/sessions/session-01.html#highlights-of-the-lecture",
    "title": "1  Introduction",
    "section": "",
    "text": "the discussion about the terms Data Science and Data Scientists\nthe DS process and basic concepts from each step of the process\nImportance of being critical and inquisitive in data science\ndifferent analyst types and skills",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-01.html#practical-lab-session",
    "href": "content/sessions/session-01.html#practical-lab-session",
    "title": "1  Introduction",
    "section": "1.2 Practical Lab Session",
    "text": "1.2 Practical Lab Session\nThis week is mainly a setup week where you get introduced to the coding environment and to Python.\nAt the end of the session, you should ..\n\nhave installed Anaconda and run it from your account\nhave tried out basic Python commands and reflect on how they operate\nhave loaded your first data file into Python and read the data in it",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-01.html#reading-lists-resources",
    "href": "content/sessions/session-01.html#reading-lists-resources",
    "title": "1  Introduction",
    "section": "1.3 Reading lists & Resources",
    "text": "1.3 Reading lists & Resources\n\n1.3.1 Required reading\n\nOn the origins of Data Science and Data Analysis (first 10 pages): Tukey, J.W., 1962. The future of data analysis. The annals of mathematical statistics, 33(1), pp.1-67. [pdf]\nA formal look at data science: Dhar, V., 2013. Data science and prediction. Communications of the ACM, 56(12), pp.64-73. [library pdf link]\nA systematic study of enterprise analysts, their daily tasks, and challenges: Kandel, Sean, et al. “Enterprise data analysis and visualization: An interview study.” Visualization and Computer Graphics, IEEE Transactions on 18.12 (2012): 2917-2926.\nOn Google’s influenza epidemic application: Ginsberg, Jeremy, et al. “Detecting influenza epidemics using search engine query data.” Nature (2008) (need to search this through our library)\nOn the critique of the Google Flu Trend project: Lazer, D., Kennedy, R., King, G. and Vespignani, A., 2014. The parable of Google Flu: traps in big data analysis. Science, 343(6176), pp.1203-1205. [pdf]\nAn applied Data Science example: Quercia, D., Schifanella, R. and Aiello, L.M., 2014, September. The shortest path to happiness: Recommending beautiful, quiet, and happy routes in the city. In Proceedings of the 25th ACM conference on Hypertext and social media (pp. 116-125). [pdf]\n\n\n\n1.3.2 Optional reading and resources\n\nOn the information pyramid: Ackoff, R.L., 1989. From data to wisdom. Journal of applied systems analysis, 16(1), pp.3-9. [a pdf link to a short extract]\nA critique of the information pyramid: https://hbr.org/2010/02/data-is-to-info-as-info-is-not\nThe survey on analyst types and skills : Analyzing the Analyzers By Harlan Harris, Sean Murphy, Marck Vaisman\nA public facing intro to Data Science : Data Science: A guide for society by Sense about Science - [pdf link]\nOn data biography: D’Ignazio, C., 2017. Creative data literacy: Bridging the gap between the data-haves and data-have nots. Information Design Journal, 23(1), pp.6-18. [pdf]",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-01.html#python-related-resources",
    "href": "content/sessions/session-01.html#python-related-resources",
    "title": "1  Introduction",
    "section": "1.4 Python related resources",
    "text": "1.4 Python related resources\nAlthough we try to cover the basics in Python programming in this tutorial, some of you, especially those who are new to Python, might benefit from some external tutorials which cover the basics. There are many resources online but here are some good links:\n\ncodecademy has a free tutorial: http://www.codecademy.com/en/tracks/python\nWikibooks for Python: http://en.wikibooks.org/wiki/Non-Programmer%27s_Tutorial_for_Python_3\nLearnpython even has an interactive console on the web: http://www.learnpython.org/\n\nAnd here are some books that can you with your learning:\n\nData science from scratch : first principles with Python / Joel Grus – http://encore.lib.warwick.ac.uk/iii/encore/record/C__Rb3426067\n\nEspecially Chapter 2: A Crash course in Python to help with the preparations – https://ebookcentral.proquest.com/lib/warw/reader.action?docID=5750897&ppg=33\n\nData science for business : what you need to know about data mining and data-analytic thinking / Foster Provost & Tom Fawcett – http://encore.lib.warwick.ac.uk/iii/encore/record/C__Rb3141595\nHunt, J., 2019. A Beginners Guide to Python 3 Programming. Springer. http://encore.lib.warwick.ac.uk/iii/encore/record/C__Rb3404588\nPython for Data Analysis : Data Wrangling with Pandas, NumPy, and IPython",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_1.html",
    "href": "content/labs/Lab_1/IM939_Lab_1_1.html",
    "title": "2  Lab: Introduction to Python",
    "section": "",
    "text": "2.1 Technological stack\nThis module uses a combination of different technologies (technological stack) besides python. You will need to be familiar with all their components to follow the labs as well as the assignments. You can find a brief description of the main components below, and you can refer to Appendix A for a more technical explanation of the setup.",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Introduction to Python</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_1.html#sec-technological-stack",
    "href": "content/labs/Lab_1/IM939_Lab_1_1.html#sec-technological-stack",
    "title": "2  Lab: Introduction to Python",
    "section": "",
    "text": "Important\n\n\n\nPlease make sure you have Anaconda Python installed and running on your computer. Instructions for doing this can be found on Appendix A.\n\n\n\n2.1.1 Python\nPython is a very popular general purpose programming language. Data scientists use it to clean up, analyse and visualise data. A major strength of python is that the core Python functionality can be extended using libraries. In future labs, you will learn about popular data science libraries such as pandas and numpy.\nIt is useful to think of programming languages as a structured way to tell the computer what to do. We cover some of the basic features of Python in this lab.\n\n\n2.1.2 Anaconda\nAnaconda1 is a distribution platform that manages and installs many tools used in data science, such as programming languages (python), libraries, IDEs2… as well as a GUI (Anaconda navigator) and a CLI (Anaconda interpreter). Anaconda does some other useful things, such as creating isolated virtual environments, which we will be using for this module.\n1 If you want to know more about Anaconda, these tutorials can be a good start: https://www.tangolearn.com/what-is-anaconda/, https://www.upgrad.com/blog/python-anaconda-tutorial/2 Some specific Integrated Development Environments (IDEs) for Python included in Anaconda are VS Code, Jupyterlab and Spyder. In this module, there’s no preferred IDE (actually, different members of the staff use different IDEs) and you can use the one you are more familiar with.\n\n\n\n\n\nVirtual environments\n\n\n\nVirtual environments are a way to install all the dependencies (and their right version) required for a certain project by isolating python and libraries’ specific versions. Every person who recreatesthe virtual environment will be using the same packages and versions, reducing errors and increasing reproducibility. While they are considered an advanced practice, and are therefore out of scope of this course, you may want to learn about Python’s environments here: https://realpython.com/python-virtual-environments-a-primer/\n\n\n\n\n2.1.3 Jupter Notebooks\nJupyter notebooks, such as this one, allow you to combine text and code into documents you can edit in the browser. The power of these notebooks is in documenting or describing what you are doing with the code alongside that code. For example, you could detail why you chose a particular clustering algorithm above the clustering code itself. In other words, it add narrative and helps clarify your workflow.\n\n\n\nThis same notebook, as displayed within Jupyterlab",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Introduction to Python</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_1.html#getting-started",
    "href": "content/labs/Lab_1/IM939_Lab_1_1.html#getting-started",
    "title": "2  Lab: Introduction to Python",
    "section": "2.2 Getting started",
    "text": "2.2 Getting started\nIf you send Python a number then it will print that number for you.\n\n45\n\n45\n\n\nYou will see both the input and output displayed. The input will have a label next to it like ‘In [1]’ where the number tells you how much code has already been sent to the Python interpreter (the programming interpreting the Python code and returnign the result). A line such as ‘In [100]’ tells you that 99 code cells have been passed to the Python interpreter in the current session.\nPython can carry out simple arithetic.\n\n44 + 87\n\n131\n\n\n\n188 / 12\n\n15.666666666666666\n\n\n\n46 - 128\n\n-82\n\n\nEach time the code in the cell is run and the result from the Python interpreter is displayed.",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Introduction to Python</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_1.html#sec-data-types",
    "href": "content/labs/Lab_1/IM939_Lab_1_1.html#sec-data-types",
    "title": "2  Lab: Introduction to Python",
    "section": "2.3 Data Types",
    "text": "2.3 Data Types\nAs we saw at this unit in the Skills Programme, programming languages use types to help them understand what a piece of data might represent. Knowing how data types work is important because they define what can be done and cannot be done with them. Each programming language has different data types which can be extended by other libraries such as Pandas, but these are some of the most frequent ones (and the ones we will be facing more frequently).\n\n2.3.1 int, floats, strings\nIntegers are whole numbers, like the ones we used above. We can check an object’s data type using type():\n\ntype(33)\n\nint\n\n\nYou can also have floats (numbers with decimal points)\n\n33.4\n\n33.4\n\n\n\ntype(33.4)\n\nfloat\n\n\nand a series of characters (strings).\n\n'I have a plan, sir.'\n\n'I have a plan, sir.'\n\n\n\ntype('I have a plan, sir.')\n\nstr\n\n\nData types are great and operators such as * do different things depending on the data type. For instance,\n\n33 * 3\n\n99\n\n\nThat seems quite sensible. What about if we had a string? Run teh below line. What is the * doing?\n\n'I have a plan, sir' * 3\n\n'I have a plan, sirI have a plan, sirI have a plan, sir'\n\n\nThere are also operators which only work with particular data types.\n\n'I have a plan, sir.' / 2\n\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n\nThis error message is very informative indeed. It tells us the line which caused the problem and that we have an error. Specifically, our error is a TypeError.\n\n\n\n\n\n\nUnderstanding the error\n\n\n\nIn this case, it says that the line 'I have a cunning plan' / 2 consists of string / int. We are trying to divide a string and int. The / operand is not able to divide a string by an int.\n\n\n\n\n2.3.2 lists and dictionaries\nYou can collect multiple values in a list. This is how they look like:\n\n[35, 'brown', 'yes']\n\n[35, 'brown', 'yes']\n\n\nAnd we can check theyr type:\n\ntype([35, 'brown', 'yes'])\n\nlist\n\n\nOr add keys to the values as a dictionary.\n\n{'age':35, 'hair colour': 'brown', 'Glasses': 'yes'}\n\n{'age': 35, 'hair colour': 'brown', 'Glasses': 'yes'}\n\n\n\ntype({'age':35, 'hair colour': 'brown', 'Glasses': 'yes'})\n\ndict",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Introduction to Python</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_1.html#variables",
    "href": "content/labs/Lab_1/IM939_Lab_1_1.html#variables",
    "title": "2  Lab: Introduction to Python",
    "section": "2.4 Variables",
    "text": "2.4 Variables\nVariables are bins for storing things in. These things can be data types. For example, the below creates a variable called my_height and stores the in 140 there.\n\nmy_height = 140\n\nThe Python interpreter is now storing the int 140 in a bin called my_height. If you pass the variable name to the interpreter then it will behave just like if you typed in 140 instead.\n\nmy_height\n\n140\n\n\n140\nVariables are neat when it comes to lists.\n\nmy_heights = [231, 234, 43]\nmy_heights\n\n[231, 234, 43]\n\n\n\nmy_heights[1]\n\n234\n\n\nWait, what happened above? What do you think the [1] does?\nYou can index multiple values from a list.\n\nmy_heights[0:2]\n\n[231, 234]",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Introduction to Python</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_1.html#bringing-it-all-together",
    "href": "content/labs/Lab_1/IM939_Lab_1_1.html#bringing-it-all-together",
    "title": "2  Lab: Introduction to Python",
    "section": "2.5 Bringing it all together",
    "text": "2.5 Bringing it all together\nWhat does the below do?\n\nradius = 40\npi = 3.1415\ncircle_area = pi * (radius * radius)\n\nlength = 12\nsquare_area = length * length\n\nmy_areas = [circle_area, square_area]\n\n\nmy_areas\n\n[5026.400000000001, 144]\n\n\nAs an aside, you can include comments which are not evaluated by the Python interpreter.\n\n# this is a comment. Python will ignore it.\n# another comment. \nn = 33",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Introduction to Python</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_1.html#congratulations",
    "href": "content/labs/Lab_1/IM939_Lab_1_1.html#congratulations",
    "title": "2  Lab: Introduction to Python",
    "section": "2.6 Congratulations",
    "text": "2.6 Congratulations\nYou’ve reached the end of the first notebook. We’ve looked at basic data type and variables. These are key components of all programming languages and a key part of working with data.\nIn the next notebook we will examine using libraries, loading in data, loops and functions.",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Introduction to Python</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_2.html",
    "href": "content/labs/Lab_1/IM939_Lab_1_2.html",
    "title": "3  Lab: Repetition",
    "section": "",
    "text": "3.1 Functions\nHow do we get the length of a list?\nlunch_ingredients = ['gravy', 'potatoes', 'meat', 'veg']\nlen(lunch_ingredients)\n\n4\nWe used a function. The syntax for a function is function_name(argument1, argument2). In our above example we passed the list lunch_ingrediants as an argument to the len function.\njames = {'age':35, 'hair colour': 'brown', 'Glasses': 'yes'}\nlen(james)\n\n3\nOne very useful function is to check a data type. In base R it may be obvious but when you move to libraries, with different data types, it may be worth checking.\ntype(james)\n\ndict\ntype(lunch_ingredients)\n\nlist\ntype(55)\n\nint",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Repetition</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_2.html#methods",
    "href": "content/labs/Lab_1/IM939_Lab_1_2.html#methods",
    "title": "3  Lab: Repetition",
    "section": "3.2 Methods",
    "text": "3.2 Methods\nWhat is really happening? Well all data types in Python have functions built in. A variable is an object (a string, a list, an int, etc.). Each object has a set of methods do stuff with that object.\nFor example, I can add a value to the lunch_ingredients list liks so\n\nlunch_ingredients.append('yorkshire pudding')\nlunch_ingredients\n\n['gravy', 'potatoes', 'meat', 'veg', 'yorkshire pudding']\n\n\nNote the objectname.method notation. By adding a yorkshire pudding, a fine addition to any meal, my lunch ingredients now number 5.\nI can count these easily.\n\nlen(lunch_ingredients)\n\n5\n\n\nWhich is the same as the following.\n\nlunch_ingredients.__len__()\n\n5\n\n\nYou can tab complete to see what methods are available for a given object. There are quite a few built in python functions.\nHowever, do be aware that different objects will likely have different methods. For instance, can we get a length of an int?\n\nlen(4)\n\nTypeError: object of type 'int' has no len()\n\n\nThe error “TypeError: object of type ‘int’ has no len()” essentially means an int object has no len dunder method.\nTo see the methods for a given object, type in the object name in a code cell, add a period ‘.’ and press tab.",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Repetition</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_2.html#flow-control",
    "href": "content/labs/Lab_1/IM939_Lab_1_2.html#flow-control",
    "title": "3  Lab: Repetition",
    "section": "3.3 Flow control",
    "text": "3.3 Flow control\n\n3.3.1 For loops\nYou may need to repeat some code (such as reading lines in a csv file). For loops allow you to repeat code but with a variable changing each loop.\nFor instance,\n\nfor ingredient in lunch_ingredients:\n    print(ingredient)\n    \n\ngravy\npotatoes\nmeat\nveg\nyorkshire pudding\n\n\nThe above loop went through each ingrediant and printed it.\nNote the syntax and spacing. The for statement finishes with a : and the next line has a tab. For loops have to be defined like that. If you miss out the tab then you will be shown an error. It does make the code more readable though.\n\nfor ingredient in lunch_ingredients:\n    print(ingredient)\n\ngravy\npotatoes\nmeat\nveg\nyorkshire pudding\n\n\nAt least the error is informative.\nWe can have more than one line in the loop.\n\nfor ingredient in lunch_ingredients:\n    print('I love to eat ' + ingredient)\n    print('food is great')\n    \n\nI love to eat gravy\nfood is great\nI love to eat potatoes\nfood is great\nI love to eat meat\nfood is great\nI love to eat veg\nfood is great\nI love to eat yorkshire pudding\nfood is great\n\n\n\n\n3.3.2 If statements\nAn if statement allows us to run different code depending on a condition.\n\nmy_num = 3\n\nif my_num &gt; 5:\n    print('Number is bigger than 5')\n    \nif my_num &lt; 5:\n    print('Number is less than 5')\n\nNumber is less than 5\n\n\nWe can include this in a for loop.\n\nfor i in range(10): #go through each number 0:10\n    print(i)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\nfor i in range(10):\n    print(i)\n    if i == 5:\n        print('We found a 5!')\n\n0\n1\n2\n3\n4\n5\nWe found a 5!\n6\n7\n8\n9\n\n\nYou can also include an else part of your if statement.\n\nfor i in range(10):\n    print(i)\n    if i == 5:\n        print('We found a 5! :)')\n    else:\n        print('Not a 5. Boo hoo :(')\n\n0\nNot a 5. Boo hoo :(\n1\nNot a 5. Boo hoo :(\n2\nNot a 5. Boo hoo :(\n3\nNot a 5. Boo hoo :(\n4\nNot a 5. Boo hoo :(\n5\nWe found a 5! :)\n6\nNot a 5. Boo hoo :(\n7\nNot a 5. Boo hoo :(\n8\nNot a 5. Boo hoo :(\n9\nNot a 5. Boo hoo :(",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Repetition</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_3.html",
    "href": "content/labs/Lab_1/IM939_Lab_1_3.html",
    "title": "4  Lab: Libraries",
    "section": "",
    "text": "4.1 Modules\nA module is a collection of code someone else has written (objects, methods, etc.). A library is a group of modules. In data science work you will use libraries such as numpy and scikit-learn. As we mentioned, Anaconda has many of these libraries built in and we do not need to install them separately.\nSometimes there are multiple ways to do something. Different libraries may have modules which have similiar functionality but slight differences.\nFor example, lets go through a comma seperated value file to make some data accessible in Python.",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lab: Libraries</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_3.html#modules",
    "href": "content/labs/Lab_1/IM939_Lab_1_3.html#modules",
    "title": "4  Lab: Libraries",
    "section": "",
    "text": "4.1.1 CSV module\n\nimport csv\n\nThe import keyword loads in a module. Here it loads in the csv module from the inbuilt Python library (see the Python docs on the csv module).\nPlease put the Facebook.csv file in the same folder as this notebook for this bit.\nThe code for going through a csv file is a bit lengthy.\n\nwith open('data/Facebook.csv', mode = 'r', encoding = 'UTF-8') as csvfile:    # open up our csv file\n    reader = csv.reader(csvfile)                                         # create a reader object which looks at our csv file\n    for row in reader:                                                   # for each row the reader finds\n        print(row)                                                       # print out the current row\n\n['\\ufefftype', 'post_message', 'link', 'link_domain', 'post_published', 'likes_count_fb', 'comments_count_fb', 'reactions_count_fb', 'shares_count_fb', 'engagement_fb']\n['status', 'Don t forget to post your photographs from heritage weekend  of you having a good time on here please.', '', '', '2017-09-09T13:57:14+0000', '5', '1', '5', '0', '6']\n['status', 'If you want to learn about Historic Coventry this Saturday we will be carrying out a guided tour around the Medieval Wall of Coventry. Booking details below', '', '', '2017-08-31T17:36:47+0000', '7', '12', '7', '0', '19']\n['event', '3 more dates for September Tours of The Medieval City Wall Below. Saturday 16th  23  rd and 30th https://www.facebook.com/events/1935828209971181/?ti=icl', 'https://www.facebook.com/events/1935828209971181/', 'facebook.com', '2017-09-05T10:46:45+0000', '2', '0', '2', '0', '2']\n['link', 'http://www.coventrytelegraph.net/whats-on/arts-culture-news/majestic-medieval-hall-coventrys-best-13448249', 'http://www.coventrytelegraph.net/whats-on/arts-culture-news/majestic-medieval-hall-coventrys-best-13448249', 'coventrytelegraph.net', '2017-08-14T08:56:04+0000', '9', '2', '10', '4', '16']\n['status', 'Different angle of St Michaels', '', '', '2017-08-26T09:04:35+0000', '11', '0', '12', '0', '12']\n['link', 'The Heritage Open Days are taking place from 7-10 September  with venues opening their doors and giving access to areas not normally open to the public - and now is the time to book a tour. As in previous years  there are some very interesting and unique tours taking place.   Make sure you don‚Äôt miss out on these by checking booking details online and booking your place.  Tours that need pre booking include: Medieval City Wall; BBC Coventry and Warwickshire Radio; Priory Undercroft and Visitor Centre; King Henry VIII School; Coventry Solihull Waste Disposal Ltd; and Coventry Central Library behind the scenes tour.  A Heritage Open Days brochure will be available from Council buildings  The Herbert  Coventry Cathedral and libraries from Wednesday (23 August).  For more details  visit the website www.coventry.gov.uk/hod.', 'http://www.coventry.gov.uk/hod', 'coventry.gov.uk', '2017-08-21T10:26:52+0000', '10', '0', '10', '13', '23']\n['status', 'Hi thanks to be able to share here & be part of tnis group ; Good to kbow iof yt area as going down soon with Natiomal Trust centre from Glasgow', '', '', '2017-08-17T08:36:07+0000', '1', '8', '1', '0', '9']\n['link', 'http://www.coventrytelegraph.net/whats-on/whats-on-news/future-coventrys-albany-theatre-been-13477790', 'http://www.coventrytelegraph.net/whats-on/whats-on-news/future-coventrys-albany-theatre-been-13477790', 'coventrytelegraph.net', '2017-08-17T09:14:59+0000', '8', '1', '9', '1', '11']\n['photo', 'https://www.facebook.com/peter.garbett.9/posts/1769150796433812  Sad we are loosing this museum visit it whilst you can', 'https://www.facebook.com/photo.php?fbid=1280170205424760&set=gm.1921178978100268&type=3', 'facebook.com', '2017-08-10T11:49:47+0000', '2', '1', '2', '0', '3']\n['link', 'https://www.facebook.com/CoventryPVC/posts/859986554155927', 'http://www.crowdfunder.co.uk/coventry-priory', 'crowdfunder.co.uk', '2017-08-07T21:13:46+0000', '6', '1', '6', '0', '7']\n['photo', 'Looking good in the sunshine.. https://www.facebook.com/visitcoventry.net/posts/501006916915750', 'https://www.facebook.com/visitcoventry.net/photos/a.373019843047792.1073741829.361929627490147/501006773582431/?type=3', 'facebook.com', '2017-08-06T21:25:32+0000', '8', '0', '8', '1', '9']\n['status', 'Best unique Pubs Whats your favourite pubs and why? in Coventry and surrounding areas.', '', '', '2017-07-19T08:35:49+0000', '0', '7', '0', '0', '7']\n['status', 'Thank you for letting me join your group', '', '', '2017-07-19T14:30:58+0000', '1', '1', '1', '0', '2']\n['link', 'https://www.facebook.com/theprideofwillenhall/posts/1378187435629358', 'http://www.coventrytelegraph.net/whats-on/whats-on-news/worlds-longest-inflatable-obstacle-course-13346712', 'coventrytelegraph.net', '2017-07-19T08:23:55+0000', '0', '0', '0', '0', '0']\n['link', 'https://www.facebook.com/peter.garbett.9/posts/1740635729285319  Coventry shortlisted!', 'https://coventry2021.co.uk/news/?is_mobile=1', 'coventry2021.co.uk', '2017-07-15T18:04:34+0000', '2', '0', '2', '0', '2']\n['photo', 'Nice. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/489764101373365/?type=3', 'https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/489764101373365/?type=3', 'facebook.com', '2017-07-13T22:06:20+0000', '3', '0', '3', '0', '3']\n['photo', 'I ve always wanted to have a nose down this passageway..  https://www.facebook.com/visitcoventry.net/posts/487218148294627', 'https://www.facebook.com/visitcoventry.net/photos/a.373019843047792.1073741829.361929627490147/487215948294847/?type=3', 'facebook.com', '2017-07-09T12:21:01+0000', '11', '1', '11', '0', '12']\n['link', 'http://www.coventrytelegraph.net/whats-on/music-nightlife-news/what-godiva-festival-weather-forecast-13275038', 'http://www.coventrytelegraph.net/whats-on/music-nightlife-news/what-godiva-festival-weather-forecast-13275038', 'coventrytelegraph.net', '2017-07-08T08:39:38+0000', '1', '0', '1', '0', '1']\n['video', 'Looks like a great idea.', 'https://www.facebook.com/stmarysguildhall/videos/10156233759843475/', 'facebook.com', '2017-07-07T15:28:24+0000', '7', '0', '9', '2', '11']\n['photo', 'Lovely pic.. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/483673398649102/?type=3', 'https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/483673398649102/?type=3', 'facebook.com', '2017-07-02T08:41:08+0000', '15', '2', '16', '1', '19']\n['status', 'What is your favourite restaurant or eatery in Coventry area?', '', '', '2017-06-29T08:55:29+0000', '3', '35', '3', '0', '38']\n['photo', 'Interesting.. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/481529385530170/?type=3', 'https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/481529385530170/?type=3', 'facebook.com', '2017-06-29T05:32:53+0000', '9', '0', '9', '0', '9']\n['link', 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'facebook.com', '2017-06-26T18:24:47+0000', '0', '0', '0', '0', '0']\n['status', 'Just saw a post about people that made me think would almost be a great motto for our fabulous city.   Let go of what s gone. Be grateful for (and save) what remains. Look forward to what is coming. ', '', '', '2017-06-25T17:17:12+0000', '4', '2', '5', '0', '7']\n['event', 'Why not make a weekend of it? It s Jaguar Super Saturday! In aid of Air Ambulance  details on joining below  if you own a Jaguar! This year there will be a  timeline  of cars from oldest to newest  many built at Browns Lane!', 'https://www.facebook.com/events/201361447045208/', 'facebook.com', '2017-06-22T19:27:54+0000', '2', '0', '2', '0', '2']\n['event', ':) we have a wonderful collection of exciting Citroen motorcars at the Museum on Sunday  if you own a Citroen you re welcome to come along with your car 10-4pm it s FREE and you may be given a leaflet about the car club! Any Citroen can come along  old or new  thank you :)', 'https://www.facebook.com/events/362084697478113/', 'facebook.com', '2017-06-22T19:23:49+0000', '1', '0', '1', '0', '1']\n['event', 'City Wall Guided Costume Tour this Saturday meeting at 2pm at Priory Visitors Centre ( behind Nando s  Trinity Street) with the Deep Fat Friar https://www.facebook.com/events/313502545772268/?ti=icl', 'https://www.facebook.com/events/313502545772268/', 'facebook.com', '2017-06-22T19:20:55+0000', '6', '0', '6', '1', '7']\n['link', 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'facebook.com', '2017-06-22T16:59:56+0000', '1', '0', '1', '0', '1']\n\n\nOk, each row looks like a list. But printing it out is pretty messy. We should dump this into a list of dictionaries (so we can pick out particular values).\n\ncsv_content = []                                   # create list to put our data into\n\nwith open('data/Facebook.csv', mode = 'r', encoding = 'UTF-8') as csvfile:  # open up csv file\n    reader = csv.DictReader(csvfile)                                   # create a reader which will read in each row and turn it into a dictionary\n    for row in reader:                                                 # for each row\n        csv_content.append(row) \n        print(row)                                                     # put the created dictionary into our list\n\n{'\\ufefftype': 'status', 'post_message': 'Don t forget to post your photographs from heritage weekend  of you having a good time on here please.', 'link': '', 'link_domain': '', 'post_published': '2017-09-09T13:57:14+0000', 'likes_count_fb': '5', 'comments_count_fb': '1', 'reactions_count_fb': '5', 'shares_count_fb': '0', 'engagement_fb': '6'}\n{'\\ufefftype': 'status', 'post_message': 'If you want to learn about Historic Coventry this Saturday we will be carrying out a guided tour around the Medieval Wall of Coventry. Booking details below', 'link': '', 'link_domain': '', 'post_published': '2017-08-31T17:36:47+0000', 'likes_count_fb': '7', 'comments_count_fb': '12', 'reactions_count_fb': '7', 'shares_count_fb': '0', 'engagement_fb': '19'}\n{'\\ufefftype': 'event', 'post_message': '3 more dates for September Tours of The Medieval City Wall Below. Saturday 16th  23  rd and 30th https://www.facebook.com/events/1935828209971181/?ti=icl', 'link': 'https://www.facebook.com/events/1935828209971181/', 'link_domain': 'facebook.com', 'post_published': '2017-09-05T10:46:45+0000', 'likes_count_fb': '2', 'comments_count_fb': '0', 'reactions_count_fb': '2', 'shares_count_fb': '0', 'engagement_fb': '2'}\n{'\\ufefftype': 'link', 'post_message': 'http://www.coventrytelegraph.net/whats-on/arts-culture-news/majestic-medieval-hall-coventrys-best-13448249', 'link': 'http://www.coventrytelegraph.net/whats-on/arts-culture-news/majestic-medieval-hall-coventrys-best-13448249', 'link_domain': 'coventrytelegraph.net', 'post_published': '2017-08-14T08:56:04+0000', 'likes_count_fb': '9', 'comments_count_fb': '2', 'reactions_count_fb': '10', 'shares_count_fb': '4', 'engagement_fb': '16'}\n{'\\ufefftype': 'status', 'post_message': 'Different angle of St Michaels', 'link': '', 'link_domain': '', 'post_published': '2017-08-26T09:04:35+0000', 'likes_count_fb': '11', 'comments_count_fb': '0', 'reactions_count_fb': '12', 'shares_count_fb': '0', 'engagement_fb': '12'}\n{'\\ufefftype': 'link', 'post_message': 'The Heritage Open Days are taking place from 7-10 September  with venues opening their doors and giving access to areas not normally open to the public - and now is the time to book a tour. As in previous years  there are some very interesting and unique tours taking place.   Make sure you don‚Äôt miss out on these by checking booking details online and booking your place.  Tours that need pre booking include: Medieval City Wall; BBC Coventry and Warwickshire Radio; Priory Undercroft and Visitor Centre; King Henry VIII School; Coventry Solihull Waste Disposal Ltd; and Coventry Central Library behind the scenes tour.  A Heritage Open Days brochure will be available from Council buildings  The Herbert  Coventry Cathedral and libraries from Wednesday (23 August).  For more details  visit the website www.coventry.gov.uk/hod.', 'link': 'http://www.coventry.gov.uk/hod', 'link_domain': 'coventry.gov.uk', 'post_published': '2017-08-21T10:26:52+0000', 'likes_count_fb': '10', 'comments_count_fb': '0', 'reactions_count_fb': '10', 'shares_count_fb': '13', 'engagement_fb': '23'}\n{'\\ufefftype': 'status', 'post_message': 'Hi thanks to be able to share here & be part of tnis group ; Good to kbow iof yt area as going down soon with Natiomal Trust centre from Glasgow', 'link': '', 'link_domain': '', 'post_published': '2017-08-17T08:36:07+0000', 'likes_count_fb': '1', 'comments_count_fb': '8', 'reactions_count_fb': '1', 'shares_count_fb': '0', 'engagement_fb': '9'}\n{'\\ufefftype': 'link', 'post_message': 'http://www.coventrytelegraph.net/whats-on/whats-on-news/future-coventrys-albany-theatre-been-13477790', 'link': 'http://www.coventrytelegraph.net/whats-on/whats-on-news/future-coventrys-albany-theatre-been-13477790', 'link_domain': 'coventrytelegraph.net', 'post_published': '2017-08-17T09:14:59+0000', 'likes_count_fb': '8', 'comments_count_fb': '1', 'reactions_count_fb': '9', 'shares_count_fb': '1', 'engagement_fb': '11'}\n{'\\ufefftype': 'photo', 'post_message': 'https://www.facebook.com/peter.garbett.9/posts/1769150796433812  Sad we are loosing this museum visit it whilst you can', 'link': 'https://www.facebook.com/photo.php?fbid=1280170205424760&set=gm.1921178978100268&type=3', 'link_domain': 'facebook.com', 'post_published': '2017-08-10T11:49:47+0000', 'likes_count_fb': '2', 'comments_count_fb': '1', 'reactions_count_fb': '2', 'shares_count_fb': '0', 'engagement_fb': '3'}\n{'\\ufefftype': 'link', 'post_message': 'https://www.facebook.com/CoventryPVC/posts/859986554155927', 'link': 'http://www.crowdfunder.co.uk/coventry-priory', 'link_domain': 'crowdfunder.co.uk', 'post_published': '2017-08-07T21:13:46+0000', 'likes_count_fb': '6', 'comments_count_fb': '1', 'reactions_count_fb': '6', 'shares_count_fb': '0', 'engagement_fb': '7'}\n{'\\ufefftype': 'photo', 'post_message': 'Looking good in the sunshine.. https://www.facebook.com/visitcoventry.net/posts/501006916915750', 'link': 'https://www.facebook.com/visitcoventry.net/photos/a.373019843047792.1073741829.361929627490147/501006773582431/?type=3', 'link_domain': 'facebook.com', 'post_published': '2017-08-06T21:25:32+0000', 'likes_count_fb': '8', 'comments_count_fb': '0', 'reactions_count_fb': '8', 'shares_count_fb': '1', 'engagement_fb': '9'}\n{'\\ufefftype': 'status', 'post_message': 'Best unique Pubs Whats your favourite pubs and why? in Coventry and surrounding areas.', 'link': '', 'link_domain': '', 'post_published': '2017-07-19T08:35:49+0000', 'likes_count_fb': '0', 'comments_count_fb': '7', 'reactions_count_fb': '0', 'shares_count_fb': '0', 'engagement_fb': '7'}\n{'\\ufefftype': 'status', 'post_message': 'Thank you for letting me join your group', 'link': '', 'link_domain': '', 'post_published': '2017-07-19T14:30:58+0000', 'likes_count_fb': '1', 'comments_count_fb': '1', 'reactions_count_fb': '1', 'shares_count_fb': '0', 'engagement_fb': '2'}\n{'\\ufefftype': 'link', 'post_message': 'https://www.facebook.com/theprideofwillenhall/posts/1378187435629358', 'link': 'http://www.coventrytelegraph.net/whats-on/whats-on-news/worlds-longest-inflatable-obstacle-course-13346712', 'link_domain': 'coventrytelegraph.net', 'post_published': '2017-07-19T08:23:55+0000', 'likes_count_fb': '0', 'comments_count_fb': '0', 'reactions_count_fb': '0', 'shares_count_fb': '0', 'engagement_fb': '0'}\n{'\\ufefftype': 'link', 'post_message': 'https://www.facebook.com/peter.garbett.9/posts/1740635729285319  Coventry shortlisted!', 'link': 'https://coventry2021.co.uk/news/?is_mobile=1', 'link_domain': 'coventry2021.co.uk', 'post_published': '2017-07-15T18:04:34+0000', 'likes_count_fb': '2', 'comments_count_fb': '0', 'reactions_count_fb': '2', 'shares_count_fb': '0', 'engagement_fb': '2'}\n{'\\ufefftype': 'photo', 'post_message': 'Nice. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/489764101373365/?type=3', 'link': 'https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/489764101373365/?type=3', 'link_domain': 'facebook.com', 'post_published': '2017-07-13T22:06:20+0000', 'likes_count_fb': '3', 'comments_count_fb': '0', 'reactions_count_fb': '3', 'shares_count_fb': '0', 'engagement_fb': '3'}\n{'\\ufefftype': 'photo', 'post_message': 'I ve always wanted to have a nose down this passageway..  https://www.facebook.com/visitcoventry.net/posts/487218148294627', 'link': 'https://www.facebook.com/visitcoventry.net/photos/a.373019843047792.1073741829.361929627490147/487215948294847/?type=3', 'link_domain': 'facebook.com', 'post_published': '2017-07-09T12:21:01+0000', 'likes_count_fb': '11', 'comments_count_fb': '1', 'reactions_count_fb': '11', 'shares_count_fb': '0', 'engagement_fb': '12'}\n{'\\ufefftype': 'link', 'post_message': 'http://www.coventrytelegraph.net/whats-on/music-nightlife-news/what-godiva-festival-weather-forecast-13275038', 'link': 'http://www.coventrytelegraph.net/whats-on/music-nightlife-news/what-godiva-festival-weather-forecast-13275038', 'link_domain': 'coventrytelegraph.net', 'post_published': '2017-07-08T08:39:38+0000', 'likes_count_fb': '1', 'comments_count_fb': '0', 'reactions_count_fb': '1', 'shares_count_fb': '0', 'engagement_fb': '1'}\n{'\\ufefftype': 'video', 'post_message': 'Looks like a great idea.', 'link': 'https://www.facebook.com/stmarysguildhall/videos/10156233759843475/', 'link_domain': 'facebook.com', 'post_published': '2017-07-07T15:28:24+0000', 'likes_count_fb': '7', 'comments_count_fb': '0', 'reactions_count_fb': '9', 'shares_count_fb': '2', 'engagement_fb': '11'}\n{'\\ufefftype': 'photo', 'post_message': 'Lovely pic.. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/483673398649102/?type=3', 'link': 'https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/483673398649102/?type=3', 'link_domain': 'facebook.com', 'post_published': '2017-07-02T08:41:08+0000', 'likes_count_fb': '15', 'comments_count_fb': '2', 'reactions_count_fb': '16', 'shares_count_fb': '1', 'engagement_fb': '19'}\n{'\\ufefftype': 'status', 'post_message': 'What is your favourite restaurant or eatery in Coventry area?', 'link': '', 'link_domain': '', 'post_published': '2017-06-29T08:55:29+0000', 'likes_count_fb': '3', 'comments_count_fb': '35', 'reactions_count_fb': '3', 'shares_count_fb': '0', 'engagement_fb': '38'}\n{'\\ufefftype': 'photo', 'post_message': 'Interesting.. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/481529385530170/?type=3', 'link': 'https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/481529385530170/?type=3', 'link_domain': 'facebook.com', 'post_published': '2017-06-29T05:32:53+0000', 'likes_count_fb': '9', 'comments_count_fb': '0', 'reactions_count_fb': '9', 'shares_count_fb': '0', 'engagement_fb': '9'}\n{'\\ufefftype': 'link', 'post_message': 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'link': 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'link_domain': 'facebook.com', 'post_published': '2017-06-26T18:24:47+0000', 'likes_count_fb': '0', 'comments_count_fb': '0', 'reactions_count_fb': '0', 'shares_count_fb': '0', 'engagement_fb': '0'}\n{'\\ufefftype': 'status', 'post_message': 'Just saw a post about people that made me think would almost be a great motto for our fabulous city.   Let go of what s gone. Be grateful for (and save) what remains. Look forward to what is coming. ', 'link': '', 'link_domain': '', 'post_published': '2017-06-25T17:17:12+0000', 'likes_count_fb': '4', 'comments_count_fb': '2', 'reactions_count_fb': '5', 'shares_count_fb': '0', 'engagement_fb': '7'}\n{'\\ufefftype': 'event', 'post_message': 'Why not make a weekend of it? It s Jaguar Super Saturday! In aid of Air Ambulance  details on joining below  if you own a Jaguar! This year there will be a  timeline  of cars from oldest to newest  many built at Browns Lane!', 'link': 'https://www.facebook.com/events/201361447045208/', 'link_domain': 'facebook.com', 'post_published': '2017-06-22T19:27:54+0000', 'likes_count_fb': '2', 'comments_count_fb': '0', 'reactions_count_fb': '2', 'shares_count_fb': '0', 'engagement_fb': '2'}\n{'\\ufefftype': 'event', 'post_message': ':) we have a wonderful collection of exciting Citroen motorcars at the Museum on Sunday  if you own a Citroen you re welcome to come along with your car 10-4pm it s FREE and you may be given a leaflet about the car club! Any Citroen can come along  old or new  thank you :)', 'link': 'https://www.facebook.com/events/362084697478113/', 'link_domain': 'facebook.com', 'post_published': '2017-06-22T19:23:49+0000', 'likes_count_fb': '1', 'comments_count_fb': '0', 'reactions_count_fb': '1', 'shares_count_fb': '0', 'engagement_fb': '1'}\n{'\\ufefftype': 'event', 'post_message': 'City Wall Guided Costume Tour this Saturday meeting at 2pm at Priory Visitors Centre ( behind Nando s  Trinity Street) with the Deep Fat Friar https://www.facebook.com/events/313502545772268/?ti=icl', 'link': 'https://www.facebook.com/events/313502545772268/', 'link_domain': 'facebook.com', 'post_published': '2017-06-22T19:20:55+0000', 'likes_count_fb': '6', 'comments_count_fb': '0', 'reactions_count_fb': '6', 'shares_count_fb': '1', 'engagement_fb': '7'}\n{'\\ufefftype': 'link', 'post_message': 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'link': 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'link_domain': 'facebook.com', 'post_published': '2017-06-22T16:59:56+0000', 'likes_count_fb': '1', 'comments_count_fb': '0', 'reactions_count_fb': '1', 'shares_count_fb': '0', 'engagement_fb': '1'}\n\n\nLook at the first entry in csv_content.\n\ncsv_content[0]\n\n{'\\ufefftype': 'status',\n 'post_message': 'Don t forget to post your photographs from heritage weekend  of you having a good time on here please.',\n 'link': '',\n 'link_domain': '',\n 'post_published': '2017-09-09T13:57:14+0000',\n 'likes_count_fb': '5',\n 'comments_count_fb': '1',\n 'reactions_count_fb': '5',\n 'shares_count_fb': '0',\n 'engagement_fb': '6'}\n\n\nDictionaries have keys. The keys method of the dictionary object will show them to us (though they are obvious from printing the first line aboe).\n\ncsv_content[0].keys()\n\ndict_keys(['\\ufefftype', 'post_message', 'link', 'link_domain', 'post_published', 'likes_count_fb', 'comments_count_fb', 'reactions_count_fb', 'shares_count_fb', 'engagement_fb'])\n\n\n\ncsv_content[0]['post_message']\n\n'Don t forget to post your photographs from heritage weekend  of you having a good time on here please.'\n\n\nWe have a list where each list element is a dictionary. So we need to index our list each time, hence the csv_content[0].\nTo go through our list we need to use a for loop. So, in order to get each post message.\n\nfor post in csv_content:\n    print(post['post_message'])\n\nDon t forget to post your photographs from heritage weekend  of you having a good time on here please.\nIf you want to learn about Historic Coventry this Saturday we will be carrying out a guided tour around the Medieval Wall of Coventry. Booking details below\n3 more dates for September Tours of The Medieval City Wall Below. Saturday 16th  23  rd and 30th https://www.facebook.com/events/1935828209971181/?ti=icl\nhttp://www.coventrytelegraph.net/whats-on/arts-culture-news/majestic-medieval-hall-coventrys-best-13448249\nDifferent angle of St Michaels\nThe Heritage Open Days are taking place from 7-10 September  with venues opening their doors and giving access to areas not normally open to the public - and now is the time to book a tour. As in previous years  there are some very interesting and unique tours taking place.   Make sure you don‚Äôt miss out on these by checking booking details online and booking your place.  Tours that need pre booking include: Medieval City Wall; BBC Coventry and Warwickshire Radio; Priory Undercroft and Visitor Centre; King Henry VIII School; Coventry Solihull Waste Disposal Ltd; and Coventry Central Library behind the scenes tour.  A Heritage Open Days brochure will be available from Council buildings  The Herbert  Coventry Cathedral and libraries from Wednesday (23 August).  For more details  visit the website www.coventry.gov.uk/hod.\nHi thanks to be able to share here & be part of tnis group ; Good to kbow iof yt area as going down soon with Natiomal Trust centre from Glasgow\nhttp://www.coventrytelegraph.net/whats-on/whats-on-news/future-coventrys-albany-theatre-been-13477790\nhttps://www.facebook.com/peter.garbett.9/posts/1769150796433812  Sad we are loosing this museum visit it whilst you can\nhttps://www.facebook.com/CoventryPVC/posts/859986554155927\nLooking good in the sunshine.. https://www.facebook.com/visitcoventry.net/posts/501006916915750\nBest unique Pubs Whats your favourite pubs and why? in Coventry and surrounding areas.\nThank you for letting me join your group\nhttps://www.facebook.com/theprideofwillenhall/posts/1378187435629358\nhttps://www.facebook.com/peter.garbett.9/posts/1740635729285319  Coventry shortlisted!\nNice. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/489764101373365/?type=3\nI ve always wanted to have a nose down this passageway..  https://www.facebook.com/visitcoventry.net/posts/487218148294627\nhttp://www.coventrytelegraph.net/whats-on/music-nightlife-news/what-godiva-festival-weather-forecast-13275038\nLooks like a great idea.\nLovely pic.. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/483673398649102/?type=3\nWhat is your favourite restaurant or eatery in Coventry area?\nInteresting.. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/481529385530170/?type=3\nhttps://www.facebook.com/BerkswellWindmill/posts/1322847697832828\nJust saw a post about people that made me think would almost be a great motto for our fabulous city.   Let go of what s gone. Be grateful for (and save) what remains. Look forward to what is coming. \nWhy not make a weekend of it? It s Jaguar Super Saturday! In aid of Air Ambulance  details on joining below  if you own a Jaguar! This year there will be a  timeline  of cars from oldest to newest  many built at Browns Lane!\n:) we have a wonderful collection of exciting Citroen motorcars at the Museum on Sunday  if you own a Citroen you re welcome to come along with your car 10-4pm it s FREE and you may be given a leaflet about the car club! Any Citroen can come along  old or new  thank you :)\nCity Wall Guided Costume Tour this Saturday meeting at 2pm at Priory Visitors Centre ( behind Nando s  Trinity Street) with the Deep Fat Friar https://www.facebook.com/events/313502545772268/?ti=icl\nhttps://www.facebook.com/BerkswellWindmill/posts/1322847697832828\n\n\nIf I want to do data science, where accessing data in a sensible way is key, then there must be a better way! There is.\n\n\n4.1.2 Pandas\nThe Pandas library is designed with data science in mind. You will examine it more in the coming weeks. Reading CSV files with pandas is very easy.\n\nimport pandas as pd\n\nThe import x as y is good practice. In the below code, anything with a pd. prefix comes from pandas. This is particulary useful for preventing a module from overwriting inbuilt Python functionality.\n\ndf = pd.read_csv('data/Facebook.csv', encoding = 'UTF-8')\n\nThat was easy. We can even pick out specific rows.\n\ndf['post_message']\n\n0     Don t forget to post your photographs from her...\n1     If you want to learn about Historic Coventry t...\n2     3 more dates for September Tours of The Mediev...\n3     http://www.coventrytelegraph.net/whats-on/arts...\n4                        Different angle of St Michaels\n5     The Heritage Open Days are taking place from 7...\n6     Hi thanks to be able to share here & be part o...\n7     http://www.coventrytelegraph.net/whats-on/what...\n8     https://www.facebook.com/peter.garbett.9/posts...\n9     https://www.facebook.com/CoventryPVC/posts/859...\n10    Looking good in the sunshine.. https://www.fac...\n11    Best unique Pubs Whats your favourite pubs and...\n12             Thank you for letting me join your group\n13    https://www.facebook.com/theprideofwillenhall/...\n14    https://www.facebook.com/peter.garbett.9/posts...\n15    Nice. https://www.facebook.com/visitcoventry.n...\n16    I ve always wanted to have a nose down this pa...\n17    http://www.coventrytelegraph.net/whats-on/musi...\n18                             Looks like a great idea.\n19    Lovely pic.. https://www.facebook.com/visitcov...\n20    What is your favourite restaurant or eatery in...\n21    Interesting.. https://www.facebook.com/visitco...\n22    https://www.facebook.com/BerkswellWindmill/pos...\n23    Just saw a post about people that made me thin...\n24    Why not make a weekend of it? It s Jaguar Supe...\n25    :) we have a wonderful collection of exciting ...\n26    City Wall Guided Costume Tour this Saturday me...\n27    https://www.facebook.com/BerkswellWindmill/pos...\nName: post_message, dtype: object\n\n\nAnd even look at the dataset in a pretty way.\n\ndf\n\n\n\n\n\n\n\n\ntype\npost_message\nlink\nlink_domain\npost_published\nlikes_count_fb\ncomments_count_fb\nreactions_count_fb\nshares_count_fb\nengagement_fb\n\n\n\n\n0\nstatus\nDon t forget to post your photographs from her...\nNaN\nNaN\n2017-09-09T13:57:14+0000\n5\n1\n5\n0\n6\n\n\n1\nstatus\nIf you want to learn about Historic Coventry t...\nNaN\nNaN\n2017-08-31T17:36:47+0000\n7\n12\n7\n0\n19\n\n\n2\nevent\n3 more dates for September Tours of The Mediev...\nhttps://www.facebook.com/events/1935828209971181/\nfacebook.com\n2017-09-05T10:46:45+0000\n2\n0\n2\n0\n2\n\n\n3\nlink\nhttp://www.coventrytelegraph.net/whats-on/arts...\nhttp://www.coventrytelegraph.net/whats-on/arts...\ncoventrytelegraph.net\n2017-08-14T08:56:04+0000\n9\n2\n10\n4\n16\n\n\n4\nstatus\nDifferent angle of St Michaels\nNaN\nNaN\n2017-08-26T09:04:35+0000\n11\n0\n12\n0\n12\n\n\n5\nlink\nThe Heritage Open Days are taking place from 7...\nhttp://www.coventry.gov.uk/hod\ncoventry.gov.uk\n2017-08-21T10:26:52+0000\n10\n0\n10\n13\n23\n\n\n6\nstatus\nHi thanks to be able to share here & be part o...\nNaN\nNaN\n2017-08-17T08:36:07+0000\n1\n8\n1\n0\n9\n\n\n7\nlink\nhttp://www.coventrytelegraph.net/whats-on/what...\nhttp://www.coventrytelegraph.net/whats-on/what...\ncoventrytelegraph.net\n2017-08-17T09:14:59+0000\n8\n1\n9\n1\n11\n\n\n8\nphoto\nhttps://www.facebook.com/peter.garbett.9/posts...\nhttps://www.facebook.com/photo.php?fbid=128017...\nfacebook.com\n2017-08-10T11:49:47+0000\n2\n1\n2\n0\n3\n\n\n9\nlink\nhttps://www.facebook.com/CoventryPVC/posts/859...\nhttp://www.crowdfunder.co.uk/coventry-priory\ncrowdfunder.co.uk\n2017-08-07T21:13:46+0000\n6\n1\n6\n0\n7\n\n\n10\nphoto\nLooking good in the sunshine.. https://www.fac...\nhttps://www.facebook.com/visitcoventry.net/pho...\nfacebook.com\n2017-08-06T21:25:32+0000\n8\n0\n8\n1\n9\n\n\n11\nstatus\nBest unique Pubs Whats your favourite pubs and...\nNaN\nNaN\n2017-07-19T08:35:49+0000\n0\n7\n0\n0\n7\n\n\n12\nstatus\nThank you for letting me join your group\nNaN\nNaN\n2017-07-19T14:30:58+0000\n1\n1\n1\n0\n2\n\n\n13\nlink\nhttps://www.facebook.com/theprideofwillenhall/...\nhttp://www.coventrytelegraph.net/whats-on/what...\ncoventrytelegraph.net\n2017-07-19T08:23:55+0000\n0\n0\n0\n0\n0\n\n\n14\nlink\nhttps://www.facebook.com/peter.garbett.9/posts...\nhttps://coventry2021.co.uk/news/?is_mobile=1\ncoventry2021.co.uk\n2017-07-15T18:04:34+0000\n2\n0\n2\n0\n2\n\n\n15\nphoto\nNice. https://www.facebook.com/visitcoventry.n...\nhttps://www.facebook.com/visitcoventry.net/pho...\nfacebook.com\n2017-07-13T22:06:20+0000\n3\n0\n3\n0\n3\n\n\n16\nphoto\nI ve always wanted to have a nose down this pa...\nhttps://www.facebook.com/visitcoventry.net/pho...\nfacebook.com\n2017-07-09T12:21:01+0000\n11\n1\n11\n0\n12\n\n\n17\nlink\nhttp://www.coventrytelegraph.net/whats-on/musi...\nhttp://www.coventrytelegraph.net/whats-on/musi...\ncoventrytelegraph.net\n2017-07-08T08:39:38+0000\n1\n0\n1\n0\n1\n\n\n18\nvideo\nLooks like a great idea.\nhttps://www.facebook.com/stmarysguildhall/vide...\nfacebook.com\n2017-07-07T15:28:24+0000\n7\n0\n9\n2\n11\n\n\n19\nphoto\nLovely pic.. https://www.facebook.com/visitcov...\nhttps://www.facebook.com/visitcoventry.net/pho...\nfacebook.com\n2017-07-02T08:41:08+0000\n15\n2\n16\n1\n19\n\n\n20\nstatus\nWhat is your favourite restaurant or eatery in...\nNaN\nNaN\n2017-06-29T08:55:29+0000\n3\n35\n3\n0\n38\n\n\n21\nphoto\nInteresting.. https://www.facebook.com/visitco...\nhttps://www.facebook.com/visitcoventry.net/pho...\nfacebook.com\n2017-06-29T05:32:53+0000\n9\n0\n9\n0\n9\n\n\n22\nlink\nhttps://www.facebook.com/BerkswellWindmill/pos...\nhttps://www.facebook.com/BerkswellWindmill/pos...\nfacebook.com\n2017-06-26T18:24:47+0000\n0\n0\n0\n0\n0\n\n\n23\nstatus\nJust saw a post about people that made me thin...\nNaN\nNaN\n2017-06-25T17:17:12+0000\n4\n2\n5\n0\n7\n\n\n24\nevent\nWhy not make a weekend of it? It s Jaguar Supe...\nhttps://www.facebook.com/events/201361447045208/\nfacebook.com\n2017-06-22T19:27:54+0000\n2\n0\n2\n0\n2\n\n\n25\nevent\n:) we have a wonderful collection of exciting ...\nhttps://www.facebook.com/events/362084697478113/\nfacebook.com\n2017-06-22T19:23:49+0000\n1\n0\n1\n0\n1\n\n\n26\nevent\nCity Wall Guided Costume Tour this Saturday me...\nhttps://www.facebook.com/events/313502545772268/\nfacebook.com\n2017-06-22T19:20:55+0000\n6\n0\n6\n1\n7\n\n\n27\nlink\nhttps://www.facebook.com/BerkswellWindmill/pos...\nhttps://www.facebook.com/BerkswellWindmill/pos...\nfacebook.com\n2017-06-22T16:59:56+0000\n1\n0\n1\n0\n1\n\n\n\n\n\n\n\nHuzzah!\nA final point, pd.read_csv returns a pandas specific object with associated methods.\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\n\ndf.dtypes\n\ntype                  object\npost_message          object\nlink                  object\nlink_domain           object\npost_published        object\nlikes_count_fb         int64\ncomments_count_fb      int64\nreactions_count_fb     int64\nshares_count_fb        int64\nengagement_fb          int64\ndtype: object\n\n\nWhere int64 are integers and object here refers to strings.",
    "crumbs": [
      "Introduction and Historical Perspectives",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lab: Libraries</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-02.html",
    "href": "content/sessions/session-02.html",
    "title": "5  Introduction",
    "section": "",
    "text": "5.1 Highlights of the lecture\nThis week explores the cultural, ethical, and critical challenges posed by data artefacts and data-intensive scientific processes. Engaging with Critical Data Studies, we discuss issues around data capture, curation, data quality, inclusion/exclusion and representativeness. The session also discusses the different kinds of data that one can encounter across disciplines, the underlying characteristics of data and how we can analytically and practically approach data quality issues and the challenge of identifying and curating appropriate data sets.\nSome of the key concepts you should remember from this week are …",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-02.html#highlights-of-the-lecture",
    "href": "content/sessions/session-02.html#highlights-of-the-lecture",
    "title": "5  Introduction",
    "section": "",
    "text": "Thinking broadly on and with data – ethical considerations, potential biases\nData type taxonomies, w.r.t., sources of data, scales of measurement and common interpretations of data: tabular, temporal, spatial, textual, network\nData wrangling: concepts and common methods, missing values and data transformation approaches",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-02.html#practical-lab-session",
    "href": "content/sessions/session-02.html#practical-lab-session",
    "title": "5  Introduction",
    "section": "5.2 Practical Lab Session",
    "text": "5.2 Practical Lab Session\nThe practical lab session walks you through the earlier stages of the data science process. We start by looking at different types of data suitable for analysis within a data science framework and move on to how to wrangle the data to make it available for further use.\nAt the end of the session, you should be able to ..\n\nPerform basic data manipulation and data merging tasks with Numpy and Pandas\nIdentify and address data quality issues\nGet familiar with elementary data visualisation tools to help you interrogate the data",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-02.html#independent-learning-reading-lists",
    "href": "content/sessions/session-02.html#independent-learning-reading-lists",
    "title": "5  Introduction",
    "section": "5.3 Independent learning & Reading lists",
    "text": "5.3 Independent learning & Reading lists\n\n5.3.1 Required reading\n\nThinking comprehensively on Data and Data Science – Data Feminism by Catherine D’Ignazio and Lauren Klein: You can read the introduction chapter and chapter-1 in relation to discussions this week: https://mitpressonpubpub.mitpress.mit.edu/data-feminism\nWatch: Databite No. 131: Data Feminism by Catherine D’Ignazio and Lauren F. Klein: https://youtu.be/Su3vIF5P06M\nOn data wrangling: Kandel, Sean, et al. “Research directions in data wrangling: Visualizations and transformations for usable and credible data.” Information Visualization 10.4 (2011): 271-288.\nOn biases in data/modelling workflows: Suresh, H. and Guttag, J.V., 2019. A framework for understanding unintended consequences of machine learning. arXiv preprint arXiv:1901.10002. [pdf]\n\n\n\n5.3.2 Background reading\n\nOn missing value removal (accessible through library resources): Acuna, Edgar, and Caroline Rodriguez. “The treatment of missing values and its effect on classifier accuracy.” Classification, Clustering, and Data Mining Applications. Springer Berlin Heidelberg, 2004. 639-647.\nSaar-Tsechansky, Maytal, and Foster Provost. “Handling Missing Values when Applying Classification Models.” Journal of Machine Learning Research 8 (2007): 1625-1657.",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html",
    "title": "6  Lab: Pandas",
    "section": "",
    "text": "6.1 Dataset\nThe Office is a humoristic TV series originally created in 2001 by Ricky Gervais and Stephen Merchant that has received several adaptations. The dataset that we will be using contains information (i.e., title, date and ratings from IMBDB) about every episode of the 9 seaons of the very successful USA’s adaptation aired between 2005 and 2013.\nThe dataset is stored in a csv file that has the following columns: season, episode, title, imdb_rating, total_votes, air_date.",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab: Pandas</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html#dataset",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html#dataset",
    "title": "6  Lab: Pandas",
    "section": "",
    "text": "The Office promotional poster",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab: Pandas</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html#starting",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html#starting",
    "title": "6  Lab: Pandas",
    "section": "6.2 Starting",
    "text": "6.2 Starting\nTo work with the dataset we will need to import pandas so we can use every feature provided by the library, as well as loading the dataset stored in the office_ratings.csv.\n\n# These two lines are added so that not all the warnings are rendered in the cell. \n# We do this not to confuse you during your learning journey with some of the warnings but \n# normally you would want them turned on since they can tell you something about things that might not be working as expected.\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# this following import will always be needed whenever you want to work with Pandas.\nimport pandas as pd\n\ndf = pd.read_csv('data/raw/office_ratings.csv', encoding='UTF-8')\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 188 entries, 0 to 187\nData columns (total 6 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   season       188 non-null    int64  \n 1   episode      188 non-null    int64  \n 2   title        188 non-null    object \n 3   imdb_rating  188 non-null    float64\n 4   total_votes  188 non-null    int64  \n 5   air_date     188 non-null    object \ndtypes: float64(1), int64(3), object(2)\nmemory usage: 8.9+ KB",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab: Pandas</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html#help",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html#help",
    "title": "6  Lab: Pandas",
    "section": "6.3 Help!",
    "text": "6.3 Help!\nPython has inbuilt documentation. To access this add a ? before an object or method.\n\n\n\n\n\n\nNote\n\n\n\nThe output of the help function has been omitted in the handbook. Please run the cells in your notebook to read the different outputs\n\n\nFor example, our dataframe\n\n?df\n\nor the dtypes property\n\n\n\n\n\n\nTip\n\n\n\nProperties of object are values associated with the object and are not called with a () at the end.\n\n\n\n?df.dtypes\n\nThe info method for dataframes.\n\n?df.info\n\nIf you would like to get help in-line like the examples above, that can give a very long help message that might not be always convenient. If you like, you can try to get the help for this following function like this:\n\n?pd.read_csv\n\nHowever, the below will be quite long – it provides you the various arguments (options) you can use with the method.\nInstead of this approach, a much better way to get help is to refer to the documentation and the API of the library that you are using. For instance, for read_csv(), this page is much more useful – https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\nWe recommend that you use a search engine very frequently.\n\n?pd.read_csv\n\nThe Pandas documentation is rather good. Relevent to our below work is:\n\nWhat kind of data does pandas handle?\nHow to calculate summary statistics?\nHow to create plots in pandas?\nHow to handle time series data with ease?\n\nI also found a rather nice series of lessons a kind person put together. There are lots of online tutorials which will help you.",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab: Pandas</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html#structure",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html#structure",
    "title": "6  Lab: Pandas",
    "section": "6.4 Structure",
    "text": "6.4 Structure\nIn Section 2.3 we introduced Python’s data types as well as how to use the function type() to retrieve an object’s data type. Pandas expands python’s data types by creating a new one called data frame\n\n\nDo you remember what are Python’s data types? You can refer to Section 2.3 for a refresher and to know more about them.\n\n\n\n\n\n\nData frames\n\n\n\nData frames are 2-dimensional data structures that store information in columns and rows, very much like data is stored in a spreadsheet or a database. Typically, every column will contain variables (or sometimes called attributes) whereas every row represents an observation. This is known as wide data frames, as opposed to long data frames.\nIn pandas, every column has a name and rows can be named, too.\n\n\nSo let’s check the what our newly created object’s (df) data type:\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\nUnsurprisingly, df is a `DataFrame`` object, provided by pandas.\nThe DataFrame object has lots of built in methods and attributes.\nThe info method gives us information about datatypes, dimensions and the presence of null values in our dataframe. Let’s see how can we use it and what information is returned:\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 188 entries, 0 to 187\nData columns (total 6 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   season       188 non-null    int64  \n 1   episode      188 non-null    int64  \n 2   title        188 non-null    object \n 3   imdb_rating  188 non-null    float64\n 4   total_votes  188 non-null    int64  \n 5   air_date     188 non-null    object \ndtypes: float64(1), int64(3), object(2)\nmemory usage: 8.9+ KB\n\n\nWe can just dtypes to check the data types of every variable in the data frame.\n\ndf.dtypes\n\nseason           int64\nepisode          int64\ntitle           object\nimdb_rating    float64\ntotal_votes      int64\nair_date        object\ndtype: object\n\n\nOr just the dimensions (e.g., rows and columns).\n\ndf.shape\n\n(188, 6)\n\n\nIn this case, there are only 188 rows. But for larger datasets we might want to look at the head (top 5) and tail (bottom 5) rows using .head() and .tail(), respectively.\n\ndf.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n\n\n\n\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n183\n9\n19\nStairmageddon\n8.0\n1484\n2013-04-11\n\n\n184\n9\n20\nPaper Airplane\n8.0\n1482\n2013-04-25\n\n\n185\n9\n21\nLivin' the Dream\n8.9\n2041\n2013-05-02\n\n\n186\n9\n22\nA.A.R.M.\n9.3\n2860\n2013-05-09\n\n\n187\n9\n23\nFinale\n9.7\n7934\n2013-05-16",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab: Pandas</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html#summary",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html#summary",
    "title": "6  Lab: Pandas",
    "section": "6.5 Summary",
    "text": "6.5 Summary\nTo get an overview of our data we can ask Python to ‘describe our (numeric) data’\n\ndf.describe()\n\n\n\n\n\n\n\n\nseason\nepisode\nimdb_rating\ntotal_votes\n\n\n\n\ncount\n188.000000\n188.000000\n188.000000\n188.000000\n\n\nmean\n5.468085\n11.877660\n8.257447\n2126.648936\n\n\nstd\n2.386245\n7.024855\n0.538067\n787.098275\n\n\nmin\n1.000000\n1.000000\n6.700000\n1393.000000\n\n\n25%\n3.000000\n6.000000\n7.900000\n1631.500000\n\n\n50%\n6.000000\n11.500000\n8.200000\n1952.500000\n\n\n75%\n7.250000\n18.000000\n8.600000\n2379.000000\n\n\nmax\n9.000000\n26.000000\n9.700000\n7934.000000\n\n\n\n\n\n\n\nor we can pull out specific statistics for numeric columns.\n\ndf.mean()\n\nTypeError: Could not convert [\"PilotDiversity DayHealth CareThe AllianceBasketballHot GirlThe DundiesSexual HarassmentOffice OlympicsThe FireHalloweenThe FightThe ClientPerformance ReviewE-Mail SurveillanceChristmas PartyBooze CruiseThe InjuryThe SecretThe CarpetBoys and GirlsValentine's DayDwight's SpeechTake Your Daughter to Work DayMichael's BirthdayDrug TestingConflict ResolutionCasino NightGay Witch HuntThe ConventionThe CoupGrief CounselingInitiationDiwaliBranch ClosingThe MergerThe ConvictA Benihana ChristmasBack from VacationTraveling SalesmenThe ReturnBen FranklinPhyllis' WeddingBusiness SchoolCocktailsThe NegotiationSafety TrainingProduct RecallWomen's AppreciationBeach GamesThe JobFun RunDunder Mifflin InfinityLaunch PartyMoneyLocal AdBranch WarsSurvivor ManThe DepositionDinner PartyChair ModelNight OutDid I Stutter?Job FairGoodbye, TobyWeight LossBusiness EthicsBaby ShowerCrime AidEmployee TransferCustomer SurveyBusiness TripFrame TobyThe SurplusMoroccan ChristmasThe DuelPrince Family PaperStress ReliefLecture Circuit: Part 1Lecture Circuit: Part 2Blood DriveGolden TicketNew BossTwo WeeksDream TeamMichael Scott Paper CompanyHeavy CompetitionBrokeCasual FridayCafe DiscoCompany PicnicGossipThe MeetingThe PromotionNiagara: Part 1Niagara: Part 2MafiaThe LoverKoi PondDouble DateMurderShareholder MeetingScott's TotsSecret SantaThe BankerSabreManager and SalesmanThe Delivery: Part 1The Delivery: Part 2St. Patrick's DayNew LeadsHappy HourSecretary's DayBody LanguageThe Cover-UpThe ChumpWhistleblowerNepotismCounselingAndy's PlaySex EdThe StingCostume ContestChristeningViewing PartyWUPHF.comChinaClassy ChristmasUltimatumThe SeminarThe SearchPDAThreat Level MidnightTodd PackerGarage SaleTraining DayMichael's Last DundiesGoodbye, MichaelThe Inner CircleDwight K. Schrute, (Acting) ManagerSearch CommitteeThe ListThe IncentiveLottoGarden PartySpookedDoomsdayPam's ReplacementGettysburgMrs. CaliforniaChristmas WishesTriviaPool PartyJury DutySpecial ProjectTallahasseeAfter HoursTest the StoreLast Day in FloridaGet the GirlWelcome PartyAngry AndyFundraiserTurf WarFree Family Portrait StudioNew GuysRoy's WeddingAndy's AncestryWork BusHere Comes TrebleThe BoatThe WhaleThe TargetDwight ChristmasLiceSuit WarehouseCustomer LoyaltyJunior SalesmanVandalismCouples DiscountMoving OnThe FarmPromosStairmageddonPaper AirplaneLivin' the DreamA.A.R.M.Finale\"\n '2005-03-242005-03-292005-04-052005-04-122005-04-192005-04-262005-09-202005-09-272005-10-042005-10-112005-10-182005-11-012005-11-082005-11-152005-11-222005-12-062006-01-052006-01-122006-01-192006-01-262006-02-022006-02-092006-03-022006-03-162006-03-302006-04-272006-05-042006-05-112006-09-212006-09-282006-10-052006-10-122006-10-192006-11-022006-11-092006-11-162006-11-302006-12-142007-01-042007-01-112007-01-182007-02-012007-02-082007-02-152007-02-222007-04-052007-04-122007-04-262007-05-032007-05-102007-05-172007-09-272007-10-042007-10-112007-10-182007-10-252007-11-012007-11-082007-11-152008-04-102008-04-172008-04-242008-05-012008-05-082008-05-152008-09-252008-10-092008-10-162008-10-232008-10-302008-11-062008-11-132008-11-202008-12-042008-12-112009-01-152009-01-222009-02-012009-02-052009-02-122009-03-052009-03-122009-03-192009-03-262009-04-092009-04-092009-04-162009-04-232009-04-302009-05-072009-05-142009-09-172009-09-242009-10-012009-10-082009-10-082009-10-152009-10-222009-10-292009-11-052009-11-122009-11-192009-12-032009-12-102010-01-212010-02-042010-02-112010-03-042010-03-042010-03-112010-03-182010-03-252010-04-222010-04-292010-05-062010-05-132010-05-202010-09-232010-09-302010-10-072010-10-142010-10-212010-10-282010-11-042010-11-112010-11-182010-12-022010-12-092011-01-202011-01-272011-02-032011-02-102011-02-172011-02-242011-03-242011-04-142011-04-212011-04-282011-05-052011-05-122011-05-192011-09-222011-09-292011-10-062011-10-132011-10-272011-11-032011-11-102011-11-172011-12-012011-12-082012-01-122012-01-192012-02-022012-02-092012-02-162012-02-232012-03-012012-03-082012-03-152012-04-122012-04-192012-04-262012-05-032012-05-102012-09-202012-09-272012-10-042012-10-182012-10-252012-11-082012-11-152012-11-292012-12-062013-01-102013-01-172013-01-242013-01-312013-01-312013-02-072013-02-142013-03-142013-04-042013-04-112013-04-252013-05-022013-05-092013-05-16'] to numeric\n\n\nNote the error triggered above due to pandas attempting to calculate the mean of the wrong type (i.e. non-numeric values). We can address that by only computing the mean of numeric values (see below):\n\ndf.mean(numeric_only=True)\n\nseason            5.468085\nepisode          11.877660\nimdb_rating       8.257447\ntotal_votes    2126.648936\ndtype: float64\n\n\nor the sum of every value within the same column:\n\ndf.sum()\n\nseason                                                      1028\nepisode                                                     2233\ntitle          PilotDiversity DayHealth CareThe AllianceBaske...\nimdb_rating                                               1552.4\ntotal_votes                                               399810\nair_date       2005-03-242005-03-292005-04-052005-04-122005-0...\ndtype: object\n\n\nSimilarly to what happened with mean(), sum() is adding all values in every observation of every attribute, regardless of their type, but this time is not producing an error. Can you see what happens with strings? And with dates?\nAgain, we can force to use numeric values only:\n\ndf.sum(numeric_only=True)\n\nseason           1028.0\nepisode          2233.0\nimdb_rating      1552.4\ntotal_votes    399810.0\ndtype: float64",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab: Pandas</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html#subsetting",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html#subsetting",
    "title": "6  Lab: Pandas",
    "section": "6.6 Subsetting",
    "text": "6.6 Subsetting\nOften times we may have a large dataset and we only need to work with just a part of it (a subset) consisting of certain columns and/or rows. Selecting specific columns and/or rows is known as subsetting.\n\n6.6.1 Selecting columns\nBecause in pandas every column has a name, we can select columns by their name or their position.\n\n6.6.1.1 Selecting by name\nTo select by name we will use the syntax df['&lt;column_name&gt;']. For example, if we wanted to select the ratings:\n\ndf['imdb_rating']\n\n0      7.6\n1      8.3\n2      7.9\n3      8.1\n4      8.4\n      ... \n183    8.0\n184    8.0\n185    8.9\n186    9.3\n187    9.7\nName: imdb_rating, Length: 188, dtype: float64\n\n\nor we could select the date in which the chapters were first aired:\n\ndf['air_date']\n\n0      2005-03-24\n1      2005-03-29\n2      2005-04-05\n3      2005-04-12\n4      2005-04-19\n          ...    \n183    2013-04-11\n184    2013-04-25\n185    2013-05-02\n186    2013-05-09\n187    2013-05-16\nName: air_date, Length: 188, dtype: object\n\n\nWe can even select more than one column!\n\ndf[['imdb_rating', 'total_votes']]\n\n\n\n\n\n\n\n\nimdb_rating\ntotal_votes\n\n\n\n\n0\n7.6\n3706\n\n\n1\n8.3\n3566\n\n\n2\n7.9\n2983\n\n\n3\n8.1\n2886\n\n\n4\n8.4\n3179\n\n\n...\n...\n...\n\n\n183\n8.0\n1484\n\n\n184\n8.0\n1482\n\n\n185\n8.9\n2041\n\n\n186\n9.3\n2860\n\n\n187\n9.7\n7934\n\n\n\n\n188 rows × 2 columns\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDid you notice that we used two sets of squared brackets ([[]])? This is needed because we need to passing a list of the column names to the __getitem__ method of the pandas dataframe object, and as you may remember from Section 2.3.2, this is the syntax used for lists (thank this stackoverflow question).\nThis is what we’d get otherwise:\n\ndf['imdb_rating', 'total_votes']\n\nKeyError: ('imdb_rating', 'total_votes')\n\n\nYou can also check out the pandas documentation on indexing and selecting data.\n\n\nWe can also apply methods to subset, such as this one to get the average rating:\n\ndf['imdb_rating'].mean()\n\n8.25744680851064\n\n\nOr to calculate the total number of votes:\n\ndf['total_votes'].sum()\n\n399810\n\n\nOr a combination of multiple columns:\n\ndf[['imdb_rating', 'total_votes']].mean()\n\nimdb_rating       8.257447\ntotal_votes    2126.648936\ndtype: float64\n\n\n\n\n6.6.1.2 Selecting by position\nIf we do not want to use column names, we can use iloc method by using the syntax &lt;object&gt;.iloc[&lt;row slice&gt;, &lt;column slice&gt;], where a slice is a range of numbers separated by a colon :. So, if we were to select the value in the 4th row and 2nd column, we’d use:\n\ndf.iloc[4,2]\n\n'Basketball'\n\n\nBut if we just wanted to select a column? In that case, we can use the same method but instead of specifiying a row, we will need to use : to indicate that we are selecting all the rows, such as:\n\ndf.iloc[:,2]\n\n0                 Pilot\n1         Diversity Day\n2           Health Care\n3          The Alliance\n4            Basketball\n             ...       \n183       Stairmageddon\n184      Paper Airplane\n185    Livin' the Dream\n186            A.A.R.M.\n187              Finale\nName: title, Length: 188, dtype: object\n\n\nConversely, if we just wanted to select all the columns from a given row, we’d use : on the right side of the , like this:\n\ndf.iloc[4,:]\n\nseason                  1\nepisode                 5\ntitle          Basketball\nimdb_rating           8.4\ntotal_votes          3179\nair_date       2005-04-19\nName: 4, dtype: object\n\n\nWe can use negative values in indexes to indicate ‘from the end’. So, an index of [-10, :] returns the 10th from last row.\n\ndf.iloc[-10,:]\n\nseason                  9\nepisode                14\ntitle           Vandalism\nimdb_rating           7.6\ntotal_votes          1402\nair_date       2013-01-31\nName: 178, dtype: object\n\n\nInstead of using tail, we could ask for the last 5 rows with an index of [-5:, :]. I read : as ‘and everything else’ in these cases.\n\ndf.iloc[-5:,:]\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n183\n9\n19\nStairmageddon\n8.0\n1484\n2013-04-11\n\n\n184\n9\n20\nPaper Airplane\n8.0\n1482\n2013-04-25\n\n\n185\n9\n21\nLivin' the Dream\n8.9\n2041\n2013-05-02\n\n\n186\n9\n22\nA.A.R.M.\n9.3\n2860\n2013-05-09\n\n\n187\n9\n23\nFinale\n9.7\n7934\n2013-05-16\n\n\n\n\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n183\n9\n19\nStairmageddon\n8.0\n1484\n2013-04-11\n\n\n184\n9\n20\nPaper Airplane\n8.0\n1482\n2013-04-25\n\n\n185\n9\n21\nLivin' the Dream\n8.9\n2041\n2013-05-02\n\n\n186\n9\n22\nA.A.R.M.\n9.3\n2860\n2013-05-09\n\n\n187\n9\n23\nFinale\n9.7\n7934\n2013-05-16\n\n\n\n\n\n\n\nNote that the row is shown on the left. That will stop you getting lost in slices of the data.\nFor the top ten rows\n\ndf.iloc[:10,:]\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n\n\n5\n1\n6\nHot Girl\n7.8\n2852\n2005-04-26\n\n\n6\n2\n1\nThe Dundies\n8.7\n3213\n2005-09-20\n\n\n7\n2\n2\nSexual Harassment\n8.2\n2736\n2005-09-27\n\n\n8\n2\n3\nOffice Olympics\n8.4\n2742\n2005-10-04\n\n\n9\n2\n4\nThe Fire\n8.4\n2713\n2005-10-11\n\n\n\n\n\n\n\nOf course, we can run methods on these slices. We could, if we wanted to, calculate the mean imdb rating of only the first and last 100 episodes. Note the indexing starts at 0 so we want the column index of 3 (0:season, 1:episode, 2:title, 3:imdb_rating).\n\ndf.iloc[:100,3].mean()\n\n8.483\n\n\n\ndf.iloc[-100:,3].mean()\n\n8.062\n\n\nIf you are unsure how many rows you have then the count method comes to the rescue.\n\ndf.iloc[-100:,3].count()\n\n100\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nseason\nepisode\nimdb_rating\ntotal_votes\n\n\n\n\ncount\n188.000000\n188.000000\n188.000000\n188.000000\n\n\nmean\n5.468085\n11.877660\n8.257447\n2126.648936\n\n\nstd\n2.386245\n7.024855\n0.538067\n787.098275\n\n\nmin\n1.000000\n1.000000\n6.700000\n1393.000000\n\n\n25%\n3.000000\n6.000000\n7.900000\n1631.500000\n\n\n50%\n6.000000\n11.500000\n8.200000\n1952.500000\n\n\n75%\n7.250000\n18.000000\n8.600000\n2379.000000\n\n\nmax\n9.000000\n26.000000\n9.700000\n7934.000000\n\n\n\n\n\n\n\nSo it looks like the last 100 episodes were less good than the first 100. I guess that is why it was cancelled.\nOur data is organised by season. Looking at the average by season might help.\n\ndf[['season', 'imdb_rating']].groupby('season').mean()\n\n\n\n\n\n\n\n\nimdb_rating\n\n\nseason\n\n\n\n\n\n1\n8.016667\n\n\n2\n8.436364\n\n\n3\n8.573913\n\n\n4\n8.600000\n\n\n5\n8.492308\n\n\n6\n8.219231\n\n\n7\n8.316667\n\n\n8\n7.666667\n\n\n9\n7.956522\n\n\n\n\n\n\n\nThe above line groups our dataframe by values in the season column and then displays the mean for each group. Pretty nifty.\nSeason 8 looks pretty bad. We can look at just the rows for season 8.\n\ndf[df['season'] == 8]\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n141\n8\n1\nThe List\n8.2\n1829\n2011-09-22\n\n\n142\n8\n2\nThe Incentive\n8.2\n1668\n2011-09-29\n\n\n143\n8\n3\nLotto\n7.3\n1601\n2011-10-06\n\n\n144\n8\n4\nGarden Party\n8.1\n1717\n2011-10-13\n\n\n145\n8\n5\nSpooked\n7.6\n1543\n2011-10-27\n\n\n146\n8\n6\nDoomsday\n7.8\n1476\n2011-11-03\n\n\n147\n8\n7\nPam's Replacement\n7.7\n1563\n2011-11-10\n\n\n148\n8\n8\nGettysburg\n7.0\n1584\n2011-11-17\n\n\n149\n8\n9\nMrs. California\n7.7\n1553\n2011-12-01\n\n\n150\n8\n10\nChristmas Wishes\n8.0\n1547\n2011-12-08\n\n\n151\n8\n11\nTrivia\n7.9\n1488\n2012-01-12\n\n\n152\n8\n12\nPool Party\n8.0\n1612\n2012-01-19\n\n\n153\n8\n13\nJury Duty\n7.5\n1478\n2012-02-02\n\n\n154\n8\n14\nSpecial Project\n7.8\n1432\n2012-02-09\n\n\n155\n8\n15\nTallahassee\n7.9\n1522\n2012-02-16\n\n\n156\n8\n16\nAfter Hours\n8.1\n1567\n2012-02-23\n\n\n157\n8\n17\nTest the Store\n7.8\n1478\n2012-03-01\n\n\n158\n8\n18\nLast Day in Florida\n7.8\n1429\n2012-03-08\n\n\n159\n8\n19\nGet the Girl\n6.7\n1642\n2012-03-15\n\n\n160\n8\n20\nWelcome Party\n7.2\n1489\n2012-04-12\n\n\n161\n8\n21\nAngry Andy\n7.1\n1585\n2012-04-19\n\n\n162\n8\n22\nFundraiser\n7.1\n1453\n2012-04-26\n\n\n163\n8\n23\nTurf War\n7.7\n1393\n2012-05-03\n\n\n164\n8\n24\nFree Family Portrait Studio\n7.8\n1464\n2012-05-10\n\n\n\n\n\n\n\n\n\n\n6.6.2 Filtering rows\nWe can filter rows matching some criteria by using the syntax &lt;object&gt;.loc[&lt;criteria&gt;]. So, if we wanted to filter all the episodes from the 8th season, we would do the following:\n\ndf.loc[df['season'] == 8]\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n141\n8\n1\nThe List\n8.2\n1829\n2011-09-22\n\n\n142\n8\n2\nThe Incentive\n8.2\n1668\n2011-09-29\n\n\n143\n8\n3\nLotto\n7.3\n1601\n2011-10-06\n\n\n144\n8\n4\nGarden Party\n8.1\n1717\n2011-10-13\n\n\n145\n8\n5\nSpooked\n7.6\n1543\n2011-10-27\n\n\n146\n8\n6\nDoomsday\n7.8\n1476\n2011-11-03\n\n\n147\n8\n7\nPam's Replacement\n7.7\n1563\n2011-11-10\n\n\n148\n8\n8\nGettysburg\n7.0\n1584\n2011-11-17\n\n\n149\n8\n9\nMrs. California\n7.7\n1553\n2011-12-01\n\n\n150\n8\n10\nChristmas Wishes\n8.0\n1547\n2011-12-08\n\n\n151\n8\n11\nTrivia\n7.9\n1488\n2012-01-12\n\n\n152\n8\n12\nPool Party\n8.0\n1612\n2012-01-19\n\n\n153\n8\n13\nJury Duty\n7.5\n1478\n2012-02-02\n\n\n154\n8\n14\nSpecial Project\n7.8\n1432\n2012-02-09\n\n\n155\n8\n15\nTallahassee\n7.9\n1522\n2012-02-16\n\n\n156\n8\n16\nAfter Hours\n8.1\n1567\n2012-02-23\n\n\n157\n8\n17\nTest the Store\n7.8\n1478\n2012-03-01\n\n\n158\n8\n18\nLast Day in Florida\n7.8\n1429\n2012-03-08\n\n\n159\n8\n19\nGet the Girl\n6.7\n1642\n2012-03-15\n\n\n160\n8\n20\nWelcome Party\n7.2\n1489\n2012-04-12\n\n\n161\n8\n21\nAngry Andy\n7.1\n1585\n2012-04-19\n\n\n162\n8\n22\nFundraiser\n7.1\n1453\n2012-04-26\n\n\n163\n8\n23\nTurf War\n7.7\n1393\n2012-05-03\n\n\n164\n8\n24\nFree Family Portrait Studio\n7.8\n1464\n2012-05-10\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the criteria\n\n\n\nTo understand why we have to write the name of the dataframe twice, we can focus on the output provided by the filtering criteria only:\n\ndf['season'] == 9\n\n0      False\n1      False\n2      False\n3      False\n4      False\n       ...  \n183     True\n184     True\n185     True\n186     True\n187     True\nName: season, Length: 188, dtype: bool\n\n\nAs you can see, it returns a boolean serie specifiying which rows are matching the criteria (True) and which ones are not (False)\nAs a side note, while writing the name of the dataframe twice may seem redundant, this means that we could filter rows based on other objects.\n\n\nWe can get an overview of the rating of all chapters within season 8 by:\n\ndf.loc[df['season'] == 8, 'imdb_rating'].describe()\n\ncount    24.000000\nmean      7.666667\nstd       0.405041\nmin       6.700000\n25%       7.450000\n50%       7.800000\n75%       7.925000\nmax       8.200000\nName: imdb_rating, dtype: float64\n\n\nGenerally pretty bad, but there is clearly one very disliked episode.",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab: Pandas</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html#adding-columns",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html#adding-columns",
    "title": "6  Lab: Pandas",
    "section": "6.7 Adding columns",
    "text": "6.7 Adding columns\nWe can add new columns pretty simply.\n\ndf['x'] = 44\ndf.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\nx\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n44\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n44\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n44\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n44\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n44\n\n\n\n\n\n\n\nOur new column can be an operation on other columns\n\ndf['rating_div_total_votes'] = df['imdb_rating'] / df['total_votes']\ndf.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\nx\nrating_div_total_votes\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n44\n0.002051\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n44\n0.002328\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n44\n0.002648\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n44\n0.002807\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n44\n0.002642\n\n\n\n\n\n\n\nor as simple as adding one to every value.\n\ndf['y'] = df['season'] + 1\ndf.iloc[0:5,:]\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\nx\nrating_div_total_votes\ny\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n44\n0.002051\n2\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n44\n0.002328\n2\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n44\n0.002648\n2\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n44\n0.002807\n2\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n44\n0.002642\n2\n\n\n\n\n\n\n\n\ny =  df['season'] + 1",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab: Pandas</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html#writing-data",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html#writing-data",
    "title": "6  Lab: Pandas",
    "section": "6.8 Writing data",
    "text": "6.8 Writing data\nPandas supports writing out data frames to various formats.\n\n?df.to_csv\n\nNow you can uncomment the code below to save your dataframe into a csv file. But before doing so, check that your data/output folder is empty, as it would override its content:\n\ndf.to_csv('data/output/my_output_ratings.csv', encoding='UTF-8')\n\nLikewise, we could export our dataset to an excel file by using to_excel:\n\n?df.to_excel\n\nNow you can uncomment the code below to save your dataframe into an excel file. But before doing so, check that your data/output folder is empty:\n\n# df.to_excel('data/output/my_output_ratings.xlsx')",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab: Pandas</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html#combining-datasets",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html#combining-datasets",
    "title": "6  Lab: Pandas",
    "section": "6.9 Combining datasets",
    "text": "6.9 Combining datasets\nIn this notebook, our dataset was created from a single file that contained all the data that was needed. However, often times data will be spread into different files that we will need to combine to create our own dataset.\nConsider the two dataframes below:\n\ndf_1 = pd.read_csv('data/raw/office1.csv', encoding='UTF-8')\ndf_2 = pd.read_csv('data/raw/office2.csv', encoding='UTF-8')\n\n\ndf_1.head()\n\n\n\n\n\n\n\n\nid\nseason\nepisode\nimdb_rating\n\n\n\n\n0\n5-1\n5\n1\n8.8\n\n\n1\n9-13\n9\n13\n7.7\n\n\n2\n5-6\n5\n6\n8.5\n\n\n3\n3-23\n3\n23\n9.3\n\n\n4\n9-16\n9\n16\n8.2\n\n\n\n\n\n\n\n\ndf_2.head()\n\n\n\n\n\n\n\n\nid\ntotal_votes\n\n\n\n\n0\n4-10\n2095\n\n\n1\n3-21\n2403\n\n\n2\n7-24\n2040\n\n\n3\n6-18\n1769\n\n\n4\n8-8\n1584\n\n\n\n\n\n\n\nAs can be seen, the total votes and imdb ratings data are split between files that we will need to combine. Usually this is done by using a shared column between the two datasets that works as an index. Gladly, head() reveals that in both cases there is a common column called id. We can join the two dataframes together using the common column.\n\ninner_join_office_df = pd.merge(df_1, df_2, on='id', how='inner')\ninner_join_office_df\n\n\n\n\n\n\n\n\nid\nseason\nepisode\nimdb_rating\ntotal_votes\n\n\n\n\n0\n5-1\n5\n1\n8.8\n2501\n\n\n1\n9-13\n9\n13\n7.7\n1394\n\n\n2\n5-6\n5\n6\n8.5\n2018\n\n\n3\n3-23\n3\n23\n9.3\n3010\n\n\n4\n9-16\n9\n16\n8.2\n1572\n\n\n...\n...\n...\n...\n...\n...\n\n\n183\n5-21\n5\n21\n8.7\n2032\n\n\n184\n2-13\n2\n13\n8.3\n2363\n\n\n185\n9-6\n9\n6\n7.8\n1455\n\n\n186\n2-2\n2\n2\n8.2\n2736\n\n\n187\n3-4\n3\n4\n8.0\n2311\n\n\n\n\n188 rows × 5 columns\n\n\n\nIn this way you can combine datasets using common columns and an inner join. We will leave that for the moment. If you want more information about merging data then see this page and the pandas documentation.\n\n6.9.1 Well done!\nWell done! You’ve reached the end of a pretty long notebook that went through a lot of details about how to work with this pretty critical package called Pandas. We haven’t really done a lot of detailed analysis in this one yet but we will use Pandas a lot and frequently.\nYour best friend will be the Pandas documentation – https://pandas.pydata.org/docs/index.html\nThis documentation is great. We particuarly recomment the User Guide that will answer most of your questions and will give you a lot of code to copy and paste first and then modify to do what you need to do – https://pandas.pydata.org/docs/user_guide/index.html#user-guide",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab: Pandas</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_2.html",
    "href": "content/labs/Lab_2/IM939_Lab_2_2.html",
    "title": "7  Lab: Visualising Data",
    "section": "",
    "text": "7.1 Starting\nAs usual, we will be starting by loading the required libraries and dataset(s):\nimport pandas as pd\n\n# This following is a small change that will make sure that the plots are created in a particular \"style\". \n# The following will mimic the famous \"ggplot\" style that is popular in the R coding realm.\n# There are many other default styles -- https://matplotlib.org/stable/gallery/style_sheets/index.html\n# feel free to change 'ggplot' in the line below and recreate the plots, see how things look\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\ndf = pd.read_csv('data/raw/office_ratings.csv', encoding='UTF-8')\n# Check our dataset.\ndf.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lab: Visualising Data</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_2.html#univariate-plots---a-single-variable",
    "href": "content/labs/Lab_2/IM939_Lab_2_2.html#univariate-plots---a-single-variable",
    "title": "7  Lab: Visualising Data",
    "section": "7.2 Univariate plots - a single variable",
    "text": "7.2 Univariate plots - a single variable\nUnivariate plots are a great way to see trends. We will create them using plot method.\n\n# Read the documentation to understand how to use plot.\n?df.plot\n\nWe can use plot to quicky visualise every variable1 in the dataset.\n1 Actually this may not be accurate. Are you missing any column? Can you guess why are they missing?\n# Create a plot for every variable in the dataset\ndf.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nSome of the variables look rather funny. It’s not a great idea to work with just the defaults here where all the variables are embedded in a single plot.\nWe can look at a specific column and that is always more helpful.\n\n# Visualise a column and add a title.\ndf['total_votes'].plot(title='Total Votes')\n\n&lt;Axes: title={'center': 'Total Votes'}&gt;\n\n\n\n\n\n\n\n\n\n\n7.2.1 Subplots\nWe can also create subplots, where every variable is plotted individually and put next to the others, while sharing the axis.\n\ndf.plot(subplots=True)\n\narray([&lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;], dtype=object)\n\n\n\n\n\n\n\n\n\nSeason and episode is not at all informative here.\nThink about why. Why are these plots less useful? The “defaults” – in this case, simply running the plot() function can only get you so far. You will need to have a good reason why you want to create a plot and what you expect to learn from them. And the next thing to ask is what kind of plot is most useful?\n\ndf[['imdb_rating', 'total_votes']].plot(subplots=True)\n\narray([&lt;Axes: &gt;, &lt;Axes: &gt;], dtype=object)\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 Histograms\nOften times, instead of seeing the actual values we may be interested in seeing how they are distributed. This is known as a histogram, and we can create them vy changing the plot type using the kind argument:\n\ndf[['imdb_rating', 'total_votes']].plot(subplots=True, kind='hist')\n\narray([&lt;Axes: ylabel='Frequency'&gt;, &lt;Axes: ylabel='Frequency'&gt;],\n      dtype=object)\n\n\n\n\n\n\n\n\n\nUnfortunatly, since subplots share axes, our x axis is bunched up. The above tells us that the all our IMDB ratings are between 0 and a little less than 1000… not useful.\nProbably best to plot them individually.\n\ndf[['imdb_rating']].plot(kind='hist', title = \"Ratings per episode\")\n\n&lt;Axes: title={'center': 'Ratings per episode'}, ylabel='Frequency'&gt;\n\n\n\n\n\n\n\n\n\nQuite a sensible gaussian shape (a central point with the frequency decreasing symmetrically).\n\ndf[['total_votes']].plot(kind='hist', title= \"Total votes per chapter\")\n\n&lt;Axes: title={'center': 'Total votes per chapter'}, ylabel='Frequency'&gt;\n\n\n\n\n\n\n\n\n\nA positively skewed distribution - many smaller values and very few high values.",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lab: Visualising Data</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_2.html#sec-dv-bibariate",
    "href": "content/labs/Lab_2/IM939_Lab_2_2.html#sec-dv-bibariate",
    "title": "7  Lab: Visualising Data",
    "section": "7.3 Bivariate - Two variables",
    "text": "7.3 Bivariate - Two variables\nSometimes, instead of visualising variables individually, we may want to see them in relation to others, so we can identify possible correlations. Scatter plots are simple ways to explore the relationship between two variables, where one is displayed on the X axis and the other one in the Y axis.\nWe may want to see if the number of votes and the imdb rating are not independent events. In other words, we want to know if these two data variables are related. We will be creating a scatterplot using the following syntax: &lt;object&gt;.plot(x = \"&lt;variable1&gt;\", y = \"&lt;variable2&gt;\", kind = \"scatter\")\n\n# Create a scatterplot\ndf.plot(x='imdb_rating', y='total_votes', kind='scatter', title='IMDB ratings and total number of votes')\n\n&lt;Axes: title={'center': 'IMDB ratings and total number of votes'}, xlabel='imdb_rating', ylabel='total_votes'&gt;\n\n\n\n\n\n\n\n\n\nThat is really interesting. The episodes with the highest rating also have the greatest number of votes. There was a cleary a great outpouring of happiness there.\n\n\n\n\n\n\nWhich episodes were the most voted?\n\n\n\n\n\nAs seen previously, we could filter our dataset:\n\ndf[df['total_votes'] &gt; 5000]\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n77\n5\n13\nStress Relief\n9.6\n5948\n2009-02-01\n\n\n137\n7\n21\nGoodbye, Michael\n9.7\n5749\n2011-04-28\n\n\n187\n9\n23\nFinale\n9.7\n7934\n2013-05-16\n\n\n\n\n\n\n\nThere are three chapters which received more than 5,000 votes.\nExcellent. We may want to know if there’s any influence of season on the ratings:\n\ndf.plot(x='season', y='imdb_rating', kind='scatter', title='IMDB ratings and season')\n\n&lt;Axes: title={'center': 'IMDB ratings and season'}, xlabel='season', ylabel='imdb_rating'&gt;\n\n\n\n\n\n\n\n\n\nSeason 8 seems to be a bit low. But nothing too extreme.\n\n7.4 Dates\nOur data contains air date information. Currently, that column is object or a string.\n\ndf.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n\n\n\n\n\n\n\n\ndf.dtypes\n\nseason           int64\nepisode          int64\ntitle           object\nimdb_rating    float64\ntotal_votes      int64\nair_date        object\ndtype: object\n\n\nWe know this is not accurate, so we can set this to be datetime instead by using the method datetime. That will help us plot the time series of the data.\n\n# Convert air_date to a date.\ndf['air_date2'] =  pd.to_datetime(df['air_date'])\n\n# Check the result\ndf.dtypes\n\nseason                  int64\nepisode                 int64\ntitle                  object\nimdb_rating           float64\ntotal_votes             int64\nair_date               object\nair_date2      datetime64[ns]\ndtype: object\n\n\n\ndf.plot(x = 'air_date2', y = 'total_votes', kind='scatter')\n\n&lt;Axes: xlabel='air_date2', ylabel='total_votes'&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe importance of using the right data type\n\n\n\nCan you spot any difference when trying to plot a date that is not stored as a date data type?\n\ndf.plot(x = 'air_date', y = 'total_votes', kind='scatter')\n\n&lt;Axes: xlabel='air_date', ylabel='total_votes'&gt;\n\n\n\n\n\n\n\n\n\nRight, this is probably not what we would expect!\n\n\nWe can look at multiple variables using subplots.\n\ndf[['air_date2', 'total_votes', 'imdb_rating']].plot(\n    x = 'air_date2', subplots=True)\n\narray([&lt;Axes: xlabel='air_date2'&gt;, &lt;Axes: xlabel='air_date2'&gt;],\n      dtype=object)\n\n\n\n\n\n\n\n\n\n\n\n7.5 Multivariate\nOur dataset is quite simple. But we can look at two variables (total_votes, imdb_rating) by a third one (season), used as grouping.\n\ndf.groupby('season').plot(\n    kind='scatter', y = 'total_votes', x = 'imdb_rating')\n\nseason\n1    Axes(0.125,0.11;0.775x0.77)\n2    Axes(0.125,0.11;0.775x0.77)\n3    Axes(0.125,0.11;0.775x0.77)\n4    Axes(0.125,0.11;0.775x0.77)\n5    Axes(0.125,0.11;0.775x0.77)\n6    Axes(0.125,0.11;0.775x0.77)\n7    Axes(0.125,0.11;0.775x0.77)\n8    Axes(0.125,0.11;0.775x0.77)\n9    Axes(0.125,0.11;0.775x0.77)\ndtype: object\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is a lot more you can do with plots with Pandas and Matplotlib. A good resource is the visualisation section of the pandas documentation.\n\n\n7.6 How to make good decisions on choosing suitable visualisations?\nThis is a difficult question to cover quickly. We are running whole modules on that as you know. However, it is very important to ask yourself first what you want to see, what are you trying to find out. The next then to think about suitable charts that can help you answer that question and to also think about what the data is, is it a categorical data feature that you are trying to visualise, or a temporal feature that has some time stamps associated with it, or is it just numbers?\nThere are lots of good guidance available to help you navigate these decisions. One of the very useful resources is the Visual Vocabulary of the Financial Times:\n\n\n\n\nFT’s Visual Vocabulary\n\n\n\nSee a high-res version of the graphic above is here: https://github.com/Financial-Times/chart-doctor/tree/main/visual-vocabulary",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lab: Visualising Data</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_2.html#dates",
    "href": "content/labs/Lab_2/IM939_Lab_2_2.html#dates",
    "title": "7  Lab: Visualising Data",
    "section": "7.4 Dates",
    "text": "7.4 Dates\nOur data contains air date information. Currently, that column is object or a string.\n\ndf.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n\n\n\n\n\n\n\n\ndf.dtypes\n\nseason           int64\nepisode          int64\ntitle           object\nimdb_rating    float64\ntotal_votes      int64\nair_date        object\ndtype: object\n\n\nWe know this is not accurate, so we can set this to be datetime instead by using the method datetime. That will help us plot the time series of the data.\n\n# Convert air_date to a date.\ndf['air_date2'] =  pd.to_datetime(df['air_date'])\n\n# Check the result\ndf.dtypes\n\nseason                  int64\nepisode                 int64\ntitle                  object\nimdb_rating           float64\ntotal_votes             int64\nair_date               object\nair_date2      datetime64[ns]\ndtype: object\n\n\n\ndf.plot(x = 'air_date2', y = 'total_votes', kind='scatter')\n\n&lt;Axes: xlabel='air_date2', ylabel='total_votes'&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe importance of using the right data type\n\n\n\nCan you spot any difference when trying to plot a date that is not stored as a date data type?\n\ndf.plot(x = 'air_date', y = 'total_votes', kind='scatter')\n\n&lt;Axes: xlabel='air_date', ylabel='total_votes'&gt;\n\n\n\n\n\n\n\n\n\nRight, this is probably not what we would expect!\n\n\nWe can look at multiple variables using subplots.\n\ndf[['air_date2', 'total_votes', 'imdb_rating']].plot(\n    x = 'air_date2', subplots=True)\n\narray([&lt;Axes: xlabel='air_date2'&gt;, &lt;Axes: xlabel='air_date2'&gt;],\n      dtype=object)",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lab: Visualising Data</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_2.html#multivariate",
    "href": "content/labs/Lab_2/IM939_Lab_2_2.html#multivariate",
    "title": "7  Lab: Visualising Data",
    "section": "7.5 Multivariate",
    "text": "7.5 Multivariate\nOur dataset is quite simple. But we can look at two variables (total_votes, imdb_rating) by a third one (season), used as grouping.\n\ndf.groupby('season').plot(\n    kind='scatter', y = 'total_votes', x = 'imdb_rating')\n\nseason\n1    Axes(0.125,0.11;0.775x0.77)\n2    Axes(0.125,0.11;0.775x0.77)\n3    Axes(0.125,0.11;0.775x0.77)\n4    Axes(0.125,0.11;0.775x0.77)\n5    Axes(0.125,0.11;0.775x0.77)\n6    Axes(0.125,0.11;0.775x0.77)\n7    Axes(0.125,0.11;0.775x0.77)\n8    Axes(0.125,0.11;0.775x0.77)\n9    Axes(0.125,0.11;0.775x0.77)\ndtype: object\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is a lot more you can do with plots with Pandas and Matplotlib. A good resource is the visualisation section of the pandas documentation.",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lab: Visualising Data</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_2.html#how-to-make-good-decisions-on-choosing-suitable-visualisations",
    "href": "content/labs/Lab_2/IM939_Lab_2_2.html#how-to-make-good-decisions-on-choosing-suitable-visualisations",
    "title": "7  Lab: Visualising Data",
    "section": "7.6 How to make good decisions on choosing suitable visualisations?",
    "text": "7.6 How to make good decisions on choosing suitable visualisations?\nThis is a difficult question to cover quickly. We are running whole modules on that as you know. However, it is very important to ask yourself first what you want to see, what are you trying to find out. The next then to think about suitable charts that can help you answer that question and to also think about what the data is, is it a categorical data feature that you are trying to visualise, or a temporal feature that has some time stamps associated with it, or is it just numbers?\nThere are lots of good guidance available to help you navigate these decisions. One of the very useful resources is the Visual Vocabulary of the Financial Times:\n\n\n\n\nFT’s Visual Vocabulary\n\n\n\nSee a high-res version of the graphic above is here: https://github.com/Financial-Times/chart-doctor/tree/main/visual-vocabulary",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lab: Visualising Data</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_3.html",
    "href": "content/labs/Lab_2/IM939_Lab_2_3.html",
    "title": "8  Lab: Missing data",
    "section": "",
    "text": "8.1 Assessing Missing values\nIn this case we will be reading the modified dataset stored in office_ratings_missing.csv where some values have been removed. The first thing we may want to do is to know where and how many of those values have been removed to inform what to do next with them.\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\ndf = pd.read_csv('data/raw/office_ratings_missing.csv', encoding = 'UTF-8')\ndf.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n7.6\nNaN\n24/03/2005\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566.0\n29/03/2005\n\n\n2\n1\n3\nHealth Care\n7.9\n2983.0\n05/04/2005\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886.0\n12/04/2005\n\n\n4\n1\n5\nBasketball\n8.4\n3179.0\n19/04/2005\nDid you notice something weird on the row number 1?\nRegretfully, exploring the head or the tail of the dataframe may not be a good idea, especially in large datasets. We may want to use info instead:\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 188 entries, 0 to 187\nData columns (total 6 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   season       188 non-null    int64  \n 1   episode      188 non-null    int64  \n 2   title        188 non-null    object \n 3   imdb_rating  170 non-null    float64\n 4   total_votes  168 non-null    float64\n 5   air_date     188 non-null    object \ndtypes: float64(2), int64(2), object(2)\nmemory usage: 8.9+ KB\nWe are missing values in our imdb_rating and total_votes columns. Now, we need to know how many values are missing. We can do that in multiple ways. One is to combine the method .isna , which returns either True (1) or False (2), and then sum the values that are true (and thus, are null):\n# Count missing values\ndf.isna().sum()\n\nseason          0\nepisode         0\ntitle           0\nimdb_rating    18\ntotal_votes    20\nair_date        0\ndtype: int64\nAnother method is to get the maximum possible values and substract the sum of existing values:\n# Count missing values\ndf.shape[0] - df.count()\n# df.shape returns the size of the dataframe and count() will only count the rows where there is a value\n\nseason          0\nepisode         0\ntitle           0\nimdb_rating    18\ntotal_votes    20\nair_date        0\ndtype: int64\nNow we have an understanding of where the issues are and how many are there. Now the question is: What to do with missing data?",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lab: Missing data</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_3.html#assessing-missing-values",
    "href": "content/labs/Lab_2/IM939_Lab_2_3.html#assessing-missing-values",
    "title": "8  Lab: Missing data",
    "section": "",
    "text": "Storing missing values\n\n\n\nAt this stage, it is important to understand how Python stores missing values. For python, any missing data will be represented as any of these values: NaN (Not a Number), NA (Not Available) or None. This is to differenciate that with cases where we may see a “gap” in the dataframe that may look like a missing data, but it is an empty string instead (\"\") or a string with a white space (\" \"). These are not considered empty values.",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lab: Missing data</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_3.html#sec-inputting-missing-data",
    "href": "content/labs/Lab_2/IM939_Lab_2_3.html#sec-inputting-missing-data",
    "title": "8  Lab: Missing data",
    "section": "8.2 Inputting values",
    "text": "8.2 Inputting values\nA quick solution is to replace missing values with either 0 or give them a roughtly central value (the mean).\nTo do this we use the fillna method, which fills any missing values (NA -Not Available or NaN )\n\ndf['imdb_rating_with_0'] = df['imdb_rating'].fillna(0)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 188 entries, 0 to 187\nData columns (total 7 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   season              188 non-null    int64  \n 1   episode             188 non-null    int64  \n 2   title               188 non-null    object \n 3   imdb_rating         170 non-null    float64\n 4   total_votes         168 non-null    float64\n 5   air_date            188 non-null    object \n 6   imdb_rating_with_0  188 non-null    float64\ndtypes: float64(3), int64(2), object(2)\nmemory usage: 10.4+ KB\n\n\nThis worked! imdb_rating_with_0 does not have any missing value. However, we are significantly altering some statistical values:\n\n\n\n\n\n\nImportant\n\n\n\nIn the above, we are saving the “fixed” (in more technical terms, “imputed”) data column in imdb_rating in a new column named imdb_rating_with_0. It is a good practice to keep the “original” data and make “copies” of data features where you have made changes. In this way, you can always go back and do things differently and also compare the data sets with and without the intervention (for instance, in the following, we will be able to compare the descriptive statistics of the two columns)\n\ndf[['imdb_rating', 'imdb_rating_with_0']].describe()\n\n\n\n\n\n\n\n\nimdb_rating\nimdb_rating_with_0\n\n\n\n\ncount\n170.000000\n188.000000\n\n\nmean\n8.261176\n7.470213\n\n\nstd\n0.514084\n2.485781\n\n\nmin\n6.800000\n0.000000\n\n\n25%\n7.900000\n7.800000\n\n\n50%\n8.200000\n8.200000\n\n\n75%\n8.600000\n8.500000\n\n\nmax\n9.700000\n9.700000\n\n\n\n\n\n\n\nIn order to try to avoid that, we can fill them with the mean:\n\ndf['imdb_rating_with_mean'] = df['imdb_rating'].fillna(df['imdb_rating'].mean())\n\ndf[['imdb_rating', 'imdb_rating_with_0', 'imdb_rating_with_mean']].describe()\n\n\n\n\n\n\n\n\nimdb_rating\nimdb_rating_with_0\nimdb_rating_with_mean\n\n\n\n\ncount\n170.000000\n188.000000\n188.000000\n\n\nmean\n8.261176\n7.470213\n8.261176\n\n\nstd\n0.514084\n2.485781\n0.488716\n\n\nmin\n6.800000\n0.000000\n6.800000\n\n\n25%\n7.900000\n7.800000\n8.000000\n\n\n50%\n8.200000\n8.200000\n8.261176\n\n\n75%\n8.600000\n8.500000\n8.500000\n\n\nmax\n9.700000\n9.700000\n9.700000\n\n\n\n\n\n\n\nWe can plot these to see what looks most reasonable (you can probably also make an educated guess here).\n\ndf['imdb_rating_with_mean'].plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\ndf['imdb_rating_with_0'].plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nGoing with the mean seems quite sensible in this case. Especially as the data is gaussian so the mean is probably an accurate represenation of the central value.\n\n# Create a histogram\nax = df['imdb_rating'].hist()\n# Plot the histogram and add a vertical line on the mean\nax.axvline(df['imdb_rating'].mean(), color='k', linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x14ddd6e90&gt;\n\n\n\n\n\n\n\n\n\n\n8.3 Transformations\nSome statistical models, such as standard linear regression, require the predicted variable to be gaussian distributed (a single central point and a roughly symmetrical decrease in frequency, see this Wolfram alpha page.\nThe distribution of votes is positively skewed (most values are low).\n\ndf['total_votes'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nA log transformation can make this data closer to a gaussian distributed data variable.\n\n\n\n\n\n\nnumpy Library\n\n\n\nWe are now brining in another library called numpy – see here: https://numpy.org/\nNumpy is a central library that most of data analysis protocols make use of. Pandas makes regular use of Numpy for instance. Several of the underlying data structures such as arrays, indexes we use in Pandas and elsewhere will build on Numpy structures. Don’t worry too much about it for now. We’ll use Numpy more later – Here is a useful user guide to give you an idea: https://numpy.org/doc/stable/user/index.html#user\nFor the log transformation we are going to use log2 method provided by numpy (numerical python).\n\n# This is how we import numpy, usually the convention is to use np as the short variable name and you can access numpy functions by `np.`\nimport numpy as np\n\ndf['total_votes_log'] = np.log2(df['total_votes'])\n\n\ndf['total_votes_log'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nThat is less skewed, but not ideal. Perhaps a square root transformation instead?\n\ndf['total_votes_sqrt'] = np.sqrt(df['total_votes'])\ndf['total_votes_sqrt'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n…well, maybe a inverse/reciprocal transformation. It is possible we have hit the limit on what we can do.\n\ndf['total_votes_recip'] = np.reciprocal(df['total_votes'])\ndf['total_votes_recip'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nAt this point, I think we should conceded that we can make the distribution less positively skewed. However, transformation are not magic and we cannot turn a heavily positively skewed distribution into a normally distributed one.\nOh well.\nWe can calculate z scores though so we can plot both total_votes and imdb_ratings on a single plot. Currently, the IMDB scores vary between 0 and 10 whereas the number of votes number in the thousands.\n\ndf['total_votes_z'] = (df['total_votes'] - df['total_votes'].mean()) / df['total_votes'].std()\ndf['imdb_rating_z'] = (df['imdb_rating'] - df['imdb_rating'].mean()) / df['imdb_rating'].std()\n\n\ndf['total_votes_z'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\ndf['imdb_rating_z'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nNow we can compare the trends in score and number of votes on a single plot.\n\n\n\n\n\n\nWarning\n\n\n\nWe are going to use a slightly different approach to creating the plots. Called to the plot() method from Pandas actually use a library called matplotlib. We are going to use the pyplot module of matplotlib directly.\n\n\n\nimport matplotlib.pyplot as plt\n\nConvert the air_date into a datetime object.\n\ndf['air_date'] =  pd.to_datetime(df['air_date'], dayfirst=True)\n\nThen call the subplots function fom pyplot to create two plots. From this we take the two plot axis (ax1, ax2) and call the method scatter for each to plot imdb_rating_z and total_votes_z.\n\nplt.style.use('ggplot')\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\nax1.scatter( df['air_date'], df['imdb_rating_z'], color = 'red')\nax1.set_title('IMDB rating')\nax2.scatter( df['air_date'], df['total_votes_z'], color = 'blue')\nax2.set_title('Total votes')\n\nText(0.5, 1.0, 'Total votes')\n\n\n\n\n\n\n\n\n\nWe can do better than that.\n\nplt.scatter(df['air_date'], df['imdb_rating_z'], color = 'red', alpha = 0.1)\nplt.scatter(df['air_date'], df['total_votes_z'], color = 'blue', alpha = 0.1)\n\n&lt;matplotlib.collections.PathCollection at 0x14e426290&gt;\n\n\n\n\n\n\n\n\n\nWe have done a lot so far. Exploring data in part 1, plotting data with the inbuilt Pandas methods in part 2 and dealing with both missing data and transfromations in part 3.\nIn part 4, we will look at creating your own functions, a plotting library called seaborn and introduce a larger dataset.",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lab: Missing data</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_3.html#transformations",
    "href": "content/labs/Lab_2/IM939_Lab_2_3.html#transformations",
    "title": "8  Lab: Missing data",
    "section": "8.3 Transformations",
    "text": "8.3 Transformations\nSome statistical models, such as standard linear regression, require the predicted variable to be gaussian distributed (a single central point and a roughly symmetrical decrease in frequency, see this Wolfram alpha page.\nThe distribution of votes is positively skewed (most values are low).\n\ndf['total_votes'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nA log transformation can make this data closer to a gaussian distributed data variable.\n\n\n\n\n\n\nnumpy Library\n\n\n\nWe are now brining in another library called numpy – see here: https://numpy.org/\nNumpy is a central library that most of data analysis protocols make use of. Pandas makes regular use of Numpy for instance. Several of the underlying data structures such as arrays, indexes we use in Pandas and elsewhere will build on Numpy structures. Don’t worry too much about it for now. We’ll use Numpy more later – Here is a useful user guide to give you an idea: https://numpy.org/doc/stable/user/index.html#user\nFor the log transformation we are going to use log2 method provided by numpy (numerical python).\n\n# This is how we import numpy, usually the convention is to use np as the short variable name and you can access numpy functions by `np.`\nimport numpy as np\n\ndf['total_votes_log'] = np.log2(df['total_votes'])\n\n\ndf['total_votes_log'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nThat is less skewed, but not ideal. Perhaps a square root transformation instead?\n\ndf['total_votes_sqrt'] = np.sqrt(df['total_votes'])\ndf['total_votes_sqrt'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n…well, maybe a inverse/reciprocal transformation. It is possible we have hit the limit on what we can do.\n\ndf['total_votes_recip'] = np.reciprocal(df['total_votes'])\ndf['total_votes_recip'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nAt this point, I think we should conceded that we can make the distribution less positively skewed. However, transformation are not magic and we cannot turn a heavily positively skewed distribution into a normally distributed one.\nOh well.\nWe can calculate z scores though so we can plot both total_votes and imdb_ratings on a single plot. Currently, the IMDB scores vary between 0 and 10 whereas the number of votes number in the thousands.\n\ndf['total_votes_z'] = (df['total_votes'] - df['total_votes'].mean()) / df['total_votes'].std()\ndf['imdb_rating_z'] = (df['imdb_rating'] - df['imdb_rating'].mean()) / df['imdb_rating'].std()\n\n\ndf['total_votes_z'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\ndf['imdb_rating_z'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nNow we can compare the trends in score and number of votes on a single plot.\n\n\n\n\n\n\nWarning\n\n\n\nWe are going to use a slightly different approach to creating the plots. Called to the plot() method from Pandas actually use a library called matplotlib. We are going to use the pyplot module of matplotlib directly.\n\n\n\nimport matplotlib.pyplot as plt\n\nConvert the air_date into a datetime object.\n\ndf['air_date'] =  pd.to_datetime(df['air_date'], dayfirst=True)\n\nThen call the subplots function fom pyplot to create two plots. From this we take the two plot axis (ax1, ax2) and call the method scatter for each to plot imdb_rating_z and total_votes_z.\n\nplt.style.use('ggplot')\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\nax1.scatter( df['air_date'], df['imdb_rating_z'], color = 'red')\nax1.set_title('IMDB rating')\nax2.scatter( df['air_date'], df['total_votes_z'], color = 'blue')\nax2.set_title('Total votes')\n\nText(0.5, 1.0, 'Total votes')\n\n\n\n\n\n\n\n\n\nWe can do better than that.\n\nplt.scatter(df['air_date'], df['imdb_rating_z'], color = 'red', alpha = 0.1)\nplt.scatter(df['air_date'], df['total_votes_z'], color = 'blue', alpha = 0.1)\n\n&lt;matplotlib.collections.PathCollection at 0x14e426290&gt;\n\n\n\n\n\n\n\n\n\nWe have done a lot so far. Exploring data in part 1, plotting data with the inbuilt Pandas methods in part 2 and dealing with both missing data and transfromations in part 3.\nIn part 4, we will look at creating your own functions, a plotting library called seaborn and introduce a larger dataset.",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lab: Missing data</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_4.html",
    "href": "content/labs/Lab_2/IM939_Lab_2_4.html",
    "title": "9  Lab: Seaborn",
    "section": "",
    "text": "9.1 Preparations\nAs usual, we need to load any package and data needed for our work.\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\noffice_df = pd.read_csv('data/raw/office_ratings.csv', encoding='UTF-8')\nAnd check our data:\noffice_df.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\nWe can try to replicate the same plots as in the previous notebooks.",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lab: Seaborn</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_4.html#scatterplots",
    "href": "content/labs/Lab_2/IM939_Lab_2_4.html#scatterplots",
    "title": "9  Lab: Seaborn",
    "section": "9.2 Scatterplots",
    "text": "9.2 Scatterplots\nThis is relatively similar to what we did in Section 7.3, but in this case we will be using seaborn’s replot() method.\n\nsns.relplot(x='total_votes', y='imdb_rating', data=office_df)\n\n\n\n\n\n\n\n\n\n9.2.1 Dates\nIf we want to create a scatterplot with dates, we will need to convert them to dates, too:\n\noffice_df['air_date'] =  pd.to_datetime(office_df['air_date'], errors='ignore')\n\ng = sns.relplot(x=\"air_date\", y=\"imdb_rating\", kind=\"scatter\", data=office_df)",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lab: Seaborn</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_4.html#functions",
    "href": "content/labs/Lab_2/IM939_Lab_2_4.html#functions",
    "title": "9  Lab: Seaborn",
    "section": "9.3 Functions",
    "text": "9.3 Functions\nWe can define our own functions. A function helps us with code we are going to run multiple times. For instance, the below function scales values between 0 and 1.\nHere is a modified function from stackoverflow.\n\noffice_df.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n\n\n\n\n\n\n\n\ndef normalize(df, feature_name):\n    result = df.copy()\n    \n    max_value = df[feature_name].max()\n    min_value = df[feature_name].min()\n    \n    result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n    \n    return result\n\nPassing the dataframe and name of the column will return a dataframe with that column scaled between 0 and 1.\n\nnormalize(office_df, 'imdb_rating')\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n0.300000\n3706\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n0.533333\n3566\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n0.400000\n2983\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n0.466667\n2886\n2005-04-12\n\n\n4\n1\n5\nBasketball\n0.566667\n3179\n2005-04-19\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n183\n9\n19\nStairmageddon\n0.433333\n1484\n2013-04-11\n\n\n184\n9\n20\nPaper Airplane\n0.433333\n1482\n2013-04-25\n\n\n185\n9\n21\nLivin' the Dream\n0.733333\n2041\n2013-05-02\n\n\n186\n9\n22\nA.A.R.M.\n0.866667\n2860\n2013-05-09\n\n\n187\n9\n23\nFinale\n1.000000\n7934\n2013-05-16\n\n\n\n\n188 rows × 6 columns\n\n\n\nReplacing the origonal dataframe. We can normalize both out votes and rating.\n\noffice_df = normalize(office_df, 'imdb_rating')\n\n\noffice_df = normalize(office_df, 'total_votes')\n\n\noffice_df\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n0.300000\n0.353616\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n0.533333\n0.332212\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n0.400000\n0.243082\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n0.466667\n0.228253\n2005-04-12\n\n\n4\n1\n5\nBasketball\n0.566667\n0.273047\n2005-04-19\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n183\n9\n19\nStairmageddon\n0.433333\n0.013912\n2013-04-11\n\n\n184\n9\n20\nPaper Airplane\n0.433333\n0.013606\n2013-04-25\n\n\n185\n9\n21\nLivin' the Dream\n0.733333\n0.099067\n2013-05-02\n\n\n186\n9\n22\nA.A.R.M.\n0.866667\n0.224278\n2013-05-09\n\n\n187\n9\n23\nFinale\n1.000000\n1.000000\n2013-05-16\n\n\n\n\n188 rows × 6 columns\n\n\n\n\n9.3.1 Long format\nSeaborn prefers a long format table. Details of melt can be found here.\n\noffice_df_long=pd.melt(office_df, id_vars=['season', 'episode', 'title', 'air_date'], value_vars=['imdb_rating', 'total_votes'])\noffice_df_long\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nair_date\nvariable\nvalue\n\n\n\n\n0\n1\n1\nPilot\n2005-03-24\nimdb_rating\n0.300000\n\n\n1\n1\n2\nDiversity Day\n2005-03-29\nimdb_rating\n0.533333\n\n\n2\n1\n3\nHealth Care\n2005-04-05\nimdb_rating\n0.400000\n\n\n3\n1\n4\nThe Alliance\n2005-04-12\nimdb_rating\n0.466667\n\n\n4\n1\n5\nBasketball\n2005-04-19\nimdb_rating\n0.566667\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n371\n9\n19\nStairmageddon\n2013-04-11\ntotal_votes\n0.013912\n\n\n372\n9\n20\nPaper Airplane\n2013-04-25\ntotal_votes\n0.013606\n\n\n373\n9\n21\nLivin' the Dream\n2013-05-02\ntotal_votes\n0.099067\n\n\n374\n9\n22\nA.A.R.M.\n2013-05-09\ntotal_votes\n0.224278\n\n\n375\n9\n23\nFinale\n2013-05-16\ntotal_votes\n1.000000\n\n\n\n\n376 rows × 6 columns\n\n\n\nWhich we can plot in seaborn like so.\n\nsns.relplot(x='air_date', y='value', size='variable', data=office_df_long)\n\n\n\n\n\n\n\n\n\n?sns.relplot",
    "crumbs": [
      "Thinking Data: Theoretical and Practical Concerns",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lab: Seaborn</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-03.html",
    "href": "content/sessions/session-03.html",
    "title": "10  Introduction",
    "section": "",
    "text": "This week discusses ways of abstracting data. We start by visiting statistics as a means of representing data and its inherent characteristics. The session moves on to discuss the notion of a “model” and visit the different schools of thought within model-ing, as well as a tour of fundamental statistical models that help abstract data and its inherent relations.\nThe practical part explores processing data and data transformations, summarizing data through descriptive statistics, the case of outliers and a brief overview of robust statistics, as well as investigating relations within different aspects of the data and explore concepts such as correlation, regression, and their relevance within different disciplinary frameworks.",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html",
    "href": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html",
    "title": "11  Lab: Data Processing and Summarization",
    "section": "",
    "text": "11.1 Outliers\nIn this section we will be applying several methods to identify outliers. We will be using a custom dataset called accord_sedan.csv that contains cars properties. The dataset is provided as a csv file.",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Lab: Data Processing and Summarization</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#outliers",
    "href": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#outliers",
    "title": "11  Lab: Data Processing and Summarization",
    "section": "",
    "text": "11.1.1 Assess data\nAs usual, we will want to start loading the data and assessing it:\n\n# Loading Data\ndf = pd.read_csv('data/raw/accord_sedan.csv')\n\n# Inspecting the few first rows of the dataframe\ndf.head()\n\n\n\n\n\n\n\n\nprice\nmileage\nyear\ntrim\nengine\ntransmission\n\n\n\n\n0\n14995\n67697\n2006\nex\n4 Cyl\nManual\n\n\n1\n11988\n73738\n2006\nex\n4 Cyl\nManual\n\n\n2\n11999\n80313\n2006\nlx\n4 Cyl\nAutomatic\n\n\n3\n12995\n86096\n2006\nlx\n4 Cyl\nAutomatic\n\n\n4\n11333\n79607\n2006\nlx\n4 Cyl\nAutomatic\n\n\n\n\n\n\n\nAnd we can get some summary statistics, too:\n\ndf.describe()\n\n\n\n\n\n\n\n\nprice\nmileage\nyear\n\n\n\n\ncount\n417.000000\n417.000000\n417.0\n\n\nmean\n12084.242206\n89725.779376\n2006.0\n\n\nstd\n2061.430034\n25957.872271\n0.0\n\n\nmin\n6900.000000\n19160.000000\n2006.0\n\n\n25%\n10779.000000\n71844.000000\n2006.0\n\n\n50%\n11995.000000\n89900.000000\n2006.0\n\n\n75%\n13000.000000\n106705.000000\n2006.0\n\n\nmax\n18995.000000\n149269.000000\n2006.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo this yourself:\n\n\n\nWe only get the summaries for the numeric values here. How about the others, such as trim, engine or transmission? Can you think of a way to get an idea of statistics that will provide you an overview of the distribution of these values? Also, do you think the mean of the year values make sense? What could be a better statistics here?\n\n## One thing to try is the describe function with an \"all\" parameter:\ndf.describe(include='all')\n\n\n\n\n\n\n\n\nprice\nmileage\nyear\ntrim\nengine\ntransmission\n\n\n\n\ncount\n417.000000\n417.000000\n417.0\n417\n417\n417\n\n\nunique\nNaN\nNaN\nNaN\n3\n2\n2\n\n\ntop\nNaN\nNaN\nNaN\nex\n4 Cyl\nAutomatic\n\n\nfreq\nNaN\nNaN\nNaN\n288\n238\n382\n\n\nmean\n12084.242206\n89725.779376\n2006.0\nNaN\nNaN\nNaN\n\n\nstd\n2061.430034\n25957.872271\n0.0\nNaN\nNaN\nNaN\n\n\nmin\n6900.000000\n19160.000000\n2006.0\nNaN\nNaN\nNaN\n\n\n25%\n10779.000000\n71844.000000\n2006.0\nNaN\nNaN\nNaN\n\n\n50%\n11995.000000\n89900.000000\n2006.0\nNaN\nNaN\nNaN\n\n\n75%\n13000.000000\n106705.000000\n2006.0\nNaN\nNaN\nNaN\n\n\nmax\n18995.000000\n149269.000000\n2006.0\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n# But this doesn't address out question. Think about visuals that can help you here? Maybe a histrogram would help?\n\nsns.histplot(data=?, x=\"?\")\n\n\n\nOnce you have an idea of the descriptive statistics, you might want to focus on the outliers. It might be difficult to spot outliers just with the descriptive statistics. This is where we can use data visualisations to identify outliers visually and also make use of a few proxy metrics to help us assess data points.\n\n\n11.1.2 Identify outliers visually\nIn this case we will explore variables individually (1D) by creating a boxplot1. We will visualise the columns price and mileage using matplotlib’s subplots, which combines multiple plots into a single figure:\n1 Boxplots may be difficult to interpret if we are not used to them. It is used to visualise distributions, where the box represents the Interquartile range (IQR), whereas the whiskers can be defined in multiple ways (e.g. the first and third quartiles, 1.5 IQR…) .  From Wikipedia: “In descriptive statistics, a box plot or boxplot is a method for graphically demonstrating the locality, spread and skewness groups of numerical data through their quartiles.”\n# 1D Visualizations\nplt.figure(figsize=(9, 5)) # Setting the figure's size: format width, height (in inches)\nplt.subplot(1,2,1) # subplot(nrows, ncols, index, **kwargs)\nplt.boxplot(df.price)\nplt.title(\"Boxplot of the Price attribute\")\nplt.subplot(1,2,2)\nplt.boxplot(df.mileage)\nplt.title(\"Boxplot of the Mileage attribute\");\n# A semicolon in Python denotes separation, rather than termination. \n# It allows you to write multiple statements on the same line. \n\n\n\n\n\n\n\n\nBut we may want to identify the 2D outliers using a scatterplot\n\n# 2D Visualization\nplt.scatter(df.price, df.mileage, alpha = .5)\nplt.xlabel('Price')\nplt.ylabel('Mileage')\nplt.title('Mileage vs. Price\\n');\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nVisually, outliers appear to be outside the ‘normal’ range of the rest of the points. A few outliers are quite obvious to spot, but the choice of the threshold (the limit after which you decide to label a point as an outlier) visually remains a very subjective matter.\n\n\n\n\n11.1.3 Computing outliers’ threshold: standard deviation\nAdd two new columns to the dataframe called isOutlierPrice and isOutlierMileage. We will define our threshold as 2 times standard deviations away from the mean for price and mileage, respectively.\n\n# Computing the isOutlierPrice column\nupper_threshold_price = df.price.mean() + 2*df.price.std() \nlower_threshold_price = df.price.mean() - 2*df.price.std()\ndf['isOutlierPrice'] = ((df.price &gt; upper_threshold_price) | (df.price &lt; lower_threshold_price))\n\n# Computing the isOutlierMileage column\nupper_threshold_mileage = df.mileage.mean() + 2*df.mileage.std()\nlower_threshold_mileage = df.mileage.mean() - 2*df.mileage.std()\ndf['isOutlierMileage'] = ((df.mileage &gt; upper_threshold_mileage) | (df.mileage &lt; lower_threshold_mileage))\n\n# Inspect the new DataFrame with the added columns\ndf.head()\n\n\n\n\n\n\n\n\nprice\nmileage\nyear\ntrim\nengine\ntransmission\nisOutlierPrice\nisOutlierMileage\n\n\n\n\n0\n14995\n67697\n2006\nex\n4 Cyl\nManual\nFalse\nFalse\n\n\n1\n11988\n73738\n2006\nex\n4 Cyl\nManual\nFalse\nFalse\n\n\n2\n11999\n80313\n2006\nlx\n4 Cyl\nAutomatic\nFalse\nFalse\n\n\n3\n12995\n86096\n2006\nlx\n4 Cyl\nAutomatic\nFalse\nFalse\n\n\n4\n11333\n79607\n2006\nlx\n4 Cyl\nAutomatic\nFalse\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlternative method\n\n\n\nWe may want to use this more succint approach:\n\n# Second way of doing the above using the np.where() function\ndf['isOutlierPrice'] = np.where(abs(df.price - df.price.mean()) &lt; 2*df.price.std(), False, True)\ndf['isOutlierMileage'] = np.where(abs(df.mileage - df.mileage.mean()) &lt; 2*df.mileage.std(), False, True)\n\n# Inspect the new DataFrame with the added columns\ndf.head()\n\n\n\n\n\n\n\n\nprice\nmileage\nyear\ntrim\nengine\ntransmission\nisOutlierPrice\nisOutlierMileage\n\n\n\n\n0\n14995\n67697\n2006\nex\n4 Cyl\nManual\nFalse\nFalse\n\n\n1\n11988\n73738\n2006\nex\n4 Cyl\nManual\nFalse\nFalse\n\n\n2\n11999\n80313\n2006\nlx\n4 Cyl\nAutomatic\nFalse\nFalse\n\n\n3\n12995\n86096\n2006\nlx\n4 Cyl\nAutomatic\nFalse\nFalse\n\n\n4\n11333\n79607\n2006\nlx\n4 Cyl\nAutomatic\nFalse\nFalse\n\n\n\n\n\n\n\nAs we’d expect, both methods produce the same output.\n\n\nNow that we have these two new columns, we could visualize these values with a different color in the plot. Observe whether they are the same as you would mark them:\n\n# Visualizing outliers in a different color\ncol = ['tomato' if i+j else 'seagreen' for i,j in zip(df.isOutlierPrice, df.isOutlierMileage)]\nplt.scatter(df.price, df.mileage, color = col)\nplt.xlabel('Price')\nplt.ylabel('Mileage')\nplt.title('Mileage vs. Price : Outliers 2+ std\\'s away from the mean\\n');\n\n\n\n\n\n\n\n\nVisually filtering out outliers can be an effective tactic if we’re just trying to conduct a quick and dirty experimentation. However, when we need to perform a solid and founded analysis, it’s better to have a robust justification for our choices.\nIn this case, we can use the deviation from the mean to define a threshold that separates ‘normal’ values from ‘outliers’. Here, we opted for a two standard deviation threshold.\nThe mathematical intuition behind this, is that under the normality assumption (if we assume our variable is normally distributed, which it almost is, refer to the next plot), then the probability of it having a value two standard deviations OR MORE away from the mean, is around 5%, which is very unlikely to happen. This is why we label these data points as outliers with respect to the (assumed) probability distribution of the variable. But this remains a way to identify 1D outliers only (identifying outliers within each column separately)\n\n# Histograms of Price and Mileage (checking the normality assumption)\nplt.subplot(1,2,1)\nplt.hist(df.price, bins = 12)\nplt.title('Price')\nplt.subplot(1,2,2)\nplt.hist(df.mileage, bins = 15)\nplt.title('Mileage');\n\n\n\n\n\n\n\n\n\n\n11.1.4 Computing outliers’ threshold: Mahalanobis distance\n\nUsing the 2D Mahalanobis distance to find outliers\n\n\n# Mean vector (computing the mean returns a Series, which we need to convert back to a DataFrame because cdist requires it)\nmean_v = df.iloc[:, 0:2].mean().to_frame().T # DataFrame.T returns the Transpose of the DataFrame\n#mean_v = np.asarray([df.price.mean(), df.mileage.mean() ]).reshape(1,2) # This is a better way of writing the line before (for our use case : cdist function)\n\n# Computing the Mahalanobis distance of each row to the mean vector\nd = cdist(df.iloc[:, 0:2], mean_v, metric='mahalanobis')\n#d = cdist(df[['price', 'mileage']].values, mean_v, metric='mahalanobis') # Another way of writing the line before\n\n# Visualizing the scatter plot while coloring each point (i.e row) with a color from a chosen gradient colormap corresponding to the mahalanobis score\nplt.figure(figsize=(12, 5))\nplt.scatter(df.price, df.mileage, c = d.flatten(), cmap = 'plasma') # in order to know why we use flatten() on d, try printing d with and without flatten\nplt.colorbar() # to show the colorbar\nplt.xlabel('Price')\nplt.ylabel('Mileage')\nplt.title('Mileage vs. Price colored by a 2D Mahalanobis score\\n');",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Lab: Data Processing and Summarization</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#q-q-plots",
    "href": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#q-q-plots",
    "title": "11  Lab: Data Processing and Summarization",
    "section": "11.2 Q-Q Plots",
    "text": "11.2 Q-Q Plots\n\n11.2.1 Data\nIn this case, we will be using data reported by countries to WHO and estimates of tuberculosis burden generated by WHO for the Global Tuberculosis Report, as part of WHO’s Global Tuberculosis Programme. You can refer to this section at WHO’s website to know more about the data and their variables.\nFor your convenience, we have stored a copy in a csv file, but you could try to replicate the code with more up-to-date data from the original website. As usual, it is a best practice to explore the data before proceeding any further:\n\n# Getting the Data\ndf_tuber = pd.read_csv('data/raw/TB_burden_countries_2014-09-29.csv')\n\ndf_tuber.head()\n\n\n\n\n\n\n\n\ncountry\niso2\niso3\niso_numeric\ng_whoregion\nyear\ne_pop_num\ne_prev_100k\ne_prev_100k_lo\ne_prev_100k_hi\n...\ne_inc_tbhiv_100k\ne_inc_tbhiv_100k_lo\ne_inc_tbhiv_100k_hi\ne_inc_tbhiv_num\ne_inc_tbhiv_num_lo\ne_inc_tbhiv_num_hi\nsource_tbhiv\nc_cdr\nc_cdr_lo\nc_cdr_hi\n\n\n\n\n0\nAfghanistan\nAF\nAFG\n4\nEMR\n1990\n11731193\n327.0\n112.0\n655.0\n...\n0.35\n0.22\n0.52\n41.0\n25.0\n60.0\nModel\n20.0\n13.0\n32.0\n\n\n1\nAfghanistan\nAF\nAFG\n4\nEMR\n1991\n12612043\n359.0\n172.0\n613.0\n...\n0.36\n0.19\n0.58\n45.0\n24.0\n73.0\nModel\n97.0\n77.0\n120.0\n\n\n2\nAfghanistan\nAF\nAFG\n4\nEMR\n1992\n13811876\n387.0\n169.0\n693.0\n...\n0.37\n0.19\n0.62\n51.0\n26.0\n86.0\nModel\nNaN\nNaN\nNaN\n\n\n3\nAfghanistan\nAF\nAFG\n4\nEMR\n1993\n15175325\n412.0\n186.0\n724.0\n...\n0.38\n0.20\n0.63\n58.0\n30.0\n95.0\nModel\nNaN\nNaN\nNaN\n\n\n4\nAfghanistan\nAF\nAFG\n4\nEMR\n1994\n16485018\n431.0\n199.0\n751.0\n...\n0.40\n0.21\n0.64\n65.0\n35.0\n100.0\nModel\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 39 columns\n\n\n\n\ndf_tuber.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4903 entries, 0 to 4902\nData columns (total 39 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   country                   4903 non-null   object \n 1   iso2                      4880 non-null   object \n 2   iso3                      4903 non-null   object \n 3   iso_numeric               4903 non-null   int64  \n 4   g_whoregion               4903 non-null   object \n 5   year                      4903 non-null   int64  \n 6   e_pop_num                 4903 non-null   int64  \n 7   e_prev_100k               4892 non-null   float64\n 8   e_prev_100k_lo            4892 non-null   float64\n 9   e_prev_100k_hi            4892 non-null   float64\n 10  e_prev_num                4892 non-null   float64\n 11  e_prev_num_lo             4892 non-null   float64\n 12  e_prev_num_hi             4892 non-null   float64\n 13  e_mort_exc_tbhiv_100k     4902 non-null   float64\n 14  e_mort_exc_tbhiv_100k_lo  4902 non-null   float64\n 15  e_mort_exc_tbhiv_100k_hi  4902 non-null   float64\n 16  e_mort_exc_tbhiv_num      4902 non-null   float64\n 17  e_mort_exc_tbhiv_num_lo   4902 non-null   float64\n 18  e_mort_exc_tbhiv_num_hi   4902 non-null   float64\n 19  source_mort               4902 non-null   object \n 20  e_inc_100k                4902 non-null   float64\n 21  e_inc_100k_lo             4902 non-null   float64\n 22  e_inc_100k_hi             4902 non-null   float64\n 23  e_inc_num                 4902 non-null   float64\n 24  e_inc_num_lo              4902 non-null   float64\n 25  e_inc_num_hi              4902 non-null   float64\n 26  e_tbhiv_prct              3653 non-null   float64\n 27  e_tbhiv_prct_lo           3457 non-null   float64\n 28  e_tbhiv_prct_hi           3457 non-null   float64\n 29  e_inc_tbhiv_100k          3597 non-null   float64\n 30  e_inc_tbhiv_100k_lo       3597 non-null   float64\n 31  e_inc_tbhiv_100k_hi       3597 non-null   float64\n 32  e_inc_tbhiv_num           3597 non-null   float64\n 33  e_inc_tbhiv_num_lo        3597 non-null   float64\n 34  e_inc_tbhiv_num_hi        3597 non-null   float64\n 35  source_tbhiv              4902 non-null   object \n 36  c_cdr                     4476 non-null   float64\n 37  c_cdr_lo                  4476 non-null   float64\n 38  c_cdr_hi                  4476 non-null   float64\ndtypes: float64(30), int64(3), object(6)\nmemory usage: 1.5+ MB\n\n\nAs you can see we have a pretty large dataset numerical and categorical variables, some of which have many missing values. In this case we will be filling missing values with the mean2\n2 Refer to Section 8.2 for a detailed explanation and alternative methods to inputting missing values.\n# Filling missing numeric values ONLY (categorical values will be untouched)\ndf_tuber = df_tuber.fillna(value=df_tuber.mean(numeric_only = True))\n\n# Count missing values\ndf_tuber.isna().sum()\n\ncountry                      0\niso2                        23\niso3                         0\niso_numeric                  0\ng_whoregion                  0\nyear                         0\ne_pop_num                    0\ne_prev_100k                  0\ne_prev_100k_lo               0\ne_prev_100k_hi               0\ne_prev_num                   0\ne_prev_num_lo                0\ne_prev_num_hi                0\ne_mort_exc_tbhiv_100k        0\ne_mort_exc_tbhiv_100k_lo     0\ne_mort_exc_tbhiv_100k_hi     0\ne_mort_exc_tbhiv_num         0\ne_mort_exc_tbhiv_num_lo      0\ne_mort_exc_tbhiv_num_hi      0\nsource_mort                  1\ne_inc_100k                   0\ne_inc_100k_lo                0\ne_inc_100k_hi                0\ne_inc_num                    0\ne_inc_num_lo                 0\ne_inc_num_hi                 0\ne_tbhiv_prct                 0\ne_tbhiv_prct_lo              0\ne_tbhiv_prct_hi              0\ne_inc_tbhiv_100k             0\ne_inc_tbhiv_100k_lo          0\ne_inc_tbhiv_100k_hi          0\ne_inc_tbhiv_num              0\ne_inc_tbhiv_num_lo           0\ne_inc_tbhiv_num_hi           0\nsource_tbhiv                 1\nc_cdr                        0\nc_cdr_lo                     0\nc_cdr_hi                     0\ndtype: int64\n\n\n\nPick one of the columns from the Tuberculosis data and copy it into a numpy array as before\n\n\n# Picking a column (I created a variable for this so I (and you (: ) can modify the column easily here and the change will be carried out everywhere I use the variable colname)\ncolname = 'e_prev_100k'\n\n# Creating a numpy array from our column\ncol = np.array(df_tuber[colname])\n\n# Printing the type of our newly created column\nprint(type(col))\n\n&lt;class 'numpy.ndarray'&gt;\n\n\n\nCompare this selected column to a Normal distribution. Then Sample from a Normal distribution and show a second Q-Q plot\n\n\n# Plotting the Q-Q Plot for our column\nsm.qqplot(col, line='r')\nplt.title('Q-Q Plot of the \"{}\" column of our dataset'.format(colname));\n\n# Plotting the Q-Q Plot for the log of our column\nsm.qqplot(np.log(col), line='r')\nplt.title('Q-Q Plot of the Log of the \"{}\" column'.format(colname));\n\n# Sampling from a Gaussian and a uniform distribution\nsample_norm = np.random.randn(1000)\nsample_unif = np.random.rand(1000)\n\n# Plotting the second Q-Q Plot for our sample (that was generated using a normal distribution)\nsm.qqplot(sample_norm, line='r')\nplt.title('Q-Q Plot of the generated sample (Gaussian)')\nsm.qqplot(sample_unif, line='r')\nplt.title('Q-Q Plot of the generated sample (Uniform)');\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGo ahead and change the colname variable (question 1) into a different column name (that you can pick from the list you have just before question 1 (but do pick a numeric column). And re-execute the code from question 1 and question 2 and you’ll see your new Q-Q plot of the column you just picked.\n\n\nHave a look at the slides from Week 03 for different shapes\n\n\nOk ? Now try to guess the shape of the distribution of our selected column (shape of its histogram) from its Q-Q Plot above.\n\n\nVisualise the column on a histogram and reflect on whether the shape you inferred from Q-Q plots and the shape of the histogram correlate\n\n\n# Histogramming the column we picked (not sure the verb exists though)\nplt.figure(figsize=(12, 5))\nplt.subplot(1,2,1)\nplt.title('Histogram of \"{}\"'.format(colname))\nplt.hist(col)\nplt.subplot(1,2,2)\nplt.title('Histogram of Log \"{}\"'.format(colname))\nplt.hist(np.log(col));\n\n\n\n\n\n\n\n\n\nOf course it does ! From the shape of the Q-Q Plot above (convex, slope upwards) and the Slide of Q-Q Plots from Week 3, we could conclude before looking at the histogram that our distribution was right tailed (or positively skewed if you’re into complex vocabulary lol). And it is !",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Lab: Data Processing and Summarization</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#distributions-sampling",
    "href": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#distributions-sampling",
    "title": "11  Lab: Data Processing and Summarization",
    "section": "11.3 Distributions, Sampling",
    "text": "11.3 Distributions, Sampling\n\nInspecting the effect of sample size on descriptive statistics\n\n\n# Defining a few variables se we can change their values easiliy without having to change the rest of the code\nn = [5, 20, 100, 2000]\nstds = [0.5, 1, 3]\n\n# Initializing empty 2D arrays where we're going to store the results of our simulation\nmean = np.empty([len(n), len(stds)])\nstd = np.empty([len(n), len(stds)])\nskewness = np.empty([len(n), len(stds)])\nkurtos = np.empty([len(n), len(stds)])\n\n# Conducting the experiments and storing the results in the respective 2D arrays\nfor i, sample_size in enumerate(n):\n    for j, theoritical_std in enumerate(stds):\n        sample = np.random.normal(loc=0, scale=theoritical_std, size=sample_size)\n        mean[i,j] = sample.mean()\n        std[i,j] = sample.std()\n        skewness[i,j] = skew(sample)\n        kurtos[i,j] = kurtosis(sample)\n\n# Turning the mean 2D array into a pandas dataframe\nmean = pd.DataFrame(mean, columns = stds, index = n)\nmean = mean.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n\n# Turning the std 2D array into a pandas dataframe\nstd = pd.DataFrame(std, columns = stds, index = n)\nstd = std.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n\n# Turning the skewness 2D array into a pandas dataframe\nskewness = pd.DataFrame(skewness, columns = stds, index = n)\nskewness = skewness.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n\n# Turning the kurtosis 2D array into a pandas dataframe\nkurtos = pd.DataFrame(kurtos, columns = stds, index = n)\nkurtos = kurtos.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n\nprint(\"GAUSSIAN DISTRIBUTION\\n\")\nprint('Results for the Mean :')\nmean # This is a dataframe containing the means of the samples generated with different values of std and sample size\nprint('Results for the Standard Deviation :')\nstd # This is a dataframe containing the standard deviations of the samples generated with different values of std and sample size\nprint('Results for the Skewness :')\nskewness # This is a dataframe containing the skews of the samples generated with different values of std and sample size\nprint('Results for the Kurtosis :')\nkurtos # This is a dataframe containing the kurtosis of the samples generated with different values of std and sample size\n\nGAUSSIAN DISTRIBUTION\n\nResults for the Mean :\nResults for the Standard Deviation :\nResults for the Skewness :\nResults for the Kurtosis :\n\n\n\n\n\n\n\n\nStandard Deviation\n0.5\n1.0\n3.0\n\n\nSample Size\n\n\n\n\n\n\n\n5\n-0.274302\n-0.973889\n-0.611043\n\n\n20\n0.119647\n-0.342947\n-0.299731\n\n\n100\n-0.307998\n-0.396327\n-0.256665\n\n\n2000\n0.065484\n0.110647\n-0.117796\n\n\n\n\n\n\n\n\nBasically, the more data you have (the bigger your sample), the more accurate your empirical estimates are going to be. Observe for example the values of mean (1st DataFrame) and variance (2nd DataFrame) for the 2000 sample size (last row). In the first one, the values are all close to 0, because we generated our sample from a Gaussian with mean 0, and the values in the second one are all close to the values in the column names (which refer to the variance of the distribution of the sample). This means that for with a sample size of 2000, our estimates are really close to the “True” values (with which we generated the sample). Also, the Skew of a Gaussian distribution should be 0, and it is confirmed in the 3rd DataFrame where the values are close to 0 in the last row (i.e big sample size).\n\n2) Same as before but with a Poisson distribution (which has just one parameter lambda instead of 2 like the gaussian)\n\n# Defining a few variables se we can change their values easiliy without having to change the rest of the code\nn = [5, 20, 100, 2000]\nlambd = [0.5, 1, 3] # In a gaussian we had two parameters, a var specified here and a mean we chose to be 0 \n#everywhere. Here we have one parameter called lambda.\n\n# Initializing empty 2D arrays where we're going to store the results of our simulation\nmean = np.empty([len(n), len(lambd)])\nstd = np.empty([len(n), len(lambd)])\nskewness = np.empty([len(n), len(lambd)])\nkurtos = np.empty([len(n), len(lambd)])\n\n# Conducting the experiments and storing the results in the respective 2D arrays\nfor i, sample_size in enumerate(n):\n    for j, theoritical_lambd in enumerate(lambd):\n        #**********************************************************************\n        sample = np.random.poisson(lam = theoritical_lambd, size = sample_size) # THIS IS WHAT WE CHANGED IN Q2 !\n        #**********************************************************************\n        mean[i,j] = sample.mean()\n        std[i,j] = sample.std()\n        skewness[i,j] = skew(sample)\n        kurtos[i,j] = kurtosis(sample)\n\n# Turning the mean 2D array into a pandas dataframe\nmean = pd.DataFrame(mean, columns = lambd, index = n)\nmean = mean.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n\n# Turning the std 2D array into a pandas dataframe\nstd = pd.DataFrame(std, columns = lambd, index = n)\nstd = std.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n\n# Turning the skewness 2D array into a pandas dataframe\nskewness = pd.DataFrame(skewness, columns = lambd, index = n)\nskewness = skewness.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n\n# Turning the kurtosis 2D array into a pandas dataframe\nkurtos = pd.DataFrame(kurtos, columns = lambd, index = n)\nkurtos = kurtos.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n\nprint(\"POISSON DISTRIBUTION\\n\")\nprint('Results for the Mean :')\nmean # This is a dataframe containing the means of the samples generated with different values of std and sample size\nprint('Results for the Standard Deviation :')\nstd # This is a dataframe containing the standard deviations of the samples generated with different values of std \n#and sample size\nprint('Results for the Skewness :')\nskewness # This is a dataframe containing the skews of the samples generated with different values of std and sample \n#size\nprint('Results for the Kurtosis :')\nkurtos # This is a dataframe containing the kurtosis of the samples generated with different values of std and sample \n#size\n\n\nPOISSON DISTRIBUTION\n\nResults for the Mean :\nResults for the Standard Deviation :\nResults for the Skewness :\nResults for the Kurtosis :\n\n\n\n\n\n\n\n\nLambda\n0.5\n1.0\n3.0\n\n\nSample Size\n\n\n\n\n\n\n\n5\n-0.921875\n-1.153061\n-1.335928\n\n\n20\n1.263773\n-0.980104\n-0.362929\n\n\n100\n2.102041\n-0.678583\n0.338336\n\n\n2000\n2.336279\n0.818621\n0.476974\n\n\n\n\n\n\n\n\nJust remember, the lambda parameter that defines the Poisson distribution is also the mean of the distribution. This is confirmed in the first DataFrame where the values (means of samples) are close to the column labels (theoretical lambda which is also equal to theoretical mean), especially in the last row.",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Lab: Data Processing and Summarization</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#robust-statistics",
    "href": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#robust-statistics",
    "title": "11  Lab: Data Processing and Summarization",
    "section": "11.4 Robust Statistics",
    "text": "11.4 Robust Statistics\n\nChoose a number of columns with different shapes, for instance, “e_prev_100k_hi” is left skewed or some columns where the variation is high or you notice potential outliers. You can make use of a series of boxplots to exploratively analyse the data for outliers\n\n\n# Listing the columns\ndf_tuber.columns\n\nIndex(['country', 'iso2', 'iso3', 'iso_numeric', 'g_whoregion', 'year',\n       'e_pop_num', 'e_prev_100k', 'e_prev_100k_lo', 'e_prev_100k_hi',\n       'e_prev_num', 'e_prev_num_lo', 'e_prev_num_hi', 'e_mort_exc_tbhiv_100k',\n       'e_mort_exc_tbhiv_100k_lo', 'e_mort_exc_tbhiv_100k_hi',\n       'e_mort_exc_tbhiv_num', 'e_mort_exc_tbhiv_num_lo',\n       'e_mort_exc_tbhiv_num_hi', 'source_mort', 'e_inc_100k', 'e_inc_100k_lo',\n       'e_inc_100k_hi', 'e_inc_num', 'e_inc_num_lo', 'e_inc_num_hi',\n       'e_tbhiv_prct', 'e_tbhiv_prct_lo', 'e_tbhiv_prct_hi',\n       'e_inc_tbhiv_100k', 'e_inc_tbhiv_100k_lo', 'e_inc_tbhiv_100k_hi',\n       'e_inc_tbhiv_num', 'e_inc_tbhiv_num_lo', 'e_inc_tbhiv_num_hi',\n       'source_tbhiv', 'c_cdr', 'c_cdr_lo', 'c_cdr_hi'],\n      dtype='object')\n\n\n\n# Alright I already know a few columns with outliers but let's try to find them together exploratively using BoxPlots\ncolname = 'c_cdr' # change the column name by choosing different ones from above (numeric ones)\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.hist(df_tuber[colname])\nplt.title('Histogram of \"{}\"'.format(colname))\nplt.subplot(1,2,2)\nplt.boxplot(df_tuber[colname]);\nplt.title('Boxplot of \"{}\"'.format(colname));\n\n\n\n\n\n\n\n\n\n# Chosen columns : I picked 3, feel free to change them and experiment\nchosen_colnames = ['e_pop_num', 'e_prev_100k', 'c_cdr']\n\n2) For the chosen columns, estimate both the conventional and the robust descriptive statistics and compare. Observe how these pairs deviate from each other based on the characteristics of the underlying data\n\n# Central Tendency : Mean vs Median (Median is the robust version of the mean, because it takes into account \n#the ordering of the points and not the actual values like the mean does)\ndf_tuber[chosen_colnames].describe().loc[['mean', '50%'], :] # The 50% is the median (50% quantile)\n\n\n\n\n\n\n\n\ne_pop_num\ne_prev_100k\nc_cdr\n\n\n\n\nmean\n2.899179e+07\n207.694422\n67.570706\n\n\n50%\n5.140332e+06\n93.000000\n70.000000\n\n\n\n\n\n\n\nLook at how the values are different between the mean and the median … LOOOOOOOK ! This is why when you have a skewed (unsymmetrical) distribution it’s usually more interesting to use the median as a measure of the central tendency of the data. One important thing to note here, for the two first attributes, the mean is higher than the median, but for the last it’s the opposite. This can tell you a thing or two about the shape of your distribution : if the mean is higher than the median, this means that the distribution is skewed to the right (right tail) which pulls the mean higher. And vice-versa.\nMoral of the story is … outliers are a pain in the a**.\n\n# Spread : Standard Deviation vs Inter-Quartile Range vs Median Absolute Deviation (MAD)\nstds = df_tuber[chosen_colnames].std()\niqrs = df_tuber[chosen_colnames].quantile(0.75) - df_tuber[chosen_colnames].quantile(0.25)\nmedianAD = mad(df_tuber[chosen_colnames])\n\noutput = pd.DataFrame(stds, columns = ['std']).T\noutput = pd.concat([output, pd.DataFrame(iqrs, columns = ['IQR']).T], ignore_index=False)\noutput = pd.concat([output, pd.DataFrame(medianAD, columns = ['MAD'], index = chosen_colnames).T], ignore_index=False, names = ['std', 'iqr', 'mad'])\noutput\n\n\n\n\n\n\n\n\ne_pop_num\ne_prev_100k\nc_cdr\n\n\n\n\nstd\n1.177827e+08\n269.418159\n25.234773\n\n\nIQR\n1.677193e+07\n280.500000\n34.000000\n\n\nMAD\n7.454908e+06\n120.090780\n25.204238\n\n\n\n\n\n\n\nThe values here are different as well, maybe more so for the “e_pop_num” attribute than the others, but that is just because of the scaling : “e_pop_num” takes big values overall compared to the other columns, which you can check with the mean values right above.\nFor the first attribute, the standard deviation is higher, and both the IQR and MAD are close to each other. For the second attribute, the inter-quartile range is slightly higher than the standard deviation, but the MAD is far below (less than half) the other two values, and the reason for that is a little bit involved : Basically, the standard deviation measures the spread by computing the squared deviation from the mean while the median absolute deviation evaluates the spread by computing the absolute deviation. This means that when the outliers have much bigger values than the “normal” points, the squared difference explodes (figuratively of course ;p) compared to the absolute difference. And this is actually the case for our second distribution (e_prev_100k) where most values are between 50 and 300 while many outliers lay above the 750 mark and go all the way up to 1800 (look at the boxplots below). For the third attribute the values are somewhat close, especially the std and the MAD, that’s because if you inspect the boxplot, this column doesn’t have many outliers to begin with.\nNonetheless, the differences are real, and if we don’t want to have to handle outliers, then we should be using robust statistics like the median to describe the central tendency and inter-quartile range or median absolute deviation to measure the spread of our data.\n\n# Boxplots of the different columns\nplt.figure(figsize=(12,20))\n\nplt.subplot(3,2,1)\nplt.hist(df_tuber[chosen_colnames[0]])\nplt.title('Histogram of \"{}\"'.format(chosen_colnames[0]))\nplt.subplot(3,2,2)\nplt.boxplot(df_tuber[chosen_colnames[0]])\nplt.title('Boxplot of \"{}\"'.format(chosen_colnames[0]))\n\nplt.subplot(3,2,3)\nplt.hist(df_tuber[chosen_colnames[1]])\nplt.title('Histogram of \"{}\"'.format(chosen_colnames[1]))\nplt.subplot(3,2,4)\nplt.boxplot(df_tuber[chosen_colnames[1]])\nplt.title('Boxplot of \"{}\"'.format(chosen_colnames[1]))\n\nplt.subplot(3,2,5)\nplt.hist(df_tuber[chosen_colnames[2]])\nplt.title('Histogram of \"{}\"'.format(chosen_colnames[2]))\nplt.subplot(3,2,6)\nplt.boxplot(df_tuber[chosen_colnames[2]])\nplt.title('Boxplot of \"{}\"'.format(chosen_colnames[2]));",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Lab: Data Processing and Summarization</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html",
    "href": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html",
    "title": "12  Lab: Regressions",
    "section": "",
    "text": "12.1 Key concepts’ refresher\nLet’s refresh some theoretical concepts to understand what we are going to do.",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Lab: Regressions</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#key-concepts-refresher",
    "href": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#key-concepts-refresher",
    "title": "12  Lab: Regressions",
    "section": "",
    "text": "12.1.1 Simple and Multiple Linear Regression\nIn the linear model the response \\(\\textbf{y}\\) depends linearly from the covariates \\(\\textbf{x}_i\\).\nIn the simple linear regression, with a single variable, we described the relationship between the predictor and the response with a straight line. The general linear model: \\[ \\textbf{y}  =  a_0+ a_1 \\textbf{x}_1 \\]\nThe parameter \\(a_0\\) is called the constant term or the intercept.\nIn the case of multiple linear regression we extend this idea by fitting a m-dimensional hyperplane to our m predictors.\n\\[ \\textbf{y}  =  a_1 \\textbf{x}_1  + \\dots + a_m \\textbf{x}_{m} \\]\nThe \\(a_i\\) are termed the parameters of the model or the coefficients.\n\n\n12.1.2 Ordinary Least Squares\nOrdinary Least Squares (OLS) is the simplest and most common estimator in which the coefficients \\(a\\)’s of the simple linear regression: \\(\\textbf{y} = a_0+a_1 \\textbf{x}\\), are chosen to minimize the square of the distance between the predicted values and the actual values.\n\\[ ||a_0 + a_1 \\textbf{x} -  \\textbf{y} ||^2_2 = \\sum_{j=1}^n (a_0+a_1 x_{j} -  y_j )^2,\\]\nThis expression is often called sum of squared errors of prediction (SSE).",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Lab: Regressions</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#case-study-climate-change-and-sea-ice-extent",
    "href": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#case-study-climate-change-and-sea-ice-extent",
    "title": "12  Lab: Regressions",
    "section": "12.2 Case study: Climate Change and Sea Ice Extent",
    "text": "12.2 Case study: Climate Change and Sea Ice Extent\nRemember, we are trying to answer the following research question: Has there been a decrease in the amount of ice in the last years?\n\n12.2.1 Data assessment\nFirst, let’s load the SeaIce.txt dataset that is already in the data folder1. It is a text file, but unlike csv files, where columns are separated by commas (,), it is a Tab separated file, where each Tab delimites the following columns:\n1 The original dataset was downloaded from https://nsidc.org/data/g02135/versions/3, which provides useful metadata information, as well as API services.\nYear: 4-digit year\nmo: 1- or 2-digit month\ndata_type: Input data set (Goddard/NRTSI-G)\nregion: Hemisphere that this data covers (N: Northern; S: Southern)\nextent: Sea ice extent in millions of square km\narea: Sea ice area in millions of square km\n\nOnce we upload the data, we can create a DataFrame using Pandas using the well known read_csv() function, but in this case, because columns are not separated by commas as expected, but Tabs, we will need to use the delim_whitespace=True argument.\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nice = pd.read_csv('data/raw/SeaIce.txt', delim_whitespace=True)\n\nice.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 424 entries, 0 to 423\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   year       424 non-null    int64  \n 1   mo         424 non-null    int64  \n 2   data_type  424 non-null    object \n 3   region     424 non-null    object \n 4   extent     424 non-null    float64\n 5   area       424 non-null    float64\ndtypes: float64(2), int64(2), object(2)\nmemory usage: 20.0+ KB\n\n\n\nice.head() \n\n\n\n\n\n\n\n\nyear\nmo\ndata_type\nregion\nextent\narea\n\n\n\n\n0\n1979\n1\nGoddard\nN\n15.54\n12.33\n\n\n1\n1980\n1\nGoddard\nN\n14.96\n11.85\n\n\n2\n1981\n1\nGoddard\nN\n15.03\n11.82\n\n\n3\n1982\n1\nGoddard\nN\n15.26\n12.11\n\n\n4\n1983\n1\nGoddard\nN\n15.10\n11.92\n\n\n\n\n\n\n\nAnd we can get some summary statistics from the numerical attributes:\n\nice.describe()\n\n\n\n\n\n\n\n\nyear\nmo\nextent\narea\n\n\n\n\ncount\n424.000000\n424.000000\n424.000000\n424.000000\n\n\nmean\n1996.000000\n6.500000\n-35.443066\n-37.921108\n\n\nstd\n10.214716\n3.474323\n686.736905\n686.566381\n\n\nmin\n1978.000000\n1.000000\n-9999.000000\n-9999.000000\n\n\n25%\n1987.000000\n3.000000\n9.272500\n6.347500\n\n\n50%\n1996.000000\n6.500000\n12.385000\n9.895000\n\n\n75%\n2005.000000\n10.000000\n14.540000\n12.222500\n\n\nmax\n2014.000000\n12.000000\n16.450000\n13.840000\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDid we receive a negative mean for extent and area? What this could possibly mean? Probably, inspecting those attributes visually could give us a clue.\n\n\n\n\n12.2.2 Data visualisation to explore data\nWe will use Seaborn’s lmplot() function to explore linear relationship of different forms, e.g. relationship between the month of the year (variable) and extent (responses):\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n\n# Visualize the data\nplt.scatter(ice.mo, ice.extent, color = 'red')\nplt.xlabel('Year')\nplt.ylabel('Extent')\n\nText(0, 0.5, 'Extent')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe detect some outlier or missing data. This might have to do with those negative mean values that we detected previously.\n\n\nWe can use numpy’s function np.unique() to find the unique elements of an array.\n\nprint ('Different values in data_type field:', np.unique(ice.data_type.values))   # there is a -9999 value!\n\nDifferent values in data_type field: ['-9999' 'Goddard' 'NRTSI-G']\n\n\nLet’s see what type of data we have, other than the ones printed above\n\nice[(ice.data_type != 'Goddard') & (ice.data_type != 'NRTSI-G')]\n\n\n\n\n\n\n\n\nyear\nmo\ndata_type\nregion\nextent\narea\n\n\n\n\n9\n1988\n1\n-9999\nN\n-9999.0\n-9999.0\n\n\n397\n1987\n12\n-9999\nN\n-9999.0\n-9999.0\n\n\n\n\n\n\n\n\n\n12.2.3 Data cleaning\nWe checked all the values and notice -9999 values in data_type field which should contain Goddard or NRTSI-G (some type of input dataset).\nIn this case, we will clean them by creating a copy of the original dataframe that does not include these instances.\n\n\n\n\n\n\nDataframe copies vs instances\n\n\n\nUnless we do not explicitly create a copy of a dataframe, when subsetting a dataframes we are actually creating instances. Whereas copies are totally independent objects from the original one, instances are reduced “views” from the original, meaning that if we change a value on the instance, we are also changing the value on the original data frame, which may not be what we wanted to do.\nAn explanation of how (and why) do we need to create copies can be found here: https://www.statology.org/pandas-copy-dataframe/\n\n\n\n# We can easily clean the data now:\nice2 = ice[ice.data_type != '-9999'].copy()\n\nprint ('shape:', ice2.shape)\n\nshape: (422, 6)\n\n\n\n# And repeat the plot, without the outliers\nplt.scatter(ice2.mo, ice2.extent, color = 'red')\nplt.xlabel('Month')\nplt.ylabel('Extent')\n\nText(0, 0.5, 'Extent')\n\n\n\n\n\n\n\n\n\n\n#here is the same plot but using the seaborn library. A transition to the Seaborn plot we have in the next cell.\n\n#sns.relplot(ice2, x = \"mo\", y = \"extent\", aspect = 2)",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Lab: Regressions</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#regression-model-fit",
    "href": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#regression-model-fit",
    "title": "12  Lab: Regressions",
    "section": "12.3 Regression model fit",
    "text": "12.3 Regression model fit\nNow that we have a clean dataset, we can use Seaborn’s lmplot() function comparing month vs extent.\nThe lmplot() function from the Seaborn module is intended for exploring linear relationships of different forms in multidimensional datesets. Input data must be in a Pandas DataFrame. To plot them, we provide the predictor (mo) and response (extent) variable names along with the dataset (ice2).\n\nsns.lmplot(ice2, x = \"mo\", y = \"extent\", aspect=2);\n\n# Uncomment below to save the resulting plot.\n#plt.savefig(\"figs/CleanedByMonth.png\", dpi = 300, bbox_inches = 'tight')\n\n\n\n\n\n\n\n\nAbove you can see ice extent data by month. You can see a monthly fluctuation of the sea ice extent, as would be expected for the different seasons of the year. In order to run regression, and avoid this fluctuation we can normalize data. This will let us see the evolution of the extent over the years.\n\n12.3.1 Normalization\n\n# Compute the mean for each month.\nmonth_means = ice2.groupby('mo').extent.mean()\n\n# Compute the variance for each month.\nmonth_variances = ice2.groupby('mo').extent.var()\n\n# Show the values:\nprint('Means:', month_means)\nprint('\\n') # Add a new line between the two prints, so they are easily distinguishible.\nprint ('Variances:',month_variances)\n\nMeans: mo\n1     14.479429\n2     15.298889\n3     15.491714\n4     14.766000\n5     13.396000\n6     11.860000\n7      9.601143\n8      7.122286\n9      6.404857\n10     8.809143\n11    10.964722\n12    13.059429\nName: extent, dtype: float64\n\n\nVariances: mo\n1     0.304906\n2     0.295804\n3     0.237209\n4     0.215378\n5     0.189901\n6     0.247918\n7     0.679175\n8     0.824577\n9     1.143902\n10    0.630361\n11    0.412511\n12    0.284870\nName: extent, dtype: float64\n\n\nTo capture variation per month, we can compute mean for the i-th interval of time (using 1979-2014) and subtract it from the set of extent values for that month . This can be converted to a relative pecentage difference by dividing it by the total avareage (1979-2014) and multiplying by 100.\n\n# Let's create a new column to hold these normalised values.\nice2['extent_norm'] = np.nan\n\n\n# run the following to check what the data types look like:\nice2.dtypes\nice2.head()\n\n\n\n\n\n\n\n\nyear\nmo\ndata_type\nregion\nextent\narea\nextent_norm\n\n\n\n\n0\n1979\n1\nGoddard\nN\n15.54\n12.33\nNaN\n\n\n1\n1980\n1\nGoddard\nN\n14.96\n11.85\nNaN\n\n\n2\n1981\n1\nGoddard\nN\n15.03\n11.82\nNaN\n\n\n3\n1982\n1\nGoddard\nN\n15.26\n12.11\nNaN\n\n\n4\n1983\n1\nGoddard\nN\n15.10\n11.92\nNaN\n\n\n\n\n\n\n\n\n# Data normalization loop. Note that we are saving the new computed values into the \nfor i in range(12):\n    ice2.extent_norm[ice2.mo == i+1] = 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean()\n    #print (\"ice2.extent[ice2.mo == i+1]\", 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean())\n    #print(month_means.mean())\n\n\n# let's check if all is in place.\nice2.head()\n\n\n\n\n\n\n\n\nyear\nmo\ndata_type\nregion\nextent\narea\nextent_norm\n\n\n\n\n0\n1979\n1\nGoddard\nN\n15.54\n12.33\n9.009934\n\n\n1\n1980\n1\nGoddard\nN\n14.96\n11.85\n4.082626\n\n\n2\n1981\n1\nGoddard\nN\n15.03\n11.82\n4.677301\n\n\n3\n1982\n1\nGoddard\nN\n15.26\n12.11\n6.631234\n\n\n4\n1983\n1\nGoddard\nN\n15.10\n11.92\n5.271976\n\n\n\n\n\n\n\n\n\nsns.lmplot(ice2 , x = \"mo\", y = \"extent_norm\", height = 5.2, aspect = 2);\n#plt.savefig(\"figs/IceExtentNormalizedByMonth.png\", dpi = 300, bbox_inches = 'tight')\n\n\n\n\n\n\n\n\n\nprint ('mean:', ice2.extent.mean())\nprint ('var:', ice2.extent.var())\n\nmean: 11.777582938388624\nvar: 9.738906257950491\n\n\nLet us consider the entire year\n\nsns.lmplot(ice2, x = \"year\", y = \"extent\", height = 5.2, aspect = 2);\n#plt.savefig(\"figs/IceExtentAllMonthsByYearlmplot.png\", dpi = 300, bbox_inches = 'tight')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant Do-it-Yourself Moment\n\n\n\nA question here! Would you like to use the variable extent or extent_norm here. What would this decision change? Reflect on this, or better, try out the above with different versions of the data. Note observations.\n\n\n\n\n12.3.2 Pearson’s correlation\nLet’s calculate Pearson’s correlation coefficient and the p-value for testing non-correlation.\n\nimport scipy.stats\nscipy.stats.pearsonr(ice2.year.values, ice2.extent.values)\n\nPearsonRResult(statistic=-0.16438687249518155, pvalue=0.000698891736147664)\n\n\n\n\n12.3.3 Simple OLS\nWe can also compute the trend as a simple linear regression (OLS) and quantitatively evaluate it.\nFor that we use using Scikit-learn, library that provides a variety of both supervised and unsupervised machine learning techniques. Scikit-learn provides an object-oriented interface centered around the concept of an Estimator. The Estimator.fit method sets the state of the estimator based on the training data. Usually, the data is comprised of a two-dimensional numpy array \\(X\\) of shape (n_samples, n_predictors) that holds the so-called feature matrix and a one-dimensional numpy array \\(\\textbf{y}\\) that holds the responses. Some estimators allow the user to control the fitting behavior. For example, the sklearn.linear_model.LinearRegression estimator allows the user to specify whether or not to fit an intercept term. This is done by setting the corresponding constructor arguments of the estimator object. During the fitting process, the state of the estimator is stored in instance attributes that have a trailing underscore (_). For example, the coefficients of a LinearRegression estimator are stored in the attribute coef_.\nEstimators that can generate predictions provide a Estimator.predict method. In the case of regression, Estimator.predict will return the predicted regression values, \\(\\hat{\\textbf{y}}\\).\n\nfrom sklearn.linear_model import LinearRegression\n\nest = LinearRegression(fit_intercept = True)\n\nx = ice2[['year']]\ny = ice2[['extent']]\n\nest.fit(x, y)\n\nprint(\"Coefficients:\", est.coef_)\nprint (\"Intercept:\", est.intercept_)\n\nCoefficients: [[-0.05018625]]\nIntercept: [111.95135764]\n\n\nWe can evaluate the model fitting by computing the mean squared error (\\(MSE\\)) and the coefficient of determination (\\(R^2\\)) of the model. The coefficient \\(R^2\\) is defined as \\((1 - \\textbf{u}/\\textbf{v})\\), where \\(\\textbf{u}\\) is the residual sum of squares \\(\\sum (\\textbf{y} - \\hat{\\textbf{y}})^2\\) and \\(\\textbf{v}\\) is the regression sum of squares \\(\\sum (\\textbf{y} - \\bar{\\textbf{y}})^2\\), where \\(\\bar{\\textbf{y}}\\) is the mean. The best possible score for \\(R^2\\) is 1.0: lower values are worse. These measures can provide a quantitative answer to the question we are facing: Is there a negative trend in the evolution of sea ice extent over recent years?\n\nfrom sklearn import metrics\n\n# Analysis for all months together.\nx = ice2[['year']]\ny = ice2[['extent']]\nmodel = LinearRegression()\nmodel.fit(x, y)\ny_hat = model.predict(x)\nplt.plot(x, y,'o', alpha = 0.5)\nplt.plot(x, y_hat, 'r', alpha = 0.5)\nplt.xlabel('year')\nplt.ylabel('extent (All months)')\nprint (\"MSE:\", metrics.mean_squared_error(y_hat, y))\nprint (\"R^2:\", metrics.r2_score(y_hat, y))\nprint (\"var:\", y.var())\n#plt.savefig(\"figs/IceExtentLinearRegressionAllMonthsByYearPrediction.png\", dpi = 300, bbox_inches = 'tight')\n\nMSE: 9.453277027370586\nR^2: -35.00545377482949\nvar: extent    9.738906\ndtype: float64\n\n\n\n\n\n\n\n\n\nWe can conclude that the data show a long-term negative trend in recent years.\n\n# Analysis for a particular month.\n#For January\njan = ice2[ice2.mo == 1];\n\nx = jan[['year']]\ny = jan[['extent']]\n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\ny_hat = model.predict(x)\n\nplt.figure()\nplt.plot(x, y,'-o', alpha = 0.5)\nplt.plot(x, y_hat, 'r', alpha = 0.5)\nplt.xlabel('year')\nplt.ylabel('extent (January)')\n\nprint (\"MSE:\", metrics.mean_squared_error(y_hat, y))\nprint (\"R^2:\", metrics.r2_score(y_hat, y))\n\nMSE: 0.05320018183025639\nR^2: 0.7810636041396222\n\n\n\n\n\n\n\n\n\nWe can also estimate the extent value for 2025. For that we use the function predict of the model.\n\nX = np.array(2025) \ny_hat = model.predict(X.reshape(-1, 1))\nj = 1 # January\n# Original value (before normalization)\ny_hat = (y_hat*month_means.mean()/100) + month_means[j]\nprint (\"Prediction of extent for January 2025 (in millions of square km):\", y_hat)\n\nPrediction of extent for January 2025 (in millions of square km): [[16.02668522]]",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Lab: Regressions</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#more-information",
    "href": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#more-information",
    "title": "12  Lab: Regressions",
    "section": "12.4 More information",
    "text": "12.4 More information\nYou can find more information about using scikit for Linear regression here: https://www.tutorialspoint.com/scikit_learn/scikit_learn_linear_regression.htm\n\n\n\n\nIgual, Laura, and Santi Seguí. 2017. “Regression Analysis.” In Introduction to Data Science: A Python Approach to Concepts, Techniques and Applications, 97–114. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-50017-1_6.",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Lab: Regressions</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html",
    "href": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html",
    "title": "13  Exercise: Regression",
    "section": "",
    "text": "13.1 Scikit Learn\nYou can use here as well Scikit Learn library. More information you can find here: https://www.tutorialspoint.com/scikit_learn/scikit_learn_linear_regression.htm",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exercise: Regression</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#wine-dataset",
    "href": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#wine-dataset",
    "title": "13  Exercise: Regression",
    "section": "13.2 Wine Dataset",
    "text": "13.2 Wine Dataset\nFor this exercise we will be using Wine Quality Dataset from Cortez et al. (2009). You can find more information about it here: https://doi.org/10.24432/C56S3T\nWhat would be your research question? What do you like to learn, given the data you have?",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exercise: Regression</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#reading-data",
    "href": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#reading-data",
    "title": "13  Exercise: Regression",
    "section": "13.3 Reading Data",
    "text": "13.3 Reading Data\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n\nwine = pd.read_excel('data/raw/winequality-red_v2.xlsx', engine = 'openpyxl')\n\n#You might need to use encoding, then the code will look like:\n# wine = pd.read_excel('data/raw/winequality-red_v2.xlsx', engine = 'openpyxl', encoding='UTF-8')",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exercise: Regression</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#data-exploration",
    "href": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#data-exploration",
    "title": "13  Exercise: Regression",
    "section": "13.4 Data exploration",
    "text": "13.4 Data exploration\nLet’s check the data, their distribution and central tendencies\n\nprint('shape:', wine.shape)\nwine.head()\n\nshape: (1599, 12)\n\n\n\n\n\n\n\n\n\nfixed_acidity\nvolatile_acidity\ncitric_acid\nresidual_sugar\nchlorides\nfree_sulfur_dioxide\ntotal_sulfur_dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\n0\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n1\n7.8\n0.88\n0.00\n2.6\n0.098\n25.0\n67.0\n0.9968\n3.20\n0.68\n9.8\n5\n\n\n2\n7.8\n0.76\n0.04\n2.3\n0.092\n15.0\n54.0\n0.9970\n3.26\n0.65\n9.8\n5\n\n\n3\n11.2\n0.28\n0.56\n1.9\n0.075\n17.0\n60.0\n0.9980\n3.16\n0.58\n9.8\n6\n\n\n4\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n\n\n\n\n\n\n13.4.1 Check your variables\nUse lmplot() function from Seaborn to explore linear relationship Input data must be in a Pandas DataFrame. To plot them, we provide the predictor and response variable names along with the dataset\nDid you find outliers or missing data? You can use function np.unique and find the unique elements of an array.\n\n?np.unique\n\nDo you need to remove any cases?\n\n \n\nDid you need to standarize data?\nIf you standarized data, try to plot them again",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exercise: Regression</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#form-ideas-about-the-data",
    "href": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#form-ideas-about-the-data",
    "title": "13  Exercise: Regression",
    "section": "13.5 Form ideas about the data",
    "text": "13.5 Form ideas about the data\nBefore you move on to exploring correlations and maybe other kinds of models, try and build some sense of understanding of the relations between the variables. What are some relations that stand out. Do you know a bit more about the wines in this dataset or wines more generally?",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exercise: Regression</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#move-on-to-building-some-simple-models",
    "href": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#move-on-to-building-some-simple-models",
    "title": "13  Exercise: Regression",
    "section": "13.6 Move on to building some simple models",
    "text": "13.6 Move on to building some simple models\nYou can calculates a Pearson correlation coefficient and the p-value for testing non-correlation.\nWe will be using the scikit-learn package here. This is a package we will be making use of very frequently.\n\nimport scipy.stats\nscipy.stats.pearsonr(wine.???.values, wine.???.values)\n\nSyntaxError: invalid syntax (987973612.py, line 2)\n\n\nusing Scikit-learn, build a simple linear regression (OLS)\n\nfrom sklearn.linear_model import LinearRegression\n\nest = LinearRegression(fit_intercept = True)\n\nx = wine[['???']]\ny = wine[['???']]\n\nest.fit(x, y)\n\nprint(\"Coefficients:\", est.coef_)\nprint (\"Intercept:\", est.intercept_)\n\nKeyError: \"None of [Index(['???'], dtype='object')] are in the [columns]\"\n\n\nWhat is the model’s mean squared error (\\(MSE\\)) and the coefficient of determination (\\(R^2\\)) ?\n\nfrom sklearn import metrics\n\n# Analysis for all months together.\nx = wdi[['???']]\ny = wdi[['???']]\nmodel = LinearRegression()\nmodel.fit(x, y)\ny_hat = model.predict(x)\nplt.plot(x, y,'o', alpha = 0.5)\nplt.plot(x, y_hat, 'r', alpha = 0.5)\nplt.xlabel('?')\nplt.ylabel('?')\nprint (\"MSE:\", metrics.mean_squared_error(y_hat, y))\nprint (\"R^2:\", metrics.r2_score(y_hat, y))\nprint (\"var:\", y.var())\nplt.savefig(\"?.png\", dpi = 300, bbox_inches = 'tight')\n\nNameError: name 'wdi' is not defined\n\n\nWhat’s the conclusion?\n\n\n\n\nCortez, Paulo, A Cerdeira, F Almeida, T Matos, and J. Reis. 2009. “Wine Quality.” UCI Machine Learning Repository. https://doi.org/10.24432/C56S3T.",
    "crumbs": [
      "Abstractions & Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exercise: Regression</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-04.html",
    "href": "content/sessions/session-04.html",
    "title": "14  Introduction",
    "section": "",
    "text": "14.1 Highlights of the lecture\nWe start our discussions this week with the notion of spaces and how spaces we construct can help us in understanding complex phenomena. We then look into how spaces come into play when we explore high-dimensional data sets. We look into how a number of techniques help us construct artificial spaces – projections – that provide the basis for further operations. We then look into the topic of cluster analysis and will discuss a number of techniques that can help us explore “structures” in low and high dimensional data sets. We will also explore how projection spaces and the notions of distance come into play.",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-04.html#practical-lab-session",
    "href": "content/sessions/session-04.html#practical-lab-session",
    "title": "14  Introduction",
    "section": "14.2 Practical Lab Session",
    "text": "14.2 Practical Lab Session\nIn the practical session, we will explore a number of computational routines and tools that support dimension reduction and projection tasks. We will also look into cluster analysis and explore functions that can help us run cluster analysis routines. We will have a particular attention on the interpretation of these complex tools and will be working with multivariate data sets.",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-04.html#reading-lists-resources",
    "href": "content/sessions/session-04.html#reading-lists-resources",
    "title": "14  Introduction",
    "section": "14.3 Reading lists & Resources",
    "text": "14.3 Reading lists & Resources\n\n14.3.1 Required reading\n\nVery good overview on variable and feature selection (have a look at the first 4 sections and can read the rest for more advanced methods): Guyon, Isabelle, and André Elisseeff. “An introduction to variable and feature selection.” The Journal of Machine Learning Research 3 (2003): 1157-1182.\n\nA good overview on dimension reduction techniques, read the Section 1 on a general discussion of dimension reduction, Section 2 on PCA and appendix H on MDS for a technical explanation.\nOn clustering methods : Tan, P., Steinbach, M. & Kumar, V., 1956 2006, Introduction to data mining, Pearson Addison Wesley, Boston, Mass; London. Chapter-7 is on cluster analysis and really a good introduction: https://www-users.cs.umn.edu/~kumar001/dmbook/ch7_clustering.pdf\n\n\n\n14.3.2 Optional reading\n\nOn using PCA in biotechnology, gives an overview introduction: Ringnér, Markus (2008). “What is principal component analysis?”. Nature biotechnology (1087-0156), 26 (3), p. 303.\nRapkin, B.D. and Luke, D.A., 1993. Cluster analysis in community research: Epistemology and practice. American Journal of Community Psychology, 21(2), pp.247-277.[pdf - login through library account]\nA non-technical introduction to MDS : Jaworska, Natalia, and Angelina Chupetlovska-Anastasova. “A review of multidimensional scaling (MDS) and its utility in various psychological domains.” Tutorials in Quantitative Methods for Psychology 5.1 (2009): 1-10.\n\n\n\n14.3.3 Further reading\n\nGalbraith, S., Daniel, J.A. and Vissel, B., 2010. A study of clustered data and approaches to its analysis. Journal of Neuroscience, 30(32), pp.10601-10608.\nFonseca, J.R., 2013. Clustering in the field of social sciences: that is your choice. International Journal of Social Research Methodology, 16(5), pp.403-428. [pdf]\nA useful blog post on Feature Selection – https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Session-04_PCA_playground.html",
    "href": "content/labs/Lab_4/IM939_Session-04_PCA_playground.html",
    "title": "15  Working with Dimension Reduction Methods",
    "section": "",
    "text": "15.1 Exploratory analysis\nLet’s do a bit of exploratory analysis to check the characteristics of the features and look for redundancies\n# Plot the training points\n\ncolumnIndexToUseX = 2\ncolumnIndexToUseY = 3\n\nx_min, x_max = X[:, columnIndexToUseX].min() - .5, X[:, columnIndexToUseX].max() + .5\ny_min, y_max = X[:, columnIndexToUseY].min() - .5, X[:, columnIndexToUseY].max() + .5\n\nplt.figure(figsize=(8, 8))\nplt.scatter(X[:, columnIndexToUseX], X[:, columnIndexToUseY], c=y, cmap=plt.cm.Set1,\n            edgecolor='k' )\nplt.xlabel(column_names[columnIndexToUseX])\nplt.ylabel(column_names[columnIndexToUseY])\n\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\n\nplt.show()\n# It would be good to have a look at the pairwise correlations. \n# This might help one to identify some redundant variables at least as long as pairs of dimensions are considered.\nimport seaborn as sns\nsns.set(style=\"ticks\")\n\n## we are cheating here and using a copy of the Iris data as made available as a Pandas DataFrame within the Seaborn package\ndf = sns.load_dataset(\"iris\")\nsns.pairplot(df, hue=\"species\")",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Working with Dimension Reduction Methods</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Session-04_PCA_playground.html#principal-component-analysis",
    "href": "content/labs/Lab_4/IM939_Session-04_PCA_playground.html#principal-component-analysis",
    "title": "15  Working with Dimension Reduction Methods",
    "section": "15.2 Principal Component Analysis",
    "text": "15.2 Principal Component Analysis\nHere we start working with PCA to find linear mappings of the original dimension space\n\nfrom sklearn.decomposition import PCA\n\n\n# I am stating how many components I would like my final projection space to have\nn_components = 2\n\n# This line creates a PCA computation object\npca = PCA(n_components=n_components)\n\n# And this line \"projects the data to the newly generated space\"\nX_pca = pca.fit_transform(X)\n\n# I set the colours that I can use to colour the different classes of iris flowers with. I may not always have this class information but in this instance, we have this information so we can use it for colouring.\ncolors = ['navy', 'turquoise', 'darkorange']\n\nplt.figure(figsize=(8, 8))\nfor color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1],\n                color=color, lw=2, label=target_name)\n\nplt.title(\"PCA of iris dataset\")\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(column_names)\npca.components_\n\n['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n\n\narray([[ 0.36138659, -0.08452251,  0.85667061,  0.3582892 ],\n       [ 0.65658877,  0.73016143, -0.17337266, -0.07548102]])\n\n\n\n15.2.1 A bit on interpretation\nOK, what are we looking at here …\nThe array of numbers we are looking at above are actually the coefficients of the first two principal components, so these “are” the components.\nThe first line consists of the “loadings” of the variables for the first component, this means:\nSepal Length’s loading in principal component 1 (PC1) is 0.36138659, Sepal Width’s is -0.08452251, Petal Length’s is 0.85667061 and Petal Width’s is 0.3582892. And the second line of numbers correspond to the loadings of the variables within the “second” principal component.\nBut then, what do these numbers tell us? One analysis we can do here is to look at the “magnitude” of these numbers. If you look at the four numbers, we notice that the largest in magnitude is for Petal Length (0.85667061) which indicates that the most important dimension for this component is Petal Length. As far as PCA is concerned, the notion of importance is how much of the “variance” of the data is explained by a principal component, so Petal Length is the feature that has the most influence on the variation within the data along the first component. And the first component is the computed axis that explains most of the variance in the data (i.e., has the highest eigenvalue in the computations – see the slides). We can do a similar analysis for the second component, and notice that Sepal Widht is the most key variable in this component but I Sepal Length is also important as seen from its higher loading value.\nIf we look at these four variables, the only variable that didn’t get a high loading is Petal Width, this is a sign that this feature might not carry similar levels of variation as the others.\n\n15.2.1.1 How about the “projection”?\nWhat we look at above in the scatterplot, what we are seeing is all the observations “projected” onto this new space defined by the two principal components. This is why dimension reduction methods are often referred to as “projection” or “embedding” methods. The idea is that the new components give you a new “space” where you can observe your data points. This is what the X_pca = pca.fit_transform(X) line is doing in the code above.",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Working with Dimension Reduction Methods</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Session-04_PCA_playground.html#lda",
    "href": "content/labs/Lab_4/IM939_Session-04_PCA_playground.html#lda",
    "title": "15  Working with Dimension Reduction Methods",
    "section": "15.3 LDA",
    "text": "15.3 LDA\nLinear Discriminant Analysis is a method which considers the class labels as well as the data during the projection\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_lda = lda.fit(X, y).transform(X)\n# Notice here that we are also feeding in the class labels\n# Since we have three classes but LDA is a binary method, \n# it will automatically run 1 vs. 2 other classes for all the combinations\n\nplt.figure(figsize=(8, 8))\nfor color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n    plt.scatter(X_lda[y == i, 0], X_lda[y == i, 1],\n                color=color, lw=2, label=target_name)\n\nplt.title(\"LDA of iris dataset\")\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(column_names)\nlda.coef_\n\n['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n\n\narray([[  6.31475846,  12.13931718, -16.94642465, -20.77005459],\n       [ -1.53119919,  -4.37604348,   4.69566531,   3.06258539],\n       [ -4.78355927,  -7.7632737 ,  12.25075935,  17.7074692 ]])",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Working with Dimension Reduction Methods</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Session-04_PCA_playground.html#a-regularised-method-sparcepca",
    "href": "content/labs/Lab_4/IM939_Session-04_PCA_playground.html#a-regularised-method-sparcepca",
    "title": "15  Working with Dimension Reduction Methods",
    "section": "15.4 A regularised method – SparcePCA",
    "text": "15.4 A regularised method – SparcePCA\n\n# A regularised version of PCA -- SparsePCA -- this is one of those Embedded methods that we talked about\nfrom sklearn.decomposition import SparsePCA\n\ns_pca = SparsePCA(n_components=n_components)\nSparse_PCA_embdedded = s_pca.fit_transform(X)\n\ncolors = ['navy', 'turquoise', 'darkorange']\n\nplt.figure(figsize=(8, 8))\nfor color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n    plt.scatter(Sparse_PCA_embdedded[y == i, 0], Sparse_PCA_embdedded[y == i, 1],\n                color=color, lw=2, label=target_name)\n\nplt.title(\"SparcePCA of iris dataset\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Let's see if we observe differences\nprint(column_names)\ns_pca.components_\n\n['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n\n\narray([[ 0.34155543, -0.04951592,  0.87443905,  0.34094634],\n       [ 0.68542067,  0.72814731,  0.        ,  0.        ]])",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Working with Dimension Reduction Methods</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Session-04_PCA_playground.html#a-non-linear-method-called-tsne",
    "href": "content/labs/Lab_4/IM939_Session-04_PCA_playground.html#a-non-linear-method-called-tsne",
    "title": "15  Working with Dimension Reduction Methods",
    "section": "15.5 A non-linear method called tSNE",
    "text": "15.5 A non-linear method called tSNE\n\nfrom sklearn.manifold import TSNE\n\nTSNE_Model = TSNE(n_components=n_components)\ntSNE_embdedded = TSNE_Model.fit_transform(X)\n\nplt.figure(figsize=(8, 8))\nfor color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n    plt.scatter(tSNE_embdedded[y == i, 0], tSNE_embdedded[y == i, 1],\n                color=color, lw=2, label=target_name)\n\nplt.title(\"tSNE of iris dataset\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n15.5.1 How is tSNE results different?\nNotice how different the results came up with tSNE. The clusters are much more visible and better seperated. However, the spread within the clusters are mostly gone. And now the axes are not a linear combination of the original features any more.",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Working with Dimension Reduction Methods</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html",
    "href": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html",
    "title": "16  Lab: Dimension Reduction",
    "section": "",
    "text": "16.1 Data preparations\nIn this notebook, we are going to use the simple Iris flower dataset (Fisher 1936). The dataset, which is famously used to introduce these methods, consists of 4 measures or attributes (the length and the width of the sepals and petals, in centimeters) describing 50 samples from three species of flowers (Iris setosa, Iris virginica and Iris versicolor).\nContrary to previous sessions, in this case, the dataset will not be read from a csv file, but it is provided by the sklearn.datasets submodule:\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.datasets import load_iris\nimport numpy as np\n\niris = load_iris()\n\n# Explore our data.\niris\n\n{'data': array([[5.1, 3.5, 1.4, 0.2],\n        [4.9, 3. , 1.4, 0.2],\n        [4.7, 3.2, 1.3, 0.2],\n        [4.6, 3.1, 1.5, 0.2],\n        [5. , 3.6, 1.4, 0.2],\n        [5.4, 3.9, 1.7, 0.4],\n        [4.6, 3.4, 1.4, 0.3],\n        [5. , 3.4, 1.5, 0.2],\n        [4.4, 2.9, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.1],\n        [5.4, 3.7, 1.5, 0.2],\n        [4.8, 3.4, 1.6, 0.2],\n        [4.8, 3. , 1.4, 0.1],\n        [4.3, 3. , 1.1, 0.1],\n        [5.8, 4. , 1.2, 0.2],\n        [5.7, 4.4, 1.5, 0.4],\n        [5.4, 3.9, 1.3, 0.4],\n        [5.1, 3.5, 1.4, 0.3],\n        [5.7, 3.8, 1.7, 0.3],\n        [5.1, 3.8, 1.5, 0.3],\n        [5.4, 3.4, 1.7, 0.2],\n        [5.1, 3.7, 1.5, 0.4],\n        [4.6, 3.6, 1. , 0.2],\n        [5.1, 3.3, 1.7, 0.5],\n        [4.8, 3.4, 1.9, 0.2],\n        [5. , 3. , 1.6, 0.2],\n        [5. , 3.4, 1.6, 0.4],\n        [5.2, 3.5, 1.5, 0.2],\n        [5.2, 3.4, 1.4, 0.2],\n        [4.7, 3.2, 1.6, 0.2],\n        [4.8, 3.1, 1.6, 0.2],\n        [5.4, 3.4, 1.5, 0.4],\n        [5.2, 4.1, 1.5, 0.1],\n        [5.5, 4.2, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.2],\n        [5. , 3.2, 1.2, 0.2],\n        [5.5, 3.5, 1.3, 0.2],\n        [4.9, 3.6, 1.4, 0.1],\n        [4.4, 3. , 1.3, 0.2],\n        [5.1, 3.4, 1.5, 0.2],\n        [5. , 3.5, 1.3, 0.3],\n        [4.5, 2.3, 1.3, 0.3],\n        [4.4, 3.2, 1.3, 0.2],\n        [5. , 3.5, 1.6, 0.6],\n        [5.1, 3.8, 1.9, 0.4],\n        [4.8, 3. , 1.4, 0.3],\n        [5.1, 3.8, 1.6, 0.2],\n        [4.6, 3.2, 1.4, 0.2],\n        [5.3, 3.7, 1.5, 0.2],\n        [5. , 3.3, 1.4, 0.2],\n        [7. , 3.2, 4.7, 1.4],\n        [6.4, 3.2, 4.5, 1.5],\n        [6.9, 3.1, 4.9, 1.5],\n        [5.5, 2.3, 4. , 1.3],\n        [6.5, 2.8, 4.6, 1.5],\n        [5.7, 2.8, 4.5, 1.3],\n        [6.3, 3.3, 4.7, 1.6],\n        [4.9, 2.4, 3.3, 1. ],\n        [6.6, 2.9, 4.6, 1.3],\n        [5.2, 2.7, 3.9, 1.4],\n        [5. , 2. , 3.5, 1. ],\n        [5.9, 3. , 4.2, 1.5],\n        [6. , 2.2, 4. , 1. ],\n        [6.1, 2.9, 4.7, 1.4],\n        [5.6, 2.9, 3.6, 1.3],\n        [6.7, 3.1, 4.4, 1.4],\n        [5.6, 3. , 4.5, 1.5],\n        [5.8, 2.7, 4.1, 1. ],\n        [6.2, 2.2, 4.5, 1.5],\n        [5.6, 2.5, 3.9, 1.1],\n        [5.9, 3.2, 4.8, 1.8],\n        [6.1, 2.8, 4. , 1.3],\n        [6.3, 2.5, 4.9, 1.5],\n        [6.1, 2.8, 4.7, 1.2],\n        [6.4, 2.9, 4.3, 1.3],\n        [6.6, 3. , 4.4, 1.4],\n        [6.8, 2.8, 4.8, 1.4],\n        [6.7, 3. , 5. , 1.7],\n        [6. , 2.9, 4.5, 1.5],\n        [5.7, 2.6, 3.5, 1. ],\n        [5.5, 2.4, 3.8, 1.1],\n        [5.5, 2.4, 3.7, 1. ],\n        [5.8, 2.7, 3.9, 1.2],\n        [6. , 2.7, 5.1, 1.6],\n        [5.4, 3. , 4.5, 1.5],\n        [6. , 3.4, 4.5, 1.6],\n        [6.7, 3.1, 4.7, 1.5],\n        [6.3, 2.3, 4.4, 1.3],\n        [5.6, 3. , 4.1, 1.3],\n        [5.5, 2.5, 4. , 1.3],\n        [5.5, 2.6, 4.4, 1.2],\n        [6.1, 3. , 4.6, 1.4],\n        [5.8, 2.6, 4. , 1.2],\n        [5. , 2.3, 3.3, 1. ],\n        [5.6, 2.7, 4.2, 1.3],\n        [5.7, 3. , 4.2, 1.2],\n        [5.7, 2.9, 4.2, 1.3],\n        [6.2, 2.9, 4.3, 1.3],\n        [5.1, 2.5, 3. , 1.1],\n        [5.7, 2.8, 4.1, 1.3],\n        [6.3, 3.3, 6. , 2.5],\n        [5.8, 2.7, 5.1, 1.9],\n        [7.1, 3. , 5.9, 2.1],\n        [6.3, 2.9, 5.6, 1.8],\n        [6.5, 3. , 5.8, 2.2],\n        [7.6, 3. , 6.6, 2.1],\n        [4.9, 2.5, 4.5, 1.7],\n        [7.3, 2.9, 6.3, 1.8],\n        [6.7, 2.5, 5.8, 1.8],\n        [7.2, 3.6, 6.1, 2.5],\n        [6.5, 3.2, 5.1, 2. ],\n        [6.4, 2.7, 5.3, 1.9],\n        [6.8, 3. , 5.5, 2.1],\n        [5.7, 2.5, 5. , 2. ],\n        [5.8, 2.8, 5.1, 2.4],\n        [6.4, 3.2, 5.3, 2.3],\n        [6.5, 3. , 5.5, 1.8],\n        [7.7, 3.8, 6.7, 2.2],\n        [7.7, 2.6, 6.9, 2.3],\n        [6. , 2.2, 5. , 1.5],\n        [6.9, 3.2, 5.7, 2.3],\n        [5.6, 2.8, 4.9, 2. ],\n        [7.7, 2.8, 6.7, 2. ],\n        [6.3, 2.7, 4.9, 1.8],\n        [6.7, 3.3, 5.7, 2.1],\n        [7.2, 3.2, 6. , 1.8],\n        [6.2, 2.8, 4.8, 1.8],\n        [6.1, 3. , 4.9, 1.8],\n        [6.4, 2.8, 5.6, 2.1],\n        [7.2, 3. , 5.8, 1.6],\n        [7.4, 2.8, 6.1, 1.9],\n        [7.9, 3.8, 6.4, 2. ],\n        [6.4, 2.8, 5.6, 2.2],\n        [6.3, 2.8, 5.1, 1.5],\n        [6.1, 2.6, 5.6, 1.4],\n        [7.7, 3. , 6.1, 2.3],\n        [6.3, 3.4, 5.6, 2.4],\n        [6.4, 3.1, 5.5, 1.8],\n        [6. , 3. , 4.8, 1.8],\n        [6.9, 3.1, 5.4, 2.1],\n        [6.7, 3.1, 5.6, 2.4],\n        [6.9, 3.1, 5.1, 2.3],\n        [5.8, 2.7, 5.1, 1.9],\n        [6.8, 3.2, 5.9, 2.3],\n        [6.7, 3.3, 5.7, 2.5],\n        [6.7, 3. , 5.2, 2.3],\n        [6.3, 2.5, 5. , 1.9],\n        [6.5, 3. , 5.2, 2. ],\n        [6.2, 3.4, 5.4, 2.3],\n        [5.9, 3. , 5.1, 1.8]]),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n 'frame': None,\n 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10'),\n 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n  Mathematical Statistics\" (John Wiley, NY, 1950).\\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n  Structure and Classification Rule for Recognition in Partially Exposed\\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n  on Information Theory, May 1972, 431-433.\\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n  conceptual clustering system finds 3 classes in the data.\\n- Many, many more ...\\n\\n|details-end|',\n 'feature_names': ['sepal length (cm)',\n  'sepal width (cm)',\n  'petal length (cm)',\n  'petal width (cm)'],\n 'filename': 'iris.csv',\n 'data_module': 'sklearn.datasets.data'}\nRegretfully, the iris dataset is not a data frame as the previous ones:\ntype(iris)\n\nsklearn.utils._bunch.Bunch\nThis means that, if we want to apply all the methods that we are familiar with, we will need to convert this odd data type into a pandas dataframe format we know and love. We can do this following this stackoverflow answer:\nimport pandas as pd\n\niris_df = pd.DataFrame(iris.data, columns = iris.feature_names)\niris_df.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\nAs usual, we may want to see some descriptive measures to get a sense of the data:\niris_df.describe()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Lab: Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#data-preparations",
    "href": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#data-preparations",
    "title": "16  Lab: Dimension Reduction",
    "section": "",
    "text": "Image of a primrose willowherb ‘’Ludwigia octovalvis’’ (Family Onagraceae), flower showing petals and sepals. Photograph made in Hawai’i by Eric Guinther and released under the GNU Free Documentation License.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is this telling us?\n\n\n\n\n\nAs can be seen from above, we do not have any missing data (all variables have 150 observations) but the scale (ranges) for every variable differ considerably (look the min and max values of sepal length and compare them with petal width).\n\n\n\n\n16.1.1 Normalisation\nSome algorithms are senstive to the size of variables. For example, if the sepal widths were in meters and the other variables in cm then an algorithm may underweight sepal widths. This means that we will need to rescale our variables to be in the safe side. There are two ways to put variables into the same scale: normalisation and standarisation.\n\nNormalisation: rescales a dataset so that each value falls between 0 and 1. It uses the following formula to do so:\n\n\\[xnew = (xi – xmin) / (xmax – xmin)\\]\n\nStandarisation: rescales a dataset to have a mean of 0 and a standard deviation of 1. It uses the following formula to do so:\n\n\\[xnew = (xi – x) / s\\]\n\n\n\n\n\n\nWhich one should we use?\n\n\n\nIf you cannot choose between them then try it both ways. You could compare the result with your raw data, the normalised data and the standardised data.\nThese blog posts may help you: (Statology) Standardization vs. Normalization: What’s the Difference? and (Analytics Vidhya) Feature Engineering: Scaling, Normalization, and Standardization (Updated 2023)).\n\n\nIn the code below we will normalise the data between 0 and 1 by using .fit_transform() from sklearn.\n\nfrom sklearn.preprocessing import MinMaxScaler\ncol_names = iris_df.columns\niris_df =  pd.DataFrame(MinMaxScaler().fit_transform(iris_df))\niris_df.columns = col_names # Column names were lost, so we need to re-introduce\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n\n\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n\n\n\n\n150 rows × 4 columns\n\n\n\nLet’s see how the the descriptive measures have changed after the transformation:\n\niris_df.describe()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n0.428704\n0.440556\n0.467458\n0.458056\n\n\nstd\n0.230018\n0.181611\n0.299203\n0.317599\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.222222\n0.333333\n0.101695\n0.083333\n\n\n50%\n0.416667\n0.416667\n0.567797\n0.500000\n\n\n75%\n0.583333\n0.541667\n0.694915\n0.708333\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\nGreat.\nOur dataset show us the length and width of both the sepal (leaf) and petals of 150 plants. The dataset is quite famous and you can find a wikipedia page with details of the dataset.\n\n\n\n\n\n\nQuestions\n\n\n\nTo motivate our exploration of the data, consider the sorts of questions we can ask:\n\nAre all our plants from the same species?\nDo some plants have similiar leaf and petal sizes?\nCan we differentiate between the plants using all 4 variables (dimensions)?\nDo we need to include both length and width, or can we reduce these dimensions and simplify our analysis?\n\n\n\n\n\n16.1.2 Initial exploration\nWe can explore a dataset with few variables using plots.\n\n16.1.2.1 Distribution (variabilty and density)\nWe’d like to see each variable’s distributions in terms of variablity and density. We have seen several ways to do this, but in this case we will be using a new plot type (a violin plot) to visualise the distribution.\nTo do so, we will be using seaborn’s violinplot(). Because we want to create a single plot with a violin plot per variable, we will need to transform our data from wide to a long format.\n\n\n\n\n\nExplanation of Violin plot. (Chambers 2017)\n\n\n\n\n\n\n\n\nWide vs long data\n\n\n\nA dataset can be written in two different formats: wide and long.\n\nwide: every row is a unique observation, where columns are variables (or attributes) describing the observation.\nlong: single observations are split into multiple rows. Usually, the first column contains the index to the observation, and there are two more columns: the name of the variable, and the actual value of the variable.\n\n\n\n\nWide vs long data (Source: Statology)\n\n\n\n\n\nimport seaborn as sns\n\n# some plots require a long dataframe structure\niris_df_long = iris_df.melt()\niris_df_long\n\n\n\n\n\n\n\n\nvariable\nvalue\n\n\n\n\n0\nsepal length (cm)\n0.222222\n\n\n1\nsepal length (cm)\n0.166667\n\n\n2\nsepal length (cm)\n0.111111\n\n\n3\nsepal length (cm)\n0.083333\n\n\n4\nsepal length (cm)\n0.194444\n\n\n...\n...\n...\n\n\n595\npetal width (cm)\n0.916667\n\n\n596\npetal width (cm)\n0.750000\n\n\n597\npetal width (cm)\n0.791667\n\n\n598\npetal width (cm)\n0.916667\n\n\n599\npetal width (cm)\n0.708333\n\n\n\n\n600 rows × 2 columns\n\n\n\nAnd now that we have transformed our data into a long data format, we can create the visualisation:\n\nsns.violinplot(data = iris_df_long, x = 'variable', y = 'value')\n\n&lt;Axes: xlabel='variable', ylabel='value'&gt;\n\n\n\n\n\n\n\n\n\n\n\n16.1.2.2 Correlations\nThe below plots use the wide data structure.\n\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n\n\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n\n\n\n\n150 rows × 4 columns\n\n\n\n\nsns.scatterplot(data = iris_df, x = 'sepal length (cm)', y = 'sepal width (cm)')\n\n&lt;Axes: xlabel='sepal length (cm)', ylabel='sepal width (cm)'&gt;\n\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data = iris_df, x = 'sepal length (cm)', y = 'petal length (cm)')\n\n&lt;Axes: xlabel='sepal length (cm)', ylabel='petal length (cm)'&gt;\n\n\n\n\n\n\n\n\n\nInteresting. There seem to be two groupings in the data.\nIt might be easier to look at all the variables at once.\n\nsns.pairplot(iris_df)\n\n\n\n\n\n\n\n\nThere seem to be some groupings in the data. Though we cannot easily identify which point corresponds to which row.",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Lab: Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#clustering",
    "href": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#clustering",
    "title": "16  Lab: Dimension Reduction",
    "section": "16.2 Clustering",
    "text": "16.2 Clustering\nA cluster is simply a group based on simliarity. There are several methods and we will use a relatively simple one called K-means clustering.\nIn K-means clustering an algorithm tries to group our items (plants in the iris dataset) based on similarity. We decide how many groups we want and the algorithm does the best it can (an accessible introduction to k-means clustering is here).\nTo start, we import the KMeans function from sklearn cluster module and turn our data into a matrix.\n\nfrom sklearn.cluster import KMeans\n\niris = iris_df.values\niris\n\narray([[0.22222222, 0.625     , 0.06779661, 0.04166667],\n       [0.16666667, 0.41666667, 0.06779661, 0.04166667],\n       [0.11111111, 0.5       , 0.05084746, 0.04166667],\n       [0.08333333, 0.45833333, 0.08474576, 0.04166667],\n       [0.19444444, 0.66666667, 0.06779661, 0.04166667],\n       [0.30555556, 0.79166667, 0.11864407, 0.125     ],\n       [0.08333333, 0.58333333, 0.06779661, 0.08333333],\n       [0.19444444, 0.58333333, 0.08474576, 0.04166667],\n       [0.02777778, 0.375     , 0.06779661, 0.04166667],\n       [0.16666667, 0.45833333, 0.08474576, 0.        ],\n       [0.30555556, 0.70833333, 0.08474576, 0.04166667],\n       [0.13888889, 0.58333333, 0.10169492, 0.04166667],\n       [0.13888889, 0.41666667, 0.06779661, 0.        ],\n       [0.        , 0.41666667, 0.01694915, 0.        ],\n       [0.41666667, 0.83333333, 0.03389831, 0.04166667],\n       [0.38888889, 1.        , 0.08474576, 0.125     ],\n       [0.30555556, 0.79166667, 0.05084746, 0.125     ],\n       [0.22222222, 0.625     , 0.06779661, 0.08333333],\n       [0.38888889, 0.75      , 0.11864407, 0.08333333],\n       [0.22222222, 0.75      , 0.08474576, 0.08333333],\n       [0.30555556, 0.58333333, 0.11864407, 0.04166667],\n       [0.22222222, 0.70833333, 0.08474576, 0.125     ],\n       [0.08333333, 0.66666667, 0.        , 0.04166667],\n       [0.22222222, 0.54166667, 0.11864407, 0.16666667],\n       [0.13888889, 0.58333333, 0.15254237, 0.04166667],\n       [0.19444444, 0.41666667, 0.10169492, 0.04166667],\n       [0.19444444, 0.58333333, 0.10169492, 0.125     ],\n       [0.25      , 0.625     , 0.08474576, 0.04166667],\n       [0.25      , 0.58333333, 0.06779661, 0.04166667],\n       [0.11111111, 0.5       , 0.10169492, 0.04166667],\n       [0.13888889, 0.45833333, 0.10169492, 0.04166667],\n       [0.30555556, 0.58333333, 0.08474576, 0.125     ],\n       [0.25      , 0.875     , 0.08474576, 0.        ],\n       [0.33333333, 0.91666667, 0.06779661, 0.04166667],\n       [0.16666667, 0.45833333, 0.08474576, 0.04166667],\n       [0.19444444, 0.5       , 0.03389831, 0.04166667],\n       [0.33333333, 0.625     , 0.05084746, 0.04166667],\n       [0.16666667, 0.66666667, 0.06779661, 0.        ],\n       [0.02777778, 0.41666667, 0.05084746, 0.04166667],\n       [0.22222222, 0.58333333, 0.08474576, 0.04166667],\n       [0.19444444, 0.625     , 0.05084746, 0.08333333],\n       [0.05555556, 0.125     , 0.05084746, 0.08333333],\n       [0.02777778, 0.5       , 0.05084746, 0.04166667],\n       [0.19444444, 0.625     , 0.10169492, 0.20833333],\n       [0.22222222, 0.75      , 0.15254237, 0.125     ],\n       [0.13888889, 0.41666667, 0.06779661, 0.08333333],\n       [0.22222222, 0.75      , 0.10169492, 0.04166667],\n       [0.08333333, 0.5       , 0.06779661, 0.04166667],\n       [0.27777778, 0.70833333, 0.08474576, 0.04166667],\n       [0.19444444, 0.54166667, 0.06779661, 0.04166667],\n       [0.75      , 0.5       , 0.62711864, 0.54166667],\n       [0.58333333, 0.5       , 0.59322034, 0.58333333],\n       [0.72222222, 0.45833333, 0.66101695, 0.58333333],\n       [0.33333333, 0.125     , 0.50847458, 0.5       ],\n       [0.61111111, 0.33333333, 0.61016949, 0.58333333],\n       [0.38888889, 0.33333333, 0.59322034, 0.5       ],\n       [0.55555556, 0.54166667, 0.62711864, 0.625     ],\n       [0.16666667, 0.16666667, 0.38983051, 0.375     ],\n       [0.63888889, 0.375     , 0.61016949, 0.5       ],\n       [0.25      , 0.29166667, 0.49152542, 0.54166667],\n       [0.19444444, 0.        , 0.42372881, 0.375     ],\n       [0.44444444, 0.41666667, 0.54237288, 0.58333333],\n       [0.47222222, 0.08333333, 0.50847458, 0.375     ],\n       [0.5       , 0.375     , 0.62711864, 0.54166667],\n       [0.36111111, 0.375     , 0.44067797, 0.5       ],\n       [0.66666667, 0.45833333, 0.57627119, 0.54166667],\n       [0.36111111, 0.41666667, 0.59322034, 0.58333333],\n       [0.41666667, 0.29166667, 0.52542373, 0.375     ],\n       [0.52777778, 0.08333333, 0.59322034, 0.58333333],\n       [0.36111111, 0.20833333, 0.49152542, 0.41666667],\n       [0.44444444, 0.5       , 0.6440678 , 0.70833333],\n       [0.5       , 0.33333333, 0.50847458, 0.5       ],\n       [0.55555556, 0.20833333, 0.66101695, 0.58333333],\n       [0.5       , 0.33333333, 0.62711864, 0.45833333],\n       [0.58333333, 0.375     , 0.55932203, 0.5       ],\n       [0.63888889, 0.41666667, 0.57627119, 0.54166667],\n       [0.69444444, 0.33333333, 0.6440678 , 0.54166667],\n       [0.66666667, 0.41666667, 0.6779661 , 0.66666667],\n       [0.47222222, 0.375     , 0.59322034, 0.58333333],\n       [0.38888889, 0.25      , 0.42372881, 0.375     ],\n       [0.33333333, 0.16666667, 0.47457627, 0.41666667],\n       [0.33333333, 0.16666667, 0.45762712, 0.375     ],\n       [0.41666667, 0.29166667, 0.49152542, 0.45833333],\n       [0.47222222, 0.29166667, 0.69491525, 0.625     ],\n       [0.30555556, 0.41666667, 0.59322034, 0.58333333],\n       [0.47222222, 0.58333333, 0.59322034, 0.625     ],\n       [0.66666667, 0.45833333, 0.62711864, 0.58333333],\n       [0.55555556, 0.125     , 0.57627119, 0.5       ],\n       [0.36111111, 0.41666667, 0.52542373, 0.5       ],\n       [0.33333333, 0.20833333, 0.50847458, 0.5       ],\n       [0.33333333, 0.25      , 0.57627119, 0.45833333],\n       [0.5       , 0.41666667, 0.61016949, 0.54166667],\n       [0.41666667, 0.25      , 0.50847458, 0.45833333],\n       [0.19444444, 0.125     , 0.38983051, 0.375     ],\n       [0.36111111, 0.29166667, 0.54237288, 0.5       ],\n       [0.38888889, 0.41666667, 0.54237288, 0.45833333],\n       [0.38888889, 0.375     , 0.54237288, 0.5       ],\n       [0.52777778, 0.375     , 0.55932203, 0.5       ],\n       [0.22222222, 0.20833333, 0.33898305, 0.41666667],\n       [0.38888889, 0.33333333, 0.52542373, 0.5       ],\n       [0.55555556, 0.54166667, 0.84745763, 1.        ],\n       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n       [0.77777778, 0.41666667, 0.83050847, 0.83333333],\n       [0.55555556, 0.375     , 0.77966102, 0.70833333],\n       [0.61111111, 0.41666667, 0.81355932, 0.875     ],\n       [0.91666667, 0.41666667, 0.94915254, 0.83333333],\n       [0.16666667, 0.20833333, 0.59322034, 0.66666667],\n       [0.83333333, 0.375     , 0.89830508, 0.70833333],\n       [0.66666667, 0.20833333, 0.81355932, 0.70833333],\n       [0.80555556, 0.66666667, 0.86440678, 1.        ],\n       [0.61111111, 0.5       , 0.69491525, 0.79166667],\n       [0.58333333, 0.29166667, 0.72881356, 0.75      ],\n       [0.69444444, 0.41666667, 0.76271186, 0.83333333],\n       [0.38888889, 0.20833333, 0.6779661 , 0.79166667],\n       [0.41666667, 0.33333333, 0.69491525, 0.95833333],\n       [0.58333333, 0.5       , 0.72881356, 0.91666667],\n       [0.61111111, 0.41666667, 0.76271186, 0.70833333],\n       [0.94444444, 0.75      , 0.96610169, 0.875     ],\n       [0.94444444, 0.25      , 1.        , 0.91666667],\n       [0.47222222, 0.08333333, 0.6779661 , 0.58333333],\n       [0.72222222, 0.5       , 0.79661017, 0.91666667],\n       [0.36111111, 0.33333333, 0.66101695, 0.79166667],\n       [0.94444444, 0.33333333, 0.96610169, 0.79166667],\n       [0.55555556, 0.29166667, 0.66101695, 0.70833333],\n       [0.66666667, 0.54166667, 0.79661017, 0.83333333],\n       [0.80555556, 0.5       , 0.84745763, 0.70833333],\n       [0.52777778, 0.33333333, 0.6440678 , 0.70833333],\n       [0.5       , 0.41666667, 0.66101695, 0.70833333],\n       [0.58333333, 0.33333333, 0.77966102, 0.83333333],\n       [0.80555556, 0.41666667, 0.81355932, 0.625     ],\n       [0.86111111, 0.33333333, 0.86440678, 0.75      ],\n       [1.        , 0.75      , 0.91525424, 0.79166667],\n       [0.58333333, 0.33333333, 0.77966102, 0.875     ],\n       [0.55555556, 0.33333333, 0.69491525, 0.58333333],\n       [0.5       , 0.25      , 0.77966102, 0.54166667],\n       [0.94444444, 0.41666667, 0.86440678, 0.91666667],\n       [0.55555556, 0.58333333, 0.77966102, 0.95833333],\n       [0.58333333, 0.45833333, 0.76271186, 0.70833333],\n       [0.47222222, 0.41666667, 0.6440678 , 0.70833333],\n       [0.72222222, 0.45833333, 0.74576271, 0.83333333],\n       [0.66666667, 0.45833333, 0.77966102, 0.95833333],\n       [0.72222222, 0.45833333, 0.69491525, 0.91666667],\n       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n       [0.69444444, 0.5       , 0.83050847, 0.91666667],\n       [0.66666667, 0.54166667, 0.79661017, 1.        ],\n       [0.66666667, 0.41666667, 0.71186441, 0.91666667],\n       [0.55555556, 0.20833333, 0.6779661 , 0.75      ],\n       [0.61111111, 0.41666667, 0.71186441, 0.79166667],\n       [0.52777778, 0.58333333, 0.74576271, 0.91666667],\n       [0.44444444, 0.41666667, 0.69491525, 0.70833333]])\n\n\nSpecify our number of clusters.\n\n\n\n\n\n\nIMPORTANT: Check if your data features are standardised/normalised!!\n\n\n\nBefore you apply techniques such as PCA, clustering or other feature embedding technniques (such as t-SNE, MDS, etc.). It is very important to make sure that the data features that go into these techniques are normalised/standardised: - you can bring the value ranges between 0 and 1 for all of them with a MixMax scaling operation - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html - you can standardise the values to have a mean of 0 and a standard deviation of 1, aka, z-score standardisation – https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html - Or make use of more specific normalisation operators that might be more suitable for a particular context. The scikit-learn collection is a good place to look for alternatives – https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\nThese operations ensure that the results are not biased/skewed/dominated by some of the inherent characteristics of the data that is simply due to the domain of values.\nScikit-learn has some very nice tutorial here: https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py\n\n\n\n\n\n\nDo-this-yourself: Check if we need to do any normalisation for this case?\n\n\n\nWe have already looked at how the data looks, what are the descriptive statistics look like, see if we need to do anything more?\n\nk_means = KMeans(n_clusters = 3, init = 'random',  n_init = 10)\n\nFit our kmeans model to the data\n\nk_means.fit(iris)\n\nKMeans(init='random', n_clusters=3, n_init=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(init='random', n_clusters=3, n_init=10)\n\n\nThe algorithm has assigned the a label to each row.\n\nk_means.labels_\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n       2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2,\n       2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0], dtype=int32)\n\n\nEach row has been assigned a label.\nTo tidy things up we should put everything into a dataframe.\n\niris_df['Three clusters'] = pd.Series(k_means.predict(iris_df.values), index = iris_df.index)\n\n\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nThree clusters\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n1\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n1\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n1\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n1\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n2\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n0\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n2\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n2\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n0\n\n\n\n\n150 rows × 5 columns\n\n\n\n\nsns.pairplot(iris_df, hue = 'Three clusters')\n\n\n\n\n\n\n\n\nThat seems quite nice. We can also do individual plots if preferred.\n\nsns.scatterplot(data = iris_df, x = 'sepal length (cm)', y = 'petal width (cm)', hue = 'Three clusters')\n\n&lt;Axes: xlabel='sepal length (cm)', ylabel='petal width (cm)'&gt;\n\n\n\n\n\n\n\n\n\nK-means works by clustering the data around central points (often called centroids, means or cluster centers). We can extract the cluster centres from the kmeans object.\n\nk_means.cluster_centers_\n\narray([[0.44125683, 0.30737705, 0.57571548, 0.54918033],\n       [0.19611111, 0.595     , 0.07830508, 0.06083333],\n       [0.70726496, 0.4508547 , 0.79704476, 0.82478632]])\n\n\nIt is tricky to plot these using seaborn but we can use a normal maplotlib scatter plot.\nLet us grab the groups.\n\ngroup1 = iris_df[iris_df['Three clusters'] == 0]\ngroup2 = iris_df[iris_df['Three clusters'] == 1]\ngroup3 = iris_df[iris_df['Three clusters'] == 2]\n\nGrab the centroids\n\nimport pandas as pd\n\ncentres = k_means.cluster_centers_\n\ndata = {'x': [centres[0][0], centres[1][0], centres[2][0]],\n        'y': [centres[0][3], centres[1][3], centres[2][3]]}\n\ndf = pd.DataFrame (data, columns = ['x', 'y'])\n\nCreate the plot\n\nimport matplotlib.pyplot as plt\n\n# Plot each group individually\nplt.scatter(\n    x = group1['sepal length (cm)'], \n    y = group1['petal width (cm)'], \n    alpha = 0.1, color = 'blue'\n)\n\nplt.scatter(\n    x = group2['sepal length (cm)'], \n    y = group2['petal width (cm)'], \n    alpha = 0.1, color = 'orange'\n)\n\nplt.scatter(\n    x = group3['sepal length (cm)'], \n    y = group3['petal width (cm)'], \n    alpha = 0.1, color = 'red'\n)\n\n# Plot cluster centres\nplt.scatter(\n    x = df['x'], \n    y = df['y'], \n    alpha = 1, color = 'black'\n)\n\n&lt;matplotlib.collections.PathCollection at 0x163096a10&gt;\n\n\n\n\n\n\n\n\n\n\n16.2.1 Number of clusters\nWhat happens if we change the number of clusters?\nTwo groups\n\nk_means_2 = KMeans(n_clusters = 2, init = 'random', n_init = 10)\nk_means_2.fit(iris)\niris_df['Two clusters'] = pd.Series(k_means_2.predict(iris_df.iloc[:,0:4].values), index = iris_df.index)\n\nNote that I have added a new column to the iris dataframe called ‘cluster 2 means’ and pass only our origonal 4 columns to the predict function (hence me using .iloc[:,0:4]).\nHow do our groupings look now (without plotting the cluster column)?\n\nsns.pairplot(iris_df.loc[:, iris_df.columns != 'Three clusters'], hue = 'Two clusters')\n\n\n\n\n\n\n\n\nHmm, does the data have more than two groups in it?\nPerhaps we should try 5 clusters instead.\n\nk_means_5 = KMeans(n_clusters = 5, init = 'random', n_init = 10)\nk_means_5.fit(iris)\niris_df['Five clusters'] = pd.Series(k_means_5.predict(iris_df.iloc[:,0:4].values), index = iris_df.index)\n\nPlot without the columns called ‘cluster’ and ‘Two cluster’\n\nsns.pairplot(iris_df.loc[:, (iris_df.columns != 'Three clusters') & (iris_df.columns != 'Two clusters')], hue = 'Five clusters')\n\n\n\n\n\n\n\n\n\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nThree clusters\nTwo clusters\nFive clusters\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n1\n1\n1\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n1\n1\n2\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n1\n1\n2\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n1\n1\n2\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n1\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n2\n0\n0\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n0\n0\n3\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n2\n0\n3\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n2\n0\n0\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n0\n0\n3\n\n\n\n\n150 rows × 7 columns\n\n\n\nWhich did best?\n\nk_means.inertia_\n\n6.982216473785234\n\n\n\nk_means_2.inertia_\n\n12.127790750538193\n\n\n\nk_means_5.inertia_\n\n4.580948640117293\n\n\nIt looks like our k = 5 model captures the data well. Intertia, looking at the sklearn documentation as the Sum of squared distances of samples to their closest cluster center..\nIf you want to dive further into this then Real Python’s practical guide to K-Means Clustering is quite good.\n\n\n16.3 Principal Component Analysis (PCA)\nPCA reduces the dimension of our data. The method derives point in an n dimentional space from our data which are uncorrelated.\nTo carry out a PCA on our Iris dataset where there are only two dimensions.\n\nfrom sklearn.decomposition import PCA\n\nn_components = 2\n\npca = PCA(n_components=n_components)\niris_pca = pca.fit(iris_df.iloc[:,0:4])\n\nWe can look at the components.\n\niris_pca.components_\n\narray([[ 0.42494212, -0.15074824,  0.61626702,  0.64568888],\n       [ 0.42320271,  0.90396711, -0.06038308, -0.00983925]])\n\n\nThese components are intersting. You may want to look at a PennState article on interpreting PCA components.\nOur second column, ‘sepal width (cm)’ is positively correlated with our second principle component whereas the first column ‘sepal length (cm)’ is postively correlated with both.\nYou may want to consider:\n\nDo we need more than two components?\nIs it useful to keep sepal length (cm) in the dataset?\n\nWe can also examine the explained variance of the each principle component.\n\niris_pca.explained_variance_\n\narray([0.23245325, 0.0324682 ])\n\n\nA nice worked example showing the link between the explained variance and the component is here.\nOur first principle component explains a lot more of the variance of data then the second.\nAnother way to explore these indicators is to look at the explained_variance_ratio_ values. These present a similar information but provide them as percentage values so they are easier to interpret. You can also create a plot and see how these percentages add up. In this case, the first two components add up to 0.96. Which means the first two features are able to represent around 96% of the variation in the data, not bad. These values are not always this high.\nA high value that is close to 100% means that the PCA is able to represent much of the variance and they will be good representations of the data without losing a lot of that variance in the underlying features. This of course is based on an assumption that variance is a good proxy about how informative a feature is.\n\niris_pca.explained_variance_ratio_\n\narray([0.84136038, 0.11751808])\n\n\n\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\n\n\n\n\n\n\n\n\n\n16.3.1 Dimension reduction\nFor our purposes, we are interested in using PCA for reducing the number of dimension in our data whilst preseving the maximal data variance.\nWe can extract the projected components from the model.\n\niris_pca_vals = pca.fit_transform(iris_df.iloc[:,0:4])\n\nThe numpy arrays contains the projected values.\n\ntype(iris_pca_vals)\n\nnumpy.ndarray\n\n\n\niris_pca_vals\n\narray([[-6.30702931e-01,  1.07577910e-01],\n       [-6.22904943e-01, -1.04259833e-01],\n       [-6.69520395e-01, -5.14170597e-02],\n       [-6.54152759e-01, -1.02884871e-01],\n       [-6.48788056e-01,  1.33487576e-01],\n       [-5.35272778e-01,  2.89615724e-01],\n       [-6.56537790e-01,  1.07244911e-02],\n       [-6.25780499e-01,  5.71335411e-02],\n       [-6.75643504e-01, -2.00703283e-01],\n       [-6.45644619e-01, -6.72080097e-02],\n       [-5.97408238e-01,  2.17151953e-01],\n       [-6.38943190e-01,  3.25988375e-02],\n       [-6.61612593e-01, -1.15605495e-01],\n       [-7.51967943e-01, -1.71313322e-01],\n       [-6.00371589e-01,  3.80240692e-01],\n       [-5.52157227e-01,  5.15255982e-01],\n       [-5.77053593e-01,  2.93709492e-01],\n       [-6.03799228e-01,  1.07167941e-01],\n       [-5.20483461e-01,  2.87627289e-01],\n       [-6.12197555e-01,  2.19140388e-01],\n       [-5.57674300e-01,  1.02109180e-01],\n       [-5.79012675e-01,  1.81065123e-01],\n       [-7.37784662e-01,  9.05588211e-02],\n       [-5.06093857e-01,  2.79470846e-02],\n       [-6.07607579e-01,  2.95285112e-02],\n       [-5.90210587e-01, -9.45510863e-02],\n       [-5.61527888e-01,  5.52901611e-02],\n       [-6.08453780e-01,  1.18310099e-01],\n       [-6.12617807e-01,  8.16682448e-02],\n       [-6.38184784e-01, -5.44873860e-02],\n       [-6.20099660e-01, -8.03970516e-02],\n       [-5.24757301e-01,  1.03336126e-01],\n       [-6.73044544e-01,  3.44711846e-01],\n       [-6.27455379e-01,  4.18257508e-01],\n       [-6.18740916e-01, -6.76179787e-02],\n       [-6.44553756e-01, -1.51267253e-02],\n       [-5.93932344e-01,  1.55623876e-01],\n       [-6.87495707e-01,  1.22141914e-01],\n       [-6.92369885e-01, -1.62014545e-01],\n       [-6.13976551e-01,  6.88891719e-02],\n       [-6.26048380e-01,  9.64357527e-02],\n       [-6.09693996e-01, -4.14325957e-01],\n       [-7.04932239e-01, -8.66839521e-02],\n       [-5.14001659e-01,  9.21355196e-02],\n       [-5.43513037e-01,  2.14636651e-01],\n       [-6.07805187e-01, -1.16425433e-01],\n       [-6.28656055e-01,  2.18526915e-01],\n       [-6.70879139e-01, -6.41961326e-02],\n       [-6.09212186e-01,  2.05396323e-01],\n       [-6.29944525e-01,  2.04916869e-02],\n       [ 2.79951766e-01,  1.79245790e-01],\n       [ 2.15141376e-01,  1.10348921e-01],\n       [ 3.22223106e-01,  1.27368010e-01],\n       [ 5.94030131e-02, -3.28502275e-01],\n       [ 2.62515235e-01, -2.95800761e-02],\n       [ 1.03831043e-01, -1.21781742e-01],\n       [ 2.44850362e-01,  1.33801733e-01],\n       [-1.71529386e-01, -3.52976762e-01],\n       [ 2.14230599e-01,  2.06607890e-02],\n       [ 1.53249619e-02, -2.12494509e-01],\n       [-1.13710323e-01, -4.93929201e-01],\n       [ 1.37348380e-01, -2.06894998e-02],\n       [ 4.39928190e-02, -3.06159511e-01],\n       [ 1.92559767e-01, -3.95507760e-02],\n       [-8.26091518e-03, -8.66610981e-02],\n       [ 2.19485489e-01,  1.09383928e-01],\n       [ 1.33272148e-01, -5.90267184e-02],\n       [-5.75757060e-04, -1.42367733e-01],\n       [ 2.54345249e-01, -2.89815304e-01],\n       [-5.60800300e-03, -2.39572672e-01],\n       [ 2.68168358e-01,  4.72705335e-02],\n       [ 9.88208151e-02, -6.96420088e-02],\n       [ 2.89086481e-01, -1.69157553e-01],\n       [ 1.45033538e-01, -7.63961345e-02],\n       [ 1.59287093e-01,  2.19853643e-04],\n       [ 2.13962718e-01,  5.99630005e-02],\n       [ 2.91913782e-01,  4.04990109e-03],\n       [ 3.69148997e-01,  6.43480720e-02],\n       [ 1.86769115e-01, -4.96694916e-02],\n       [-6.87697501e-02, -1.85648007e-01],\n       [-2.15759776e-02, -2.87970157e-01],\n       [-5.89248844e-02, -2.86536746e-01],\n       [ 3.23412419e-02, -1.41140786e-01],\n       [ 2.88906394e-01, -1.31550706e-01],\n       [ 1.09664252e-01, -8.25379800e-02],\n       [ 1.82266934e-01,  1.38247021e-01],\n       [ 2.77724803e-01,  1.05903632e-01],\n       [ 1.95615410e-01, -2.38550997e-01],\n       [ 3.76839264e-02, -5.41130122e-02],\n       [ 4.68406593e-02, -2.53171683e-01],\n       [ 5.54365941e-02, -2.19190186e-01],\n       [ 1.75833387e-01, -8.62037590e-04],\n       [ 4.90676225e-02, -1.79829525e-01],\n       [-1.53444261e-01, -3.78886428e-01],\n       [ 6.69726607e-02, -1.68132343e-01],\n       [ 3.30293747e-02, -4.29708545e-02],\n       [ 6.62142547e-02, -8.10461198e-02],\n       [ 1.35679197e-01, -2.32914079e-02],\n       [-1.58634575e-01, -2.89139847e-01],\n       [ 6.20502279e-02, -1.17687974e-01],\n       [ 6.22771338e-01,  1.16807265e-01],\n       [ 3.46009609e-01, -1.56291874e-01],\n       [ 6.17986434e-01,  1.00519741e-01],\n       [ 4.17789309e-01, -2.68903690e-02],\n       [ 5.63621248e-01,  3.05994289e-02],\n       [ 7.50122599e-01,  1.52133800e-01],\n       [ 1.35857804e-01, -3.30462554e-01],\n       [ 6.08945212e-01,  8.35018443e-02],\n       [ 5.11020215e-01, -1.32575915e-01],\n       [ 7.20608541e-01,  3.34580389e-01],\n       [ 4.24135062e-01,  1.13914054e-01],\n       [ 4.37723702e-01, -8.78049736e-02],\n       [ 5.40793776e-01,  6.93466165e-02],\n       [ 3.63226514e-01, -2.42764625e-01],\n       [ 4.74246948e-01, -1.20676423e-01],\n       [ 5.13932631e-01,  9.88816323e-02],\n       [ 4.24670824e-01,  3.53096310e-02],\n       [ 7.49026039e-01,  4.63778390e-01],\n       [ 8.72194272e-01,  9.33798117e-03],\n       [ 2.82963372e-01, -3.18443776e-01],\n       [ 6.14733184e-01,  1.53566018e-01],\n       [ 3.22133832e-01, -1.40500924e-01],\n       [ 7.58030401e-01,  8.79453649e-02],\n       [ 3.57235237e-01, -9.50568671e-02],\n       [ 5.31036706e-01,  1.68539991e-01],\n       [ 5.46962123e-01,  1.87812429e-01],\n       [ 3.28704908e-01, -6.81237595e-02],\n       [ 3.14783811e-01, -5.57223965e-03],\n       [ 5.16585543e-01, -5.40299414e-02],\n       [ 4.84826663e-01,  1.15348658e-01],\n       [ 6.33043632e-01,  5.92290940e-02],\n       [ 6.87490917e-01,  4.91179916e-01],\n       [ 5.43489246e-01, -5.44399104e-02],\n       [ 2.91133358e-01, -5.82085481e-02],\n       [ 3.05410131e-01, -1.61757644e-01],\n       [ 7.63507935e-01,  1.68186703e-01],\n       [ 5.47805644e-01,  1.58976299e-01],\n       [ 4.06585699e-01,  6.12192966e-02],\n       [ 2.92534659e-01, -1.63044284e-02],\n       [ 5.35871344e-01,  1.19790986e-01],\n       [ 6.13864965e-01,  9.30029331e-02],\n       [ 5.58343139e-01,  1.22041374e-01],\n       [ 3.46009609e-01, -1.56291874e-01],\n       [ 6.23819644e-01,  1.39763503e-01],\n       [ 6.38651518e-01,  1.66900115e-01],\n       [ 5.51461624e-01,  5.98413741e-02],\n       [ 4.07146497e-01, -1.71820871e-01],\n       [ 4.47142619e-01,  3.75600193e-02],\n       [ 4.88207585e-01,  1.49677521e-01],\n       [ 3.12066323e-01, -3.11303854e-02]])\n\n\nEach row corresponds to a row in our data.\n\niris_pca_vals.shape\n\n(150, 2)\n\n\n\niris_df.shape\n\n(150, 7)\n\n\nWe can add the component to our dataset. I prefer to keep everything in one table and it is not at all required. You can just assign the values whichever variables you prefer.\n\niris_df['c1'] = [item[0] for item in iris_pca_vals]\niris_df['c2'] = [item[1] for item in iris_pca_vals]\n\n\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nThree clusters\nTwo clusters\nFive clusters\nc1\nc2\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n1\n1\n1\n-0.630703\n0.107578\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n1\n1\n2\n-0.622905\n-0.104260\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n1\n1\n2\n-0.669520\n-0.051417\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n1\n1\n2\n-0.654153\n-0.102885\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n1\n1\n1\n-0.648788\n0.133488\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n2\n0\n0\n0.551462\n0.059841\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n0\n0\n3\n0.407146\n-0.171821\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n2\n0\n3\n0.447143\n0.037560\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n2\n0\n0\n0.488208\n0.149678\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n0\n0\n3\n0.312066\n-0.031130\n\n\n\n\n150 rows × 9 columns\n\n\n\nPlotting out our data on our new two component space.\n\nsns.scatterplot(data = iris_df, x = 'c1', y = 'c2')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\n\n\n\n\nWe have reduced our three dimensions to two.\nWe can also colour by our clusters. What does this show us and is it useful?\n\nsns.scatterplot(data = iris_df, x = 'c1', y = 'c2', hue = 'Three clusters')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\n\n\n\n\n\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nThree clusters\nTwo clusters\nFive clusters\nc1\nc2\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n1\n1\n1\n-0.630703\n0.107578\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n1\n1\n2\n-0.622905\n-0.104260\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n1\n1\n2\n-0.669520\n-0.051417\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n1\n1\n2\n-0.654153\n-0.102885\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n1\n1\n1\n-0.648788\n0.133488\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n2\n0\n0\n0.551462\n0.059841\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n0\n0\n3\n0.407146\n-0.171821\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n2\n0\n3\n0.447143\n0.037560\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n2\n0\n0\n0.488208\n0.149678\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n0\n0\n3\n0.312066\n-0.031130\n\n\n\n\n150 rows × 9 columns\n\n\n\n\n\n16.3.2 PCA to Clusters\nWe have reduced our 4D dataset to 2D whilst keeping the data variance. Reducing the data to fewer dimensions can help with the ‘curse of dimensionality’, reduce the change of overfitting a machine learning model (see here) and reduce the computational complexity of a model fit.\nPutting our new dimensions into a kMeans model\n\nk_means_pca = KMeans(n_clusters = 3, init = 'random', n_init = 10)\niris_pca_kmeans = k_means_pca.fit(iris_df.iloc[:,-2:])\n\n\ntype(iris_df.iloc[:,-2:].values)\n\nnumpy.ndarray\n\n\n\niris_df['PCA 3 clusters'] = pd.Series(k_means_pca.predict(iris_df.iloc[:,-2:].values), index = iris_df.index)\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nThree clusters\nTwo clusters\nFive clusters\nc1\nc2\nPCA 3 clusters\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n1\n1\n1\n-0.630703\n0.107578\n0\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n1\n1\n2\n-0.622905\n-0.104260\n0\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n1\n1\n2\n-0.669520\n-0.051417\n0\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n1\n1\n2\n-0.654153\n-0.102885\n0\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n1\n1\n1\n-0.648788\n0.133488\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n2\n0\n0\n0.551462\n0.059841\n1\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n0\n0\n3\n0.407146\n-0.171821\n2\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n2\n0\n3\n0.447143\n0.037560\n1\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n2\n0\n0\n0.488208\n0.149678\n1\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n0\n0\n3\n0.312066\n-0.031130\n2\n\n\n\n\n150 rows × 10 columns\n\n\n\nAs we only have two dimensions we can easily plot this on a single scatterplot.\n\n# a different seaborn theme\n# see https://python-graph-gallery.com/104-seaborn-themes/\nsns.set_style(\"darkgrid\")\nsns.scatterplot(data = iris_df, x = 'c1', y = 'c2', hue = 'PCA 3 clusters')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\n\n\n\n\nI suspect having two clusters would work better. We should try a few different models.\nCopying the code from here we can fit multiple numbers of clusters.\n\nks = range(1, 10)\ninertias = [] # Create an empty list (will be populated later)\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k, n_init = 10)\n    \n    # Fit model to samples\n    model.fit(iris_df.iloc[:,-2:])\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n    \nplt.plot(ks, inertias, '-o', color='black')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n\n\n\n\n\n\n\n\nThree seems ok. We clearly want no more than three.\nThese types of plots show an point about model complexity. More free parameters in the model (here the number of clusters) will improve how well the model captures the data, often with reducing returns. However, a model which overfits the data will not be able to fit new data well - referred to overfitting. Randomish internet blogs introduce the topic pretty well, see here, and also wikipedia, see here.\n\n\n16.3.3 Missing values\nFinally, how we deal with missing values can impact the results of PCA and kMeans clustering.\nLets us load in the iris dataset again and randomly remove 10% of the data (see code from here).\n\nimport numpy as np\n\nx = load_iris()\n\n\niris_df = pd.DataFrame(x.data, columns = x.feature_names)\n\nmask = np.random.choice([True, False], size = iris_df.shape, p = [0.2, 0.8])\nmask[mask.all(1),-1] = 0\n\ndf = iris_df.mask(mask)\n\ndf.isna().sum()\n\nsepal length (cm)    23\nsepal width (cm)     28\npetal length (cm)    21\npetal width (cm)     31\ndtype: int64\n\n\n\ndf\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\nNaN\nNaN\nNaN\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\nNaN\n0.2\n\n\n4\n5.0\n3.6\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\nNaN\nNaN\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\nNaN\nNaN\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\nNaN\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\nAbout 20% of the data is randomly an NaN.\n\n16.3.3.1 Zeroing\nWe can 0 them and fit our models.\n\ndf_1 = df.copy()\ndf_1 = df_1.fillna(0)\n\n\ndf_1\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n0.0\n0.0\n0.0\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n0.0\n0.2\n\n\n4\n5.0\n3.6\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n0.0\n0.0\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n0.0\n0.0\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n0.0\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\n\nk_means_zero = KMeans(n_clusters = 4, init = 'random', n_init = 10)\nk_means_zero.fit(df_1)\ndf_1['Four clusters'] = pd.Series(k_means_zero.predict(df_1.iloc[:,0:4].values), index = df_1.index)\nsns.pairplot(df_1, hue = 'Four clusters')\n\n\n\n\n\n\n\n\nWhat impact has zeroing the values had on our results?\nNow, onto PCA.\n\n# PCA analysis\nn_components = 2\n\npca = PCA(n_components=n_components)\ndf_1_pca = pca.fit(df_1.iloc[:,0:4])\n\n# Extract projected values\ndf_1_pca_vals = df_1_pca.transform(df_1.iloc[:,0:4])\ndf_1['c1'] = [item[0] for item in df_1_pca_vals]\ndf_1['c2'] = [item[1] for item in df_1_pca_vals]\n\nsns.scatterplot(data = df_1, x = 'c1', y = 'c2')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\n\n\n\n\n\ndf_1_pca.explained_variance_\n\narray([5.3176492 , 4.20322317])\n\n\n\ndf_1_pca.components_\n\narray([[-0.84439271, -0.02201056, -0.52209728, -0.11802933],\n       [-0.53545858,  0.00318789,  0.82384805,  0.18587184]])\n\n\n\n\n16.3.3.2 Replacing with the average\n\ndf_2 = df.copy()\nfor i in range(4):\n    df_2.iloc[:,i] = df_2.iloc[:,i].fillna(df_2.iloc[:,i].mean())\n\n\ndf_2\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.100000\n3.500000\n1.400000\n0.200000\n\n\n1\n5.806299\n3.045902\n3.735659\n0.200000\n\n\n2\n4.700000\n3.200000\n1.300000\n0.200000\n\n\n3\n4.600000\n3.100000\n3.735659\n0.200000\n\n\n4\n5.000000\n3.600000\n3.735659\n1.178992\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.700000\n3.000000\n3.735659\n1.178992\n\n\n146\n6.300000\n2.500000\n5.000000\n1.900000\n\n\n147\n6.500000\n3.045902\n3.735659\n2.000000\n\n\n148\n6.200000\n3.400000\n5.400000\n2.300000\n\n\n149\n5.806299\n3.000000\n5.100000\n1.800000\n\n\n\n\n150 rows × 4 columns\n\n\n\n\nk_means_zero = KMeans(n_clusters = 4, init = 'random', n_init = 10)\nk_means_zero.fit(df_2)\ndf_2['Four clusters'] = pd.Series(k_means_zero.predict(df_2.iloc[:,0:4].values), index = df_2.index)\nsns.pairplot(df_2, hue = 'Four clusters')\n\n\n\n\n\n\n\n\n\n# PCA analysis\nn_components = 2\n\npca = PCA(n_components=n_components)\ndf_2_pca = pca.fit(df_2.iloc[:,0:4])\n\n# Extract projected values\ndf_2_pca_vals = df_2_pca.transform(df_2.iloc[:,0:4])\ndf_2['c1'] = [item[0] for item in df_2_pca_vals]\ndf_2['c2'] = [item[1] for item in df_2_pca_vals]\n\nsns.scatterplot(data = df_2, x = 'c1', y = 'c2')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\n\n\n\n\n\ndf_2_pca.explained_variance_\n\narray([3.31519797, 0.27508431])\n\n\n\ndf_2_pca.components_\n\narray([[ 0.3347291 , -0.07564839,  0.88336309,  0.31922313],\n       [ 0.86390194,  0.33976241, -0.34674533,  0.13417384]])\n\n\n\n\n\n\n16.4 Useful resources\nThe scikit learn UserGuide is very good. Both approaches here are often referred to as unsupervised learning methods and you can find the scikit learn section on these here.\nIf you have issues with the documentation then also look at the scikit-learn examples.\nAlso, in no particular order:\n\nThe In-Depth sections of the Python Data Science Handbook. More for machine learning but interesting all the same.\nPython for Data Analysis (ebook is available via Warwick library)\n\nIn case you are bored:\n\nStack abuse - Some fun blog entries to look at\nTowards data science - a blog that contains a mix of intro, intermediate and advanced topics. Nice to skim through to try and undrestand something new.\n\nPlease do try out some of the techniques detailed in the lecture material The simple examples found in the scikit learn documentation are rather good. Generally, I find it much easier to try to understand a method using a simple dataset.\n\n\n\n\n\n\n\n\n\nFisher, R. A. 1936. “Iris.” UCI Machine Learning Repository. https://doi.org/10.24432/C56C76.",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Lab: Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#principal-component-analysis-pca",
    "href": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#principal-component-analysis-pca",
    "title": "16  Lab: Dimension Reduction",
    "section": "16.3 Principal Component Analysis (PCA)",
    "text": "16.3 Principal Component Analysis (PCA)\nPCA reduces the dimension of our data. The method derives point in an n dimentional space from our data which are uncorrelated.\nTo carry out a PCA on our Iris dataset where there are only two dimensions.\n\nfrom sklearn.decomposition import PCA\n\nn_components = 2\n\npca = PCA(n_components=n_components)\niris_pca = pca.fit(iris_df.iloc[:,0:4])\n\nWe can look at the components.\n\niris_pca.components_\n\narray([[ 0.42494212, -0.15074824,  0.61626702,  0.64568888],\n       [ 0.42320271,  0.90396711, -0.06038308, -0.00983925]])\n\n\nThese components are intersting. You may want to look at a PennState article on interpreting PCA components.\nOur second column, ‘sepal width (cm)’ is positively correlated with our second principle component whereas the first column ‘sepal length (cm)’ is postively correlated with both.\nYou may want to consider:\n\nDo we need more than two components?\nIs it useful to keep sepal length (cm) in the dataset?\n\nWe can also examine the explained variance of the each principle component.\n\niris_pca.explained_variance_\n\narray([0.23245325, 0.0324682 ])\n\n\nA nice worked example showing the link between the explained variance and the component is here.\nOur first principle component explains a lot more of the variance of data then the second.\nAnother way to explore these indicators is to look at the explained_variance_ratio_ values. These present a similar information but provide them as percentage values so they are easier to interpret. You can also create a plot and see how these percentages add up. In this case, the first two components add up to 0.96. Which means the first two features are able to represent around 96% of the variation in the data, not bad. These values are not always this high.\nA high value that is close to 100% means that the PCA is able to represent much of the variance and they will be good representations of the data without losing a lot of that variance in the underlying features. This of course is based on an assumption that variance is a good proxy about how informative a feature is.\n\niris_pca.explained_variance_ratio_\n\narray([0.84136038, 0.11751808])\n\n\n\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\n\n\n\n\n\n\n\n\n\n16.3.1 Dimension reduction\nFor our purposes, we are interested in using PCA for reducing the number of dimension in our data whilst preseving the maximal data variance.\nWe can extract the projected components from the model.\n\niris_pca_vals = pca.fit_transform(iris_df.iloc[:,0:4])\n\nThe numpy arrays contains the projected values.\n\ntype(iris_pca_vals)\n\nnumpy.ndarray\n\n\n\niris_pca_vals\n\narray([[-6.30702931e-01,  1.07577910e-01],\n       [-6.22904943e-01, -1.04259833e-01],\n       [-6.69520395e-01, -5.14170597e-02],\n       [-6.54152759e-01, -1.02884871e-01],\n       [-6.48788056e-01,  1.33487576e-01],\n       [-5.35272778e-01,  2.89615724e-01],\n       [-6.56537790e-01,  1.07244911e-02],\n       [-6.25780499e-01,  5.71335411e-02],\n       [-6.75643504e-01, -2.00703283e-01],\n       [-6.45644619e-01, -6.72080097e-02],\n       [-5.97408238e-01,  2.17151953e-01],\n       [-6.38943190e-01,  3.25988375e-02],\n       [-6.61612593e-01, -1.15605495e-01],\n       [-7.51967943e-01, -1.71313322e-01],\n       [-6.00371589e-01,  3.80240692e-01],\n       [-5.52157227e-01,  5.15255982e-01],\n       [-5.77053593e-01,  2.93709492e-01],\n       [-6.03799228e-01,  1.07167941e-01],\n       [-5.20483461e-01,  2.87627289e-01],\n       [-6.12197555e-01,  2.19140388e-01],\n       [-5.57674300e-01,  1.02109180e-01],\n       [-5.79012675e-01,  1.81065123e-01],\n       [-7.37784662e-01,  9.05588211e-02],\n       [-5.06093857e-01,  2.79470846e-02],\n       [-6.07607579e-01,  2.95285112e-02],\n       [-5.90210587e-01, -9.45510863e-02],\n       [-5.61527888e-01,  5.52901611e-02],\n       [-6.08453780e-01,  1.18310099e-01],\n       [-6.12617807e-01,  8.16682448e-02],\n       [-6.38184784e-01, -5.44873860e-02],\n       [-6.20099660e-01, -8.03970516e-02],\n       [-5.24757301e-01,  1.03336126e-01],\n       [-6.73044544e-01,  3.44711846e-01],\n       [-6.27455379e-01,  4.18257508e-01],\n       [-6.18740916e-01, -6.76179787e-02],\n       [-6.44553756e-01, -1.51267253e-02],\n       [-5.93932344e-01,  1.55623876e-01],\n       [-6.87495707e-01,  1.22141914e-01],\n       [-6.92369885e-01, -1.62014545e-01],\n       [-6.13976551e-01,  6.88891719e-02],\n       [-6.26048380e-01,  9.64357527e-02],\n       [-6.09693996e-01, -4.14325957e-01],\n       [-7.04932239e-01, -8.66839521e-02],\n       [-5.14001659e-01,  9.21355196e-02],\n       [-5.43513037e-01,  2.14636651e-01],\n       [-6.07805187e-01, -1.16425433e-01],\n       [-6.28656055e-01,  2.18526915e-01],\n       [-6.70879139e-01, -6.41961326e-02],\n       [-6.09212186e-01,  2.05396323e-01],\n       [-6.29944525e-01,  2.04916869e-02],\n       [ 2.79951766e-01,  1.79245790e-01],\n       [ 2.15141376e-01,  1.10348921e-01],\n       [ 3.22223106e-01,  1.27368010e-01],\n       [ 5.94030131e-02, -3.28502275e-01],\n       [ 2.62515235e-01, -2.95800761e-02],\n       [ 1.03831043e-01, -1.21781742e-01],\n       [ 2.44850362e-01,  1.33801733e-01],\n       [-1.71529386e-01, -3.52976762e-01],\n       [ 2.14230599e-01,  2.06607890e-02],\n       [ 1.53249619e-02, -2.12494509e-01],\n       [-1.13710323e-01, -4.93929201e-01],\n       [ 1.37348380e-01, -2.06894998e-02],\n       [ 4.39928190e-02, -3.06159511e-01],\n       [ 1.92559767e-01, -3.95507760e-02],\n       [-8.26091518e-03, -8.66610981e-02],\n       [ 2.19485489e-01,  1.09383928e-01],\n       [ 1.33272148e-01, -5.90267184e-02],\n       [-5.75757060e-04, -1.42367733e-01],\n       [ 2.54345249e-01, -2.89815304e-01],\n       [-5.60800300e-03, -2.39572672e-01],\n       [ 2.68168358e-01,  4.72705335e-02],\n       [ 9.88208151e-02, -6.96420088e-02],\n       [ 2.89086481e-01, -1.69157553e-01],\n       [ 1.45033538e-01, -7.63961345e-02],\n       [ 1.59287093e-01,  2.19853643e-04],\n       [ 2.13962718e-01,  5.99630005e-02],\n       [ 2.91913782e-01,  4.04990109e-03],\n       [ 3.69148997e-01,  6.43480720e-02],\n       [ 1.86769115e-01, -4.96694916e-02],\n       [-6.87697501e-02, -1.85648007e-01],\n       [-2.15759776e-02, -2.87970157e-01],\n       [-5.89248844e-02, -2.86536746e-01],\n       [ 3.23412419e-02, -1.41140786e-01],\n       [ 2.88906394e-01, -1.31550706e-01],\n       [ 1.09664252e-01, -8.25379800e-02],\n       [ 1.82266934e-01,  1.38247021e-01],\n       [ 2.77724803e-01,  1.05903632e-01],\n       [ 1.95615410e-01, -2.38550997e-01],\n       [ 3.76839264e-02, -5.41130122e-02],\n       [ 4.68406593e-02, -2.53171683e-01],\n       [ 5.54365941e-02, -2.19190186e-01],\n       [ 1.75833387e-01, -8.62037590e-04],\n       [ 4.90676225e-02, -1.79829525e-01],\n       [-1.53444261e-01, -3.78886428e-01],\n       [ 6.69726607e-02, -1.68132343e-01],\n       [ 3.30293747e-02, -4.29708545e-02],\n       [ 6.62142547e-02, -8.10461198e-02],\n       [ 1.35679197e-01, -2.32914079e-02],\n       [-1.58634575e-01, -2.89139847e-01],\n       [ 6.20502279e-02, -1.17687974e-01],\n       [ 6.22771338e-01,  1.16807265e-01],\n       [ 3.46009609e-01, -1.56291874e-01],\n       [ 6.17986434e-01,  1.00519741e-01],\n       [ 4.17789309e-01, -2.68903690e-02],\n       [ 5.63621248e-01,  3.05994289e-02],\n       [ 7.50122599e-01,  1.52133800e-01],\n       [ 1.35857804e-01, -3.30462554e-01],\n       [ 6.08945212e-01,  8.35018443e-02],\n       [ 5.11020215e-01, -1.32575915e-01],\n       [ 7.20608541e-01,  3.34580389e-01],\n       [ 4.24135062e-01,  1.13914054e-01],\n       [ 4.37723702e-01, -8.78049736e-02],\n       [ 5.40793776e-01,  6.93466165e-02],\n       [ 3.63226514e-01, -2.42764625e-01],\n       [ 4.74246948e-01, -1.20676423e-01],\n       [ 5.13932631e-01,  9.88816323e-02],\n       [ 4.24670824e-01,  3.53096310e-02],\n       [ 7.49026039e-01,  4.63778390e-01],\n       [ 8.72194272e-01,  9.33798117e-03],\n       [ 2.82963372e-01, -3.18443776e-01],\n       [ 6.14733184e-01,  1.53566018e-01],\n       [ 3.22133832e-01, -1.40500924e-01],\n       [ 7.58030401e-01,  8.79453649e-02],\n       [ 3.57235237e-01, -9.50568671e-02],\n       [ 5.31036706e-01,  1.68539991e-01],\n       [ 5.46962123e-01,  1.87812429e-01],\n       [ 3.28704908e-01, -6.81237595e-02],\n       [ 3.14783811e-01, -5.57223965e-03],\n       [ 5.16585543e-01, -5.40299414e-02],\n       [ 4.84826663e-01,  1.15348658e-01],\n       [ 6.33043632e-01,  5.92290940e-02],\n       [ 6.87490917e-01,  4.91179916e-01],\n       [ 5.43489246e-01, -5.44399104e-02],\n       [ 2.91133358e-01, -5.82085481e-02],\n       [ 3.05410131e-01, -1.61757644e-01],\n       [ 7.63507935e-01,  1.68186703e-01],\n       [ 5.47805644e-01,  1.58976299e-01],\n       [ 4.06585699e-01,  6.12192966e-02],\n       [ 2.92534659e-01, -1.63044284e-02],\n       [ 5.35871344e-01,  1.19790986e-01],\n       [ 6.13864965e-01,  9.30029331e-02],\n       [ 5.58343139e-01,  1.22041374e-01],\n       [ 3.46009609e-01, -1.56291874e-01],\n       [ 6.23819644e-01,  1.39763503e-01],\n       [ 6.38651518e-01,  1.66900115e-01],\n       [ 5.51461624e-01,  5.98413741e-02],\n       [ 4.07146497e-01, -1.71820871e-01],\n       [ 4.47142619e-01,  3.75600193e-02],\n       [ 4.88207585e-01,  1.49677521e-01],\n       [ 3.12066323e-01, -3.11303854e-02]])\n\n\nEach row corresponds to a row in our data.\n\niris_pca_vals.shape\n\n(150, 2)\n\n\n\niris_df.shape\n\n(150, 7)\n\n\nWe can add the component to our dataset. I prefer to keep everything in one table and it is not at all required. You can just assign the values whichever variables you prefer.\n\niris_df['c1'] = [item[0] for item in iris_pca_vals]\niris_df['c2'] = [item[1] for item in iris_pca_vals]\n\n\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nThree clusters\nTwo clusters\nFive clusters\nc1\nc2\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n1\n1\n1\n-0.630703\n0.107578\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n1\n1\n2\n-0.622905\n-0.104260\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n1\n1\n2\n-0.669520\n-0.051417\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n1\n1\n2\n-0.654153\n-0.102885\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n1\n1\n1\n-0.648788\n0.133488\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n2\n0\n0\n0.551462\n0.059841\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n0\n0\n3\n0.407146\n-0.171821\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n2\n0\n3\n0.447143\n0.037560\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n2\n0\n0\n0.488208\n0.149678\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n0\n0\n3\n0.312066\n-0.031130\n\n\n\n\n150 rows × 9 columns\n\n\n\nPlotting out our data on our new two component space.\n\nsns.scatterplot(data = iris_df, x = 'c1', y = 'c2')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\n\n\n\n\nWe have reduced our three dimensions to two.\nWe can also colour by our clusters. What does this show us and is it useful?\n\nsns.scatterplot(data = iris_df, x = 'c1', y = 'c2', hue = 'Three clusters')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\n\n\n\n\n\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nThree clusters\nTwo clusters\nFive clusters\nc1\nc2\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n1\n1\n1\n-0.630703\n0.107578\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n1\n1\n2\n-0.622905\n-0.104260\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n1\n1\n2\n-0.669520\n-0.051417\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n1\n1\n2\n-0.654153\n-0.102885\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n1\n1\n1\n-0.648788\n0.133488\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n2\n0\n0\n0.551462\n0.059841\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n0\n0\n3\n0.407146\n-0.171821\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n2\n0\n3\n0.447143\n0.037560\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n2\n0\n0\n0.488208\n0.149678\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n0\n0\n3\n0.312066\n-0.031130\n\n\n\n\n150 rows × 9 columns\n\n\n\n\n\n16.3.2 PCA to Clusters\nWe have reduced our 4D dataset to 2D whilst keeping the data variance. Reducing the data to fewer dimensions can help with the ‘curse of dimensionality’, reduce the change of overfitting a machine learning model (see here) and reduce the computational complexity of a model fit.\nPutting our new dimensions into a kMeans model\n\nk_means_pca = KMeans(n_clusters = 3, init = 'random', n_init = 10)\niris_pca_kmeans = k_means_pca.fit(iris_df.iloc[:,-2:])\n\n\ntype(iris_df.iloc[:,-2:].values)\n\nnumpy.ndarray\n\n\n\niris_df['PCA 3 clusters'] = pd.Series(k_means_pca.predict(iris_df.iloc[:,-2:].values), index = iris_df.index)\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nThree clusters\nTwo clusters\nFive clusters\nc1\nc2\nPCA 3 clusters\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n1\n1\n1\n-0.630703\n0.107578\n0\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n1\n1\n2\n-0.622905\n-0.104260\n0\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n1\n1\n2\n-0.669520\n-0.051417\n0\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n1\n1\n2\n-0.654153\n-0.102885\n0\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n1\n1\n1\n-0.648788\n0.133488\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n2\n0\n0\n0.551462\n0.059841\n1\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n0\n0\n3\n0.407146\n-0.171821\n2\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n2\n0\n3\n0.447143\n0.037560\n1\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n2\n0\n0\n0.488208\n0.149678\n1\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n0\n0\n3\n0.312066\n-0.031130\n2\n\n\n\n\n150 rows × 10 columns\n\n\n\nAs we only have two dimensions we can easily plot this on a single scatterplot.\n\n# a different seaborn theme\n# see https://python-graph-gallery.com/104-seaborn-themes/\nsns.set_style(\"darkgrid\")\nsns.scatterplot(data = iris_df, x = 'c1', y = 'c2', hue = 'PCA 3 clusters')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\n\n\n\n\nI suspect having two clusters would work better. We should try a few different models.\nCopying the code from here we can fit multiple numbers of clusters.\n\nks = range(1, 10)\ninertias = [] # Create an empty list (will be populated later)\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k, n_init = 10)\n    \n    # Fit model to samples\n    model.fit(iris_df.iloc[:,-2:])\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n    \nplt.plot(ks, inertias, '-o', color='black')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n\n\n\n\n\n\n\n\nThree seems ok. We clearly want no more than three.\nThese types of plots show an point about model complexity. More free parameters in the model (here the number of clusters) will improve how well the model captures the data, often with reducing returns. However, a model which overfits the data will not be able to fit new data well - referred to overfitting. Randomish internet blogs introduce the topic pretty well, see here, and also wikipedia, see here.\n\n\n16.3.3 Missing values\nFinally, how we deal with missing values can impact the results of PCA and kMeans clustering.\nLets us load in the iris dataset again and randomly remove 10% of the data (see code from here).\n\nimport numpy as np\n\nx = load_iris()\n\n\niris_df = pd.DataFrame(x.data, columns = x.feature_names)\n\nmask = np.random.choice([True, False], size = iris_df.shape, p = [0.2, 0.8])\nmask[mask.all(1),-1] = 0\n\ndf = iris_df.mask(mask)\n\ndf.isna().sum()\n\nsepal length (cm)    23\nsepal width (cm)     28\npetal length (cm)    21\npetal width (cm)     31\ndtype: int64\n\n\n\ndf\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\nNaN\nNaN\nNaN\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\nNaN\n0.2\n\n\n4\n5.0\n3.6\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\nNaN\nNaN\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\nNaN\nNaN\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\nNaN\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\nAbout 20% of the data is randomly an NaN.\n\n16.3.3.1 Zeroing\nWe can 0 them and fit our models.\n\ndf_1 = df.copy()\ndf_1 = df_1.fillna(0)\n\n\ndf_1\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n0.0\n0.0\n0.0\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n0.0\n0.2\n\n\n4\n5.0\n3.6\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n0.0\n0.0\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n0.0\n0.0\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n0.0\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\n\nk_means_zero = KMeans(n_clusters = 4, init = 'random', n_init = 10)\nk_means_zero.fit(df_1)\ndf_1['Four clusters'] = pd.Series(k_means_zero.predict(df_1.iloc[:,0:4].values), index = df_1.index)\nsns.pairplot(df_1, hue = 'Four clusters')\n\n\n\n\n\n\n\n\nWhat impact has zeroing the values had on our results?\nNow, onto PCA.\n\n# PCA analysis\nn_components = 2\n\npca = PCA(n_components=n_components)\ndf_1_pca = pca.fit(df_1.iloc[:,0:4])\n\n# Extract projected values\ndf_1_pca_vals = df_1_pca.transform(df_1.iloc[:,0:4])\ndf_1['c1'] = [item[0] for item in df_1_pca_vals]\ndf_1['c2'] = [item[1] for item in df_1_pca_vals]\n\nsns.scatterplot(data = df_1, x = 'c1', y = 'c2')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\n\n\n\n\n\ndf_1_pca.explained_variance_\n\narray([5.3176492 , 4.20322317])\n\n\n\ndf_1_pca.components_\n\narray([[-0.84439271, -0.02201056, -0.52209728, -0.11802933],\n       [-0.53545858,  0.00318789,  0.82384805,  0.18587184]])\n\n\n\n\n16.3.3.2 Replacing with the average\n\ndf_2 = df.copy()\nfor i in range(4):\n    df_2.iloc[:,i] = df_2.iloc[:,i].fillna(df_2.iloc[:,i].mean())\n\n\ndf_2\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.100000\n3.500000\n1.400000\n0.200000\n\n\n1\n5.806299\n3.045902\n3.735659\n0.200000\n\n\n2\n4.700000\n3.200000\n1.300000\n0.200000\n\n\n3\n4.600000\n3.100000\n3.735659\n0.200000\n\n\n4\n5.000000\n3.600000\n3.735659\n1.178992\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.700000\n3.000000\n3.735659\n1.178992\n\n\n146\n6.300000\n2.500000\n5.000000\n1.900000\n\n\n147\n6.500000\n3.045902\n3.735659\n2.000000\n\n\n148\n6.200000\n3.400000\n5.400000\n2.300000\n\n\n149\n5.806299\n3.000000\n5.100000\n1.800000\n\n\n\n\n150 rows × 4 columns\n\n\n\n\nk_means_zero = KMeans(n_clusters = 4, init = 'random', n_init = 10)\nk_means_zero.fit(df_2)\ndf_2['Four clusters'] = pd.Series(k_means_zero.predict(df_2.iloc[:,0:4].values), index = df_2.index)\nsns.pairplot(df_2, hue = 'Four clusters')\n\n\n\n\n\n\n\n\n\n# PCA analysis\nn_components = 2\n\npca = PCA(n_components=n_components)\ndf_2_pca = pca.fit(df_2.iloc[:,0:4])\n\n# Extract projected values\ndf_2_pca_vals = df_2_pca.transform(df_2.iloc[:,0:4])\ndf_2['c1'] = [item[0] for item in df_2_pca_vals]\ndf_2['c2'] = [item[1] for item in df_2_pca_vals]\n\nsns.scatterplot(data = df_2, x = 'c1', y = 'c2')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\n\n\n\n\n\ndf_2_pca.explained_variance_\n\narray([3.31519797, 0.27508431])\n\n\n\ndf_2_pca.components_\n\narray([[ 0.3347291 , -0.07564839,  0.88336309,  0.31922313],\n       [ 0.86390194,  0.33976241, -0.34674533,  0.13417384]])",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Lab: Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#useful-resources",
    "href": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#useful-resources",
    "title": "16  Lab: Dimension Reduction",
    "section": "16.4 Useful resources",
    "text": "16.4 Useful resources\nThe scikit learn UserGuide is very good. Both approaches here are often referred to as unsupervised learning methods and you can find the scikit learn section on these here.\nIf you have issues with the documentation then also look at the scikit-learn examples.\nAlso, in no particular order:\n\nThe In-Depth sections of the Python Data Science Handbook. More for machine learning but interesting all the same.\nPython for Data Analysis (ebook is available via Warwick library)\n\nIn case you are bored:\n\nStack abuse - Some fun blog entries to look at\nTowards data science - a blog that contains a mix of intro, intermediate and advanced topics. Nice to skim through to try and undrestand something new.\n\nPlease do try out some of the techniques detailed in the lecture material The simple examples found in the scikit learn documentation are rather good. Generally, I find it much easier to try to understand a method using a simple dataset.",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Lab: Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_2_Crime.html",
    "href": "content/labs/Lab_4/IM939_Lab_4_2_Crime.html",
    "title": "17  Lab: Dimension reduction (2)",
    "section": "",
    "text": "17.1 Data\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\ndf = pd.read_csv('data/censusCrimeClean.csv')\n\ndf\n\n\n\n\n\n\n\n\ncommunityname\nfold\npopulation\nhouseholdsize\nracepctblack\nracePctWhite\nracePctAsian\nracePctHisp\nagePct12t21\nagePct12t29\n...\nNumStreet\nPctForeignBorn\nPctBornSameState\nPctSameHouse85\nPctSameCity85\nPctSameState85\nLandArea\nPopDens\nPctUsePubTrans\nViolentCrimesPerPop\n\n\n\n\n0\nLakewoodcity\n1\n0.19\n0.33\n0.02\n0.90\n0.12\n0.17\n0.34\n0.47\n...\n0.00\n0.12\n0.42\n0.50\n0.51\n0.64\n0.12\n0.26\n0.20\n0.20\n\n\n1\nTukwilacity\n1\n0.00\n0.16\n0.12\n0.74\n0.45\n0.07\n0.26\n0.59\n...\n0.00\n0.21\n0.50\n0.34\n0.60\n0.52\n0.02\n0.12\n0.45\n0.67\n\n\n2\nAberdeentown\n1\n0.00\n0.42\n0.49\n0.56\n0.17\n0.04\n0.39\n0.47\n...\n0.00\n0.14\n0.49\n0.54\n0.67\n0.56\n0.01\n0.21\n0.02\n0.43\n\n\n3\nWillingborotownship\n1\n0.04\n0.77\n1.00\n0.08\n0.12\n0.10\n0.51\n0.50\n...\n0.00\n0.19\n0.30\n0.73\n0.64\n0.65\n0.02\n0.39\n0.28\n0.12\n\n\n4\nBethlehemtownship\n1\n0.01\n0.55\n0.02\n0.95\n0.09\n0.05\n0.38\n0.38\n...\n0.00\n0.11\n0.72\n0.64\n0.61\n0.53\n0.04\n0.09\n0.02\n0.03\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1989\nTempleTerracecity\n10\n0.01\n0.40\n0.10\n0.87\n0.12\n0.16\n0.43\n0.51\n...\n0.00\n0.22\n0.28\n0.34\n0.48\n0.39\n0.01\n0.28\n0.05\n0.09\n\n\n1990\nSeasidecity\n10\n0.05\n0.96\n0.46\n0.28\n0.83\n0.32\n0.69\n0.86\n...\n0.00\n0.53\n0.25\n0.17\n0.10\n0.00\n0.02\n0.37\n0.20\n0.45\n\n\n1991\nWaterburytown\n10\n0.16\n0.37\n0.25\n0.69\n0.04\n0.25\n0.35\n0.50\n...\n0.02\n0.25\n0.68\n0.61\n0.79\n0.76\n0.08\n0.32\n0.18\n0.23\n\n\n1992\nWalthamcity\n10\n0.08\n0.51\n0.06\n0.87\n0.22\n0.10\n0.58\n0.74\n...\n0.01\n0.45\n0.64\n0.54\n0.59\n0.52\n0.03\n0.38\n0.33\n0.19\n\n\n1993\nOntariocity\n10\n0.20\n0.78\n0.14\n0.46\n0.24\n0.77\n0.50\n0.62\n...\n0.08\n0.68\n0.50\n0.34\n0.35\n0.68\n0.11\n0.30\n0.05\n0.48\n\n\n\n\n1994 rows × 102 columns",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Lab: Dimension reduction (2)</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_2_Crime.html#data",
    "href": "content/labs/Lab_4/IM939_Lab_4_2_Crime.html#data",
    "title": "17  Lab: Dimension reduction (2)",
    "section": "",
    "text": "Do-this-yourself: Let’s say “hi” to the data\n\n\n\nYou have probably learnt by now that the best start to working with a new data set is to get to know it. What would be a good way to approach this?\nA few steps we always take: - Use the internal spreadsheet viewer in JupyterLab, go into the data folder and have a look at the data. - Check the size of the dataset. Do you remember how to look at the shape of a dataset? - Get a summary of the data, observe the descriptive statistics. - Check the data types and see if there are mixes of different types, such as numerical, categorical, textual, etc.. - Create a visualisation to get a first peek into the relations. Think about the number of features. Is a visualisation feasible?\n\n\n\n\n\n\n\n\nDo-this-yourself: Check if we need to do any normalisation for this case?\n\n\n\nWhat are the descriptive statistics look like, see if we need to do anything more?\nUsually a standardisation is recommended before you apply PCA on a dataset. Here is that link again – https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Lab: Dimension reduction (2)</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_2_Crime.html#principal-component-analysis",
    "href": "content/labs/Lab_4/IM939_Lab_4_2_Crime.html#principal-component-analysis",
    "title": "17  Lab: Dimension reduction (2)",
    "section": "17.2 Principal Component Analysis",
    "text": "17.2 Principal Component Analysis\nNote for the above: In case you wondered, we have done some cleaning and preprocessing on this data to make things easier for you, so normalisation might not be needed or at least won’t make a lot of difference to the results. But the real life will always be much more messier.\nOK, you probably have noticed that there is a lot of data features we have here. Let’s start experimenting with the PCA technique and see what it could surface. Maybe it will help us reduce the dimensionality a little but and tell us something about the features.\n\nfrom sklearn.decomposition import PCA\n\nn_components = 2\n \npca = PCA(n_components=n_components)\ndf_pca = pca.fit(df.iloc[:, 1:])\n\n\ndf_pca.explained_variance_ratio_\n\narray([0.67387831, 0.08863102])\n\n\n\n\n\n\n\n\nWhat do these numbers mean?\n\n\n\n\ndf_pca_vals = pca.fit_transform(df.iloc[:,1:])\ndf['c1'] = [item[0] for item in df_pca_vals]\ndf['c2'] = [item[1] for item in df_pca_vals]\n\n\nimport seaborn as sns\nsns.scatterplot(data = df, x = 'c1', y = 'c2')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\n\n\n\n\nHmm, that looks problematic. What is going on? When you see patterns like this, you will need to take a step back and look at everything closely.\nWe should look at the component loadings. Though there are going to be over 200 of them. We can put them into a Pandas dataframe and then sort them.\n\n\n\ndata = {'columns' : df.columns[1:102],\n        'component 1' : df_pca.components_[0],\n        'component 2' : df_pca.components_[1]}\n\n\nloadings = pd.DataFrame(data)\nloadings.sort_values(by=['component 1'], ascending=False, key=abs) \n\n\n\n\n\n\n\n\ncolumns\ncomponent 1\ncomponent 2\n\n\n\n\n0\nfold\n0.999788\n-0.016430\n\n\n12\npctUrban\n0.004944\n0.146079\n\n\n37\nPctOccupManu\n-0.004403\n-0.132659\n\n\n85\nRentHighQ\n0.004078\n0.188187\n\n\n13\nmedIncome\n0.003932\n0.185649\n\n\n...\n...\n...\n...\n\n\n92\nPctForeignBorn\n0.000076\n0.024639\n\n\n3\nracepctblack\n0.000066\n-0.130905\n\n\n5\nracePctAsian\n0.000064\n0.061561\n\n\n99\nPctUsePubTrans\n0.000037\n0.036078\n\n\n6\nracePctHisp\n-0.000002\n-0.064135\n\n\n\n\n101 rows × 3 columns\n\n\n\n\nloadings.sort_values(by=['component 2'], ascending=False) \n\n\n\n\n\n\n\n\ncolumns\ncomponent 1\ncomponent 2\n\n\n\n\n85\nRentHighQ\n0.004078\n0.188187\n\n\n13\nmedIncome\n0.003932\n0.185649\n\n\n46\nPctYoungKids2Par\n0.003818\n0.178675\n\n\n20\nmedFamInc\n0.003025\n0.177225\n\n\n45\nPctKids2Par\n0.002385\n0.169581\n\n\n...\n...\n...\n...\n\n\n51\nPctIlleg\n-0.001022\n-0.153504\n\n\n31\nPctNotHSGrad\n-0.002967\n-0.158646\n\n\n18\npctWPubAsst\n-0.003697\n-0.173593\n\n\n29\nPctPopUnderPov\n-0.003594\n-0.190548\n\n\n78\nPctHousNoPhone\n-0.001759\n-0.198116\n\n\n\n\n101 rows × 3 columns\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nDo you notice anything extraordinary within these variable loadings?\nRemember, the variable loadings tell us to what extent the information encoded along a principal component axis is determined by a single variable. In other words, how important a variable is. Haev a look and see if anything is too problematic here.\n\n\nThe fold variable is messing with our PCA. What does that variable look like.\n\ndf.fold.value_counts()\n\nfold\n1     200\n2     200\n3     200\n4     200\n5     199\n6     199\n7     199\n8     199\n9     199\n10    199\nName: count, dtype: int64\n\n\n\ndf['fold'].unique()\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n\n\nAha! It looks to be some sort of categorical variable. Looking into the dataset more, it is actually a variable used for cross tabling.\nWe should not include this variable in our analysis.\n\ndf.iloc[:, 2:-2]\n\n\n\n\n\n\n\n\npopulation\nhouseholdsize\nracepctblack\nracePctWhite\nracePctAsian\nracePctHisp\nagePct12t21\nagePct12t29\nagePct16t24\nagePct65up\n...\nNumStreet\nPctForeignBorn\nPctBornSameState\nPctSameHouse85\nPctSameCity85\nPctSameState85\nLandArea\nPopDens\nPctUsePubTrans\nViolentCrimesPerPop\n\n\n\n\n0\n0.19\n0.33\n0.02\n0.90\n0.12\n0.17\n0.34\n0.47\n0.29\n0.32\n...\n0.00\n0.12\n0.42\n0.50\n0.51\n0.64\n0.12\n0.26\n0.20\n0.20\n\n\n1\n0.00\n0.16\n0.12\n0.74\n0.45\n0.07\n0.26\n0.59\n0.35\n0.27\n...\n0.00\n0.21\n0.50\n0.34\n0.60\n0.52\n0.02\n0.12\n0.45\n0.67\n\n\n2\n0.00\n0.42\n0.49\n0.56\n0.17\n0.04\n0.39\n0.47\n0.28\n0.32\n...\n0.00\n0.14\n0.49\n0.54\n0.67\n0.56\n0.01\n0.21\n0.02\n0.43\n\n\n3\n0.04\n0.77\n1.00\n0.08\n0.12\n0.10\n0.51\n0.50\n0.34\n0.21\n...\n0.00\n0.19\n0.30\n0.73\n0.64\n0.65\n0.02\n0.39\n0.28\n0.12\n\n\n4\n0.01\n0.55\n0.02\n0.95\n0.09\n0.05\n0.38\n0.38\n0.23\n0.36\n...\n0.00\n0.11\n0.72\n0.64\n0.61\n0.53\n0.04\n0.09\n0.02\n0.03\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1989\n0.01\n0.40\n0.10\n0.87\n0.12\n0.16\n0.43\n0.51\n0.35\n0.30\n...\n0.00\n0.22\n0.28\n0.34\n0.48\n0.39\n0.01\n0.28\n0.05\n0.09\n\n\n1990\n0.05\n0.96\n0.46\n0.28\n0.83\n0.32\n0.69\n0.86\n0.73\n0.14\n...\n0.00\n0.53\n0.25\n0.17\n0.10\n0.00\n0.02\n0.37\n0.20\n0.45\n\n\n1991\n0.16\n0.37\n0.25\n0.69\n0.04\n0.25\n0.35\n0.50\n0.31\n0.54\n...\n0.02\n0.25\n0.68\n0.61\n0.79\n0.76\n0.08\n0.32\n0.18\n0.23\n\n\n1992\n0.08\n0.51\n0.06\n0.87\n0.22\n0.10\n0.58\n0.74\n0.63\n0.41\n...\n0.01\n0.45\n0.64\n0.54\n0.59\n0.52\n0.03\n0.38\n0.33\n0.19\n\n\n1993\n0.20\n0.78\n0.14\n0.46\n0.24\n0.77\n0.50\n0.62\n0.40\n0.17\n...\n0.08\n0.68\n0.50\n0.34\n0.35\n0.68\n0.11\n0.30\n0.05\n0.48\n\n\n\n\n1994 rows × 100 columns\n\n\n\n\nfrom sklearn.decomposition import PCA\n\nn_components = 2\n\npca_no_fold = PCA(n_components=n_components)\ndf_pca_no_fold = pca_no_fold.fit(df.iloc[:, 2:-2])\n\ndf_pca_vals = pca_no_fold.fit_transform(df.iloc[:, 2:-2])\n\ndf['c1_no_fold'] = [item[0] for item in df_pca_vals]\ndf['c2_no_fold'] = [item[1] for item in df_pca_vals]\n\nsns.scatterplot(data = df, x = 'c1_no_fold', y = 'c2_no_fold')\n\n&lt;Axes: xlabel='c1_no_fold', ylabel='c2_no_fold'&gt;\n\n\n\n\n\n\n\n\n\nHow do our component loadings look?\n\ndata = {'columns' : df.iloc[:, 2:-4].columns,\n        'component 1' : df_pca_no_fold.components_[0],\n        'component 2' : df_pca_no_fold.components_[1]}\n\n\nloadings = pd.DataFrame(data)\nloadings_sorted = loadings.sort_values(by=['component 1'], ascending=False)\nloadings_sorted.iloc[1:10,:]\n\n\n\n\n\n\n\n\ncolumns\ncomponent 1\ncomponent 2\n\n\n\n\n12\nmedIncome\n0.185822\n0.040774\n\n\n45\nPctYoungKids2Par\n0.178807\n-0.017237\n\n\n19\nmedFamInc\n0.177283\n0.031895\n\n\n44\nPctKids2Par\n0.169533\n-0.042621\n\n\n43\nPctFam2Par\n0.163277\n-0.031094\n\n\n82\nRentLowQ\n0.163120\n0.119274\n\n\n85\nMedRent\n0.163120\n0.106135\n\n\n80\nOwnOccMedVal\n0.159850\n0.135830\n\n\n83\nRentMedian\n0.159605\n0.113360\n\n\n\n\n\n\n\n\nloadings_sorted = loadings.sort_values(by=['component 2'], ascending=False)\nloadings_sorted.iloc[1:10,:]\n\n\n\n\n\n\n\n\ncolumns\ncomponent 1\ncomponent 2\n\n\n\n\n59\nPctRecImmig10\n-0.000556\n0.253476\n\n\n57\nPctRecImmig5\n-0.000901\n0.252463\n\n\n56\nPctRecentImmig\n0.001797\n0.248239\n\n\n91\nPctForeignBorn\n0.024743\n0.241783\n\n\n61\nPctNotSpeakEnglWell\n-0.048595\n0.207575\n\n\n5\nracePctHisp\n-0.063868\n0.187826\n\n\n68\nPctPersDenseHous\n-0.087193\n0.184704\n\n\n11\npctUrban\n0.146557\n0.183317\n\n\n4\nracePctAsian\n0.061547\n0.160710\n\n\n\n\n\n\n\nInteresting that our first component variables are income related whereas our second component variables are immigration related. If we look at the projections of the model, coloured by Crime, what do we see?\n\nsns.scatterplot(data = df,\n                x = 'c1_no_fold',\n                y = 'c2_no_fold',\n                hue = 'ViolentCrimesPerPop',\n                size = 'ViolentCrimesPerPop')\n\n&lt;Axes: xlabel='c1_no_fold', ylabel='c2_no_fold'&gt;",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Lab: Dimension reduction (2)</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_2_Crime.html#clustering",
    "href": "content/labs/Lab_4/IM939_Lab_4_2_Crime.html#clustering",
    "title": "17  Lab: Dimension reduction (2)",
    "section": "17.3 Clustering",
    "text": "17.3 Clustering\nWhat about clustering using these income and immigration components?\n\n17.3.1 Defining number of clusters\n\nfrom sklearn.cluster import KMeans\n\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k, n_init = 10)\n    \n    # Fit model to samples\n    model.fit(df[['c1_no_fold', 'c2_no_fold']])\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n\nimport matplotlib.pyplot as plt\n\nplt.plot(ks, inertias, '-o', color='black')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n\n\n\n\n\n\n\n\nFour clusters looks good.",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Lab: Dimension reduction (2)</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_2_Crime.html#visualising-clusters",
    "href": "content/labs/Lab_4/IM939_Lab_4_2_Crime.html#visualising-clusters",
    "title": "17  Lab: Dimension reduction (2)",
    "section": "17.4 Visualising clusters",
    "text": "17.4 Visualising clusters\n\nk_means_4 = KMeans(n_clusters = 3, init = 'random', n_init = 10)\nk_means_4.fit(df[['c1_no_fold', 'c2_no_fold']])\ndf['Four clusters'] = pd.Series(k_means_4.predict(df[['c1_no_fold', 'c2_no_fold']].values), index = df.index)\n\n\nsns.scatterplot(data = df, x = 'c1_no_fold', y = 'c2_no_fold', hue = 'Four clusters')\n\n&lt;Axes: xlabel='c1_no_fold', ylabel='c2_no_fold'&gt;\n\n\n\n\n\n\n\n\n\nHmm, might we interpret this plot? The plot is unclear. Let us assume c1 is an income component and c2 is an immigration component. Cluster 0 are places high on income and low on immigration. Cluster 2 are low on income and low on immigration. The interesting group are those high on immigration and relatively low on income.\nWe can include crime on the plot.\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = (15,10)\nsns.scatterplot(data = df, x = 'c1_no_fold', y = 'c2_no_fold', hue = 'Four clusters', size = 'ViolentCrimesPerPop')\n\n&lt;Axes: xlabel='c1_no_fold', ylabel='c2_no_fold'&gt;\n\n\n\n\n\n\n\n\n\nAlas, we stop here in this exercise. You could work on this further. There are outliers you might want to remove. Or, at this stage, you might want to look at more components, 3 or 4 and look for some other factors.\n\n\n\n\nRedmond, Michael. 2009. “Communities and Crime.” UCI Machine Learning Repository. https://doi.org/10.24432/C53W3X.",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Lab: Dimension reduction (2)</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_Exercises.html",
    "href": "content/labs/Lab_4/IM939_Lab_4_Exercises.html",
    "title": "18  Exercise: Wine dataset",
    "section": "",
    "text": "18.1 Load data and import libraries\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport seaborn as sn?\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PC?\nfrom sklearn.decomposition import S????ePCA\nfrom sklearn.manifold import TSNE\n\ndf = pd.read_excel('data/winequality-red_v2.xlsx')\n\n\n  Cell In[1], line 9\n    from sklearn.decomposition import S????ePCA\n                                       ^\nSyntaxError: invalid syntax\ndf.h??d()\n\n\n  Cell In[2], line 1\n    df.h??d()\n        ^\nSyntaxError: invalid syntax\n# May take a while depending on your computer\n# feel free not to run this\nsns.pair????(df)\n\n\n  Cell In[3], line 3\n    sns.pair????(df)\n            ^\nSyntaxError: invalid syntax",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exercise: Wine dataset</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_Exercises.html#sparcepca",
    "href": "content/labs/Lab_4/IM939_Lab_4_Exercises.html#sparcepca",
    "title": "18  Exercise: Wine dataset",
    "section": "20.1 SparcePCA",
    "text": "20.1 SparcePCA\n\ns_pca = SparsePCA(n_components=n_components)\ndf_s_pca = s_pca.fit(df.????[:, 0:11])\n\n\n  Cell In[9], line 2\n    df_s_pca = s_pca.fit(df.????[:, 0:11])\n                            ^\nSyntaxError: invalid syntax\n\n\n\n\n\ndf_s_pca_vals = s_pca.fit_?????????(df.iloc[:, 0:11])\ndf['c1 spca'] = [item[0] for item in df_s_pca_vals]\ndf['c2 spca'] = [item[1] for item in df_s_pca_vals]\n\n\n  Cell In[10], line 1\n    df_s_pca_vals = s_pca.fit_?????????(df.iloc[:, 0:11])\n                              ^\nSyntaxError: invalid syntax\n\n\n\n\n\nsns.scatterplot(data = df, x = 'c1 spca', y = 'c2 spca', hue = 'quality')\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 sns.scatterplot(data = df, x = 'c1 spca', y = 'c2 spca', hue = 'quality')\n\nNameError: name 'sns' is not defined",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exercise: Wine dataset</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_Exercises.html#tsne",
    "href": "content/labs/Lab_4/IM939_Lab_4_Exercises.html#tsne",
    "title": "18  Exercise: Wine dataset",
    "section": "20.2 tSNE",
    "text": "20.2 tSNE\n\ntsne_model = TSNE(n_components=n_components)\ndf_tsne = tsne_model.fit(df.iloc[:, 0:11])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 tsne_model = TSNE(n_components=n_components)\n      2 df_tsne = tsne_model.fit(df.iloc[:, 0:11])\n\nNameError: name 'TSNE' is not defined\n\n\n\n\ndf_tsne_vals = tsne_model.fit_transform(df.iloc[:, 0:11])\ndf['c1 tsne'] = [item[0] for item in ??_tsne_vals]\ndf['c2 tsne'] = [item[1] for item in df_tsne_vals]\n\n\n  Cell In[13], line 2\n    df['c1 tsne'] = [item[0] for item in ??_tsne_vals]\n                                         ^\nSyntaxError: invalid syntax\n\n\n\n\n\n# This plot does not look right\n# I am not sure why.\nsns.scatterplot(data = ??, x = 'c1 tsne', y = 'c1 tsne', hue = 'quality')\n\n\n  Cell In[14], line 3\n    sns.scatterplot(data = ??, x = 'c1 tsne', y = 'c1 tsne', hue = 'quality')\n                           ^\nSyntaxError: invalid syntax\n\n\n\n\nThat looks concerning - there is a straight line. It looks like something in the above code might not be correct.\nCan you find out what that might be?\nHint: think about when you would get a straight line in a scatterplot?\nOnce you fixed the error above, you will notice a different structure to the ones you observed in the PCA runs. There isn’t really a clear next step which of these projections one should adopt.\nFor now, we will use PCA components. PCA would be a good choice if the interpretability of the components is important to us. Since PCA is a linear projection method, the components carry the weights of each raw feature which enable us to make inferences about the axes. However, if we are more interested in finding structures and identify groups of similar items, t-SNE might be a better projection to use since it emphasises proximity but the axes don’t mean much since the layout is formed stochastically (fancy speak for saying that there is randomness in the algorithm and the layout will be different each time your run it).\n\ndata = {'columns' : df.iloc[:, 0:11].columns,\n        'component 1' : df_pca.components_[0],\n        'component 2' : df_pca.components_[1]}\n\n\nloadings = pd.?????????(data)\nloadings_sorted = loadings.sort_values(by=['component 1'], ascending=False)\nloadings_sorted.iloc[1:10,:]\n\n\n  Cell In[15], line 6\n    loadings = pd.?????????(data)\n                  ^\nSyntaxError: invalid syntax\n\n\n\n\n\nloadings_sorted = loadings.sort_values(by=['component 2'], ascending=False)\nloadings_sorted.iloc[1:10,:]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 1\n----&gt; 1 loadings_sorted = loadings.sort_values(by=['component 2'], ascending=False)\n      2 loadings_sorted.iloc[1:10,:]\n\nNameError: name 'loadings' is not defined",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exercise: Wine dataset</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_Exercises.html#clustering",
    "href": "content/labs/Lab_4/IM939_Lab_4_Exercises.html#clustering",
    "title": "18  Exercise: Wine dataset",
    "section": "20.3 Clustering",
    "text": "20.3 Clustering\n\nfrom sklearn.cluster import KMeans\n\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    ????? = KMeans(n_clusters=k)\n    \n    # Fit model to samples\n    model.fit(df[['c1', 'c2']])\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n\nimport matplotlib.pyplot as plt\n\nplt.plot(ks, inertias, '-o', color='black')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n\nObject `??? = KMeans(n_clusters=k)` not found.\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 10\n      7 get_ipython().run_line_magic('pinfo2', '??? = KMeans(n_clusters=k)')\n      9 # Fit model to samples\n---&gt; 10 model.fit(df[['c1', 'c2']])\n     12 # Append the inertia to the list of inertias\n     13 inertias.append(model.inertia_)\n\nNameError: name 'model' is not defined\n\n\n\n\nk_means_3 = KMeans(n_clusters = 3, init = 'random')\nk_means_3.fit(df[['c1', 'c2']])\ndf['Three clusters'] = pd.Series(k_means_3.???????(df[['c1', 'c2']].values), index = df.index)\n\n\n  Cell In[18], line 3\n    df['Three clusters'] = pd.Series(k_means_3.???????(df[['c1', 'c2']].values), index = df.index)\n                                               ^\nSyntaxError: invalid syntax\n\n\n\n\n\nsns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Three clusters')\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 sns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Three clusters')\n\nNameError: name 'sns' is not defined\n\n\n\nConsider:\n\nIs that useful?\nWhat might it mean?\n\nOutside of this session go back to normalising the data and try out different methods for normalisation as well (e.g., centering around the mean), clustering the raw data (and not the projections from PCA), trying to get tSNE working or using different numbers of components.",
    "crumbs": [
      "Structures & Spaces",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exercise: Wine dataset</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-05.html",
    "href": "content/sessions/session-05.html",
    "title": "19  Introduction",
    "section": "",
    "text": "19.1 Reading lists & Resources",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-05.html#reading-lists-resources",
    "href": "content/sessions/session-05.html#reading-lists-resources",
    "title": "19  Introduction",
    "section": "",
    "text": "19.1.1 Required reading\n\nBreiman, L., 2001. Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), pp.199-231. [pdf - article + comments/responses]\n\nThis is a delightful read from a legend, the article is on the first 18 pages followed by comment letters. The relevant parts for this week’s discussion are on sections 5 and section-8 (remember the discussion on Rashomon effect?), however, the opening parts are also mind opening.\n\nPage, S., 2018. Why “Many-Model Thinkers” Make Better Decisions. [online] Harvard Business Review. Available at: https://hbr.org/2018/11/why-many-model-thinkers-make-better-decisions [Accessed 30 October 2020]. [pdf]\n\nScott Page also has a full book on this which I can only recommend [link]\n\nHeale, R. and Twycross, A., 2015. Validity and reliability in quantitative studies. Evidence-based nursing, 18(3), pp.66-67. [pdf]\nJames, G., Witten, D., Hastie, T. and Tibshirani, R., 2013. An introduction to statistical learning (Vol. 112, p. 18). New York: Springer. [link to the book] [the book page for other materials such as code]\nThis is an amazing book overall and it’s freely available. For this week, you can check Chapter-5 on “Resampling Methods”. Focus on Section 5.1 and also look into 5.2 on Bootstrapping.\n\n\n\n19.1.2 Optional reading\n\nBarbour, R.S., 2001. Checklists for improving rigour in qualitative research: a case of the tail wagging the dog?. Bmj, 322(7294), pp.1115-1117. [pdf]\nRule, A., Birmingham, A., Zuniga, C., Altintas, I., Huang, S.C., Knight, R., Moshiri, N., Nguyen, M.H., Rosenthal, S.B., Pérez, F. and Rose, P.W., 2019. Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks. [pdf]",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_5/IM939_Lab_5_1.html",
    "href": "content/labs/Lab_5/IM939_Lab_5_1.html",
    "title": "21  Lab: Working with multiple models",
    "section": "",
    "text": "21.1 Data exploration and wrangling\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\n\ndf = pd.read_excel('data/london-borough-profilesV3.xlsx', engine = 'openpyxl')\ndf.columns\n\nIndex(['Code', 'Area/INDICATOR', 'Inner/ Outer London',\n       'GLA Population Estimate 2013', 'GLA Household Estimate 2013',\n       'Inland Area (Hectares)', 'Population density (per hectare) 2013',\n       'Average Age, 2013', 'Proportion of population aged 0-15, 2013',\n       'Proportion of population of working-age, 2013',\n       'Proportion of population aged 65 and over, 2013',\n       '% of resident population born abroad (2013)',\n       'Largest migrant population by country of birth (2013)',\n       '% of largest migrant population (2013)',\n       'Second largest migrant population by country of birth (2013)',\n       '% of second largest migrant population (2013)',\n       'Third largest migrant population by country of birth (2013)',\n       '% of third largest migrant population (2013)',\n       '% of population from BAME groups (2013)',\n       '% people aged 3+ whose main language is not English (2011 census)',\n       'Overseas nationals entering the UK (NINo), (2013/14)',\n       'New migrant (NINo) rates, (2013/14)', 'Employment rate (%) (2013/14)',\n       'Male employment rate (2013/14)', 'Female employment rate (2013/14)',\n       'Unemployment rate (2013/14)', 'Youth Unemployment rate (2013/14)',\n       'Proportion of 16-18 year olds who are NEET (%) (2013)',\n       'Proportion of the working-age population who claim benefits (%) (Feb-2014)',\n       '% working-age with a disability (2012)',\n       'Proportion of working age people with no qualifications (%) 2013',\n       'Proportion of working age people in London with degree or equivalent and above (%) 2013',\n       'Gross Annual Pay, (2013)', 'Gross Annual Pay - Male (2013)',\n       'Gross Annual Pay - Female (2013)',\n       '% adults that volunteered in past 12 months (2010/11 to 2012/13)',\n       'Number of jobs by workplace (2012)',\n       '% of employment that is in public sector (2012)', 'Jobs Density, 2012',\n       'Number of active businesses, 2012',\n       'Two-year business survival rates 2012',\n       'Crime rates per thousand population 2013/14',\n       'Fires per thousand population (2013)',\n       'Ambulance incidents per hundred population (2013)',\n       'Median House Price, 2013',\n       'Average Band D Council Tax charge (£), 2014/15',\n       'New Homes (net) 2012/13', 'Homes Owned outright, (2013) %',\n       'Being bought with mortgage or loan, (2013) %',\n       'Rented from Local Authority or Housing Association, (2013) %',\n       'Rented from Private landlord, (2013) %',\n       '% of area that is Greenspace, 2005', 'Total carbon emissions (2012)',\n       'Household Waste Recycling Rate, 2012/13',\n       'Number of cars, (2011 Census)',\n       'Number of cars per household, (2011 Census)',\n       '% of adults who cycle at least once per month, 2011/12',\n       'Average Public Transport Accessibility score, 2012',\n       'Indices of Multiple Deprivation 2010 Rank of Average Score',\n       'Income Support claimant rate (Feb-14)',\n       '% children living in out-of-work families (2013)',\n       'Achievement of 5 or more A*- C grades at GCSE or equivalent including English and Maths, 2012/13',\n       'Rates of Children Looked After (2013)',\n       '% of pupils whose first language is not English (2014)',\n       'Male life expectancy, (2010-12)', 'Female life expectancy, (2010-12)',\n       'Teenage conception rate (2012)',\n       'Life satisfaction score 2012-13 (out of 10)',\n       'Worthwhileness score 2012-13 (out of 10)',\n       'Happiness score 2012-13 (out of 10)',\n       'Anxiety score 2012-13 (out of 10)', 'Political control in council',\n       'Proportion of seats won by Conservatives in 2014 election',\n       'Proportion of seats won by Labour in 2014 election',\n       'Proportion of seats won by Lib Dems in 2014 election',\n       'Turnout at 2014 local elections'],\n      dtype='object')\ndf\n\n\n\n\n\n\n\n\nCode\nArea/INDICATOR\nInner/ Outer London\nGLA Population Estimate 2013\nGLA Household Estimate 2013\nInland Area (Hectares)\nPopulation density (per hectare) 2013\nAverage Age, 2013\nProportion of population aged 0-15, 2013\nProportion of population of working-age, 2013\n...\nTeenage conception rate (2012)\nLife satisfaction score 2012-13 (out of 10)\nWorthwhileness score 2012-13 (out of 10)\nHappiness score 2012-13 (out of 10)\nAnxiety score 2012-13 (out of 10)\nPolitical control in council\nProportion of seats won by Conservatives in 2014 election\nProportion of seats won by Labour in 2014 election\nProportion of seats won by Lib Dems in 2014 election\nTurnout at 2014 local elections\n\n\n\n\n0\nE09000001\nCity of London\nInner London\n8000\n4514.371383\n290.4\n27.525868\n41.303887\n7.948036\n77.541617\n...\n.\n8.10\n8.23\n7.44\nx\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nE09000002\nBarking and Dagenham\nOuter London\n195600\n73261.408580\n3610.8\n54.160527\n33.228935\n26.072939\n63.835021\n...\n35.4\n7.06\n7.57\n6.97\n3.3\nLab\n0.000000\n100.000000\n0.000000\n38.16\n\n\n2\nE09000003\nBarnet\nOuter London\n370000\n141385.794900\n8674.8\n42.651374\n36.896246\n20.886408\n65.505593\n...\n14.7\n7.35\n7.79\n7.27\n2.63\nCons\n50.793651\n42.857143\n1.587302\n41.1\n\n\n3\nE09000004\nBexley\nOuter London\n236500\n94701.226400\n6058.1\n39.044243\n38.883039\n20.282830\n63.146450\n...\n25.8\n7.47\n7.75\n7.21\n3.22\nCons\n71.428571\n23.809524\n0.000000\nnot avail\n\n\n4\nE09000005\nBrent\nOuter London\n320200\n114318.553900\n4323.3\n74.063670\n35.262694\n20.462585\n68.714872\n...\n19.6\n7.23\n7.32\n7.09\n3.33\nLab\n9.523810\n88.888889\n1.587302\n33\n\n\n5\nE09000006\nBromley\nOuter London\n317400\n134012.675100\n15013.5\n21.137655\n39.844502\n19.648001\n62.927051\n...\n24.2\n7.63\n7.80\n7.36\n3.2\nCons\n85.000000\n11.666667\n0.000000\nnot avail\n\n\n6\nE09000007\nCamden\nInner London\n228400\n100841.916500\n2178.9\n104.820985\n35.842413\n15.632617\n73.313473\n...\n18.1\n7.22\n7.37\n7.13\n3.25\nLab\n22.222222\n74.074074\n1.851852\n38.69\n\n\n7\nE09000008\nCroydon\nOuter London\n373100\n150053.929000\n8650.4\n43.129707\n36.570761\n21.641888\n65.638589\n...\n28.6\n7.00\n7.46\n7.11\n3.02\nLab\n42.857143\n57.142857\n0.000000\n38\n\n\n8\nE09000009\nEaling\nOuter London\n344900\n126860.977600\n5554.4\n62.089808\n35.637099\n20.642035\n68.216689\n...\n22.4\n7.24\n7.48\n7.44\n3.58\nLab\n17.391304\n76.811594\n5.797101\n41.3\n\n\n9\nE09000010\nEnfield\nOuter London\n322400\n124601.954700\n8083.2\n39.888486\n36.062871\n22.182556\n65.114934\n...\n26.4\n7.18\n7.57\n7.41\n2.51\nLab\n34.920635\n65.079365\n0.000000\n37.79\n\n\n10\nE09000011\nGreenwich\nOuter London\n262800\n104999.593900\n4733.4\n55.516088\n34.737054\n21.754144\n67.650982\n...\n34.7\n7.16\n7.49\n7.05\n3.68\nLab\n15.686275\n84.313725\n0.000000\n37.25\n\n\n11\nE09000012\nHackney\nInner London\n256600\n106655.687500\n1904.9\n134.717347\n32.624060\n20.513252\n72.376652\n...\n28.8\n7.07\n7.42\n7.02\n3.61\nLab\n7.017544\n87.719298\n5.263158\n42.89\n\n\n12\nE09000013\nHammersmith and Fulham\nInner London\n181000\n79688.691790\n1639.7\n110.384364\n34.933408\n16.764645\n73.768103\n...\n25.6\n7.23\n7.60\n6.94\n3.15\nLab\n43.478261\n56.521739\n0.000000\n38\n\n\n13\nE09000014\nHaringey\nInner London\n263100\n105459.284200\n2959.8\n88.883425\n34.384912\n19.877305\n71.199466\n...\n33.1\n7.20\n7.44\n7.13\n3.07\nLab\n0.000000\n84.210526\n15.789474\n38.1\n\n\n14\nE09000015\nHarrow\nOuter London\n245900\n86968.599850\n5046.3\n48.722544\n37.695216\n20.110244\n65.413384\n...\n14.2\n7.34\n7.53\n7.35\n3.17\nLab\n41.269841\n53.968254\n1.587302\n41\n\n\n15\nE09000016\nHavering\nOuter London\n242600\n99202.437800\n11235.0\n21.589910\n40.296140\n18.781558\n62.788014\n...\n26.4\n7.40\n7.65\n7.24\n3.17\nNo Overall Control\n40.740741\n1.851852\n0.000000\n43\n\n\n16\nE09000017\nHillingdon\nOuter London\n287500\n104650.805600\n11570.1\n24.851919\n36.171629\n20.911751\n66.121424\n...\n27.7\n7.35\n7.63\n7.34\n3.34\nCons\n64.615385\n35.384615\n0.000000\n35.76\n\n\n17\nE09000018\nHounslow\nOuter London\n264300\n98840.744350\n5597.8\n47.209459\n35.263763\n20.616902\n68.542073\n...\n30.4\n7.30\n7.60\n7.29\n3.51\nLab\n18.333333\n81.666667\n0.000000\n36.8\n\n\n18\nE09000019\nIslington\nInner London\n215900\n97616.282240\n1485.7\n145.324910\n34.419467\n15.863559\n75.457376\n...\n30.1\n7.08\n7.22\n6.85\n3.74\nLab\n0.000000\n97.916667\n0.000000\n38.4\n\n\n19\nE09000020\nKensington and Chelsea\nInner London\n155700\n77210.897790\n1212.4\n128.427401\n38.308554\n15.908516\n70.878237\n...\n17.7\n7.68\n7.92\n7.51\n3.06\nCons\n74.000000\n24.000000\n2.000000\nnot avail\n\n\n20\nE09000021\nKingston upon Thames\nOuter London\n166400\n65782.630740\n3726.1\n44.660475\n36.885957\n18.994951\n67.980436\n...\n20\n7.29\n7.45\n7.18\n3.23\nCons\n58.333333\n4.166667\n37.500000\nnot avail\n\n\n21\nE09000022\nLambeth\nInner London\n313800\n134512.450200\n2681.0\n117.055632\n33.862641\n17.899970\n74.455090\n...\n33.2\n7.09\n7.40\n6.97\n3.69\nLab\n4.761905\n93.650794\n0.000000\n32\n\n\n22\nE09000023\nLewisham\nInner London\n286000\n120439.358900\n3514.9\n81.366697\n34.540168\n20.620499\n69.996836\n...\n42\n7.23\n7.71\n7.13\n3.35\nLab\n0.000000\n98.148148\n0.000000\n37.2\n\n\n23\nE09000024\nMerton\nOuter London\n205400\n81047.637240\n3762.5\n54.603478\n36.262610\n20.203379\n67.896608\n...\n25.5\n7.18\n7.54\n7.13\n3.59\nLab\n33.333333\n60.000000\n1.666667\n41\n\n\n24\nE09000025\nNewham\nInner London\n323400\n107793.332200\n3619.8\n89.338992\n31.423439\n22.508503\n70.809763\n...\n24.1\n7.22\n7.51\n7.32\n3.36\nLab\n0.000000\n100.000000\n0.000000\n40.62\n\n\n25\nE09000026\nRedbridge\nOuter London\n289900\n103053.713300\n5641.9\n51.374341\n35.531757\n22.684198\n65.238253\n...\n16.2\n7.28\n7.44\n7.34\n3.12\nLab\n39.682540\n55.555556\n4.761905\n39.7\n\n\n26\nE09000027\nRichmond upon Thames\nOuter London\n191300\n81353.161690\n5740.7\n33.331460\n38.290205\n20.202975\n65.548776\n...\n19.9\n7.42\n7.69\n7.33\n3.56\nCons\n72.222222\n0.000000\n27.777778\n46.3\n\n\n27\nE09000028\nSouthwark\nInner London\n298400\n124613.892000\n2886.2\n103.403127\n33.754401\n18.507644\n73.722633\n...\n31.8\n7.27\n7.68\n7.20\n3.28\nLab\n3.174603\n76.190476\n20.634921\nnot avail\n\n\n28\nE09000029\nSutton\nOuter London\n196400\n80748.477060\n4384.7\n44.802887\n38.310437\n20.151661\n64.951011\n...\n25.8\n7.25\n7.57\n7.13\n3.34\nLib Dem\n16.666667\n0.000000\n83.333333\n42.2\n\n\n29\nE09000030\nTower Hamlets\nInner London\n271100\n109280.540100\n1978.1\n137.067542\n31.064282\n19.602621\n74.531880\n...\n24.3\n7.28\n7.56\n7.32\n2.93\nTower Hamlets First\n15.555556\n44.444444\n0.000000\nnot avail\n\n\n30\nE09000031\nWaltham Forest\nOuter London\n267700\n100524.458900\n3880.8\n68.970286\n34.502988\n21.784829\n68.105760\n...\n29.9\n7.24\n7.72\n7.26\n2.99\nLab\n26.666667\n73.333333\n0.000000\nnot avail\n\n\n31\nE09000032\nWandsworth\nInner London\n311800\n131562.012900\n3426.4\n91.009107\n34.414955\n17.342442\n73.693151\n...\n25.5\n7.23\n7.55\n7.28\n3.55\nCons\n68.333333\n31.666667\n0.000000\nnot avail\n\n\n32\nE09000033\nWestminster\nInner London\n226600\n108550.111900\n2148.7\n105.457732\n36.729055\n14.781940\n73.924420\n...\n21.2\n7.09\n7.44\n7.09\n3.58\nCons\n73.333333\n26.666667\n0.000000\nnot avail\n\n\n\n\n33 rows × 76 columns\nLots of different features. We also have really odd NaN values such as x and not avail. We can try and get rid of this.\ndef isnumber(x):\n    try:\n        float(x)\n        return True\n    except:\n        if (len(x) &gt; 1) & (\"not avail\" not in x):\n            return True\n        else:\n            return False\n\n# apply isnumber function to every element\ndf = df[df.applymap(isnumber)]\ndf.head()\n\n\n\n\n\n\n\n\nCode\nArea/INDICATOR\nInner/ Outer London\nGLA Population Estimate 2013\nGLA Household Estimate 2013\nInland Area (Hectares)\nPopulation density (per hectare) 2013\nAverage Age, 2013\nProportion of population aged 0-15, 2013\nProportion of population of working-age, 2013\n...\nTeenage conception rate (2012)\nLife satisfaction score 2012-13 (out of 10)\nWorthwhileness score 2012-13 (out of 10)\nHappiness score 2012-13 (out of 10)\nAnxiety score 2012-13 (out of 10)\nPolitical control in council\nProportion of seats won by Conservatives in 2014 election\nProportion of seats won by Labour in 2014 election\nProportion of seats won by Lib Dems in 2014 election\nTurnout at 2014 local elections\n\n\n\n\n0\nE09000001\nCity of London\nInner London\n8000\n4514.371383\n290.4\n27.525868\n41.303887\n7.948036\n77.541617\n...\nNaN\n8.10\n8.23\n7.44\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nE09000002\nBarking and Dagenham\nOuter London\n195600\n73261.408580\n3610.8\n54.160527\n33.228935\n26.072939\n63.835021\n...\n35.4\n7.06\n7.57\n6.97\n3.3\nLab\n0.000000\n100.000000\n0.000000\n38.16\n\n\n2\nE09000003\nBarnet\nOuter London\n370000\n141385.794900\n8674.8\n42.651374\n36.896246\n20.886408\n65.505593\n...\n14.7\n7.35\n7.79\n7.27\n2.63\nCons\n50.793651\n42.857143\n1.587302\n41.1\n\n\n3\nE09000004\nBexley\nOuter London\n236500\n94701.226400\n6058.1\n39.044243\n38.883039\n20.282830\n63.146450\n...\n25.8\n7.47\n7.75\n7.21\n3.22\nCons\n71.428571\n23.809524\n0.000000\nNaN\n\n\n4\nE09000005\nBrent\nOuter London\n320200\n114318.553900\n4323.3\n74.063670\n35.262694\n20.462585\n68.714872\n...\n19.6\n7.23\n7.32\n7.09\n3.33\nLab\n9.523810\n88.888889\n1.587302\n33\n\n\n\n\n5 rows × 76 columns\nThat looks much cleaner. The missing values are all NaN now. This will help us fill them in and/or address them in some way.\n# get only numeric columns\nnumericColumns = df._get_numeric_data()\nnumericColumns.head()\n\n\n\n\n\n\n\n\nGLA Population Estimate 2013\nGLA Household Estimate 2013\nInland Area (Hectares)\nPopulation density (per hectare) 2013\nAverage Age, 2013\nProportion of population aged 0-15, 2013\nProportion of population of working-age, 2013\nProportion of population aged 65 and over, 2013\n% of population from BAME groups (2013)\n% people aged 3+ whose main language is not English (2011 census)\n...\nAverage Public Transport Accessibility score, 2012\nIndices of Multiple Deprivation 2010 Rank of Average Score\nIncome Support claimant rate (Feb-14)\nRates of Children Looked After (2013)\nLife satisfaction score 2012-13 (out of 10)\nWorthwhileness score 2012-13 (out of 10)\nHappiness score 2012-13 (out of 10)\nProportion of seats won by Conservatives in 2014 election\nProportion of seats won by Labour in 2014 election\nProportion of seats won by Lib Dems in 2014 election\n\n\n\n\n0\n8000\n4514.371383\n290.4\n27.525868\n41.303887\n7.948036\n77.541617\n14.510348\n22.557238\n17.138103\n...\n7.631205\n262\n0.527983\n98\n8.10\n8.23\n7.44\nNaN\nNaN\nNaN\n\n\n1\n195600\n73261.408580\n3610.8\n54.160527\n33.228935\n26.072939\n63.835021\n10.092040\n45.712357\n18.724201\n...\n2.994817\n22\n4.041773\n76\n7.06\n7.57\n6.97\n0.000000\n100.000000\n0.000000\n\n\n2\n370000\n141385.794900\n8674.8\n42.651374\n36.896246\n20.886408\n65.505593\n13.607999\n37.148811\n23.405037\n...\n2.994527\n176\n1.736905\n37\n7.35\n7.79\n7.27\n50.793651\n42.857143\n1.587302\n\n\n3\n236500\n94701.226400\n6058.1\n39.044243\n38.883039\n20.282830\n63.146450\n16.570720\n19.620095\n6.031289\n...\n2.513007\n174\n2.236355\n47\n7.47\n7.75\n7.21\n71.428571\n23.809524\n0.000000\n\n\n4\n320200\n114318.553900\n4323.3\n74.063670\n35.262694\n20.462585\n68.714872\n10.822543\n64.948141\n37.151120\n...\n3.702753\n35\n2.256102\n49\n7.23\n7.32\n7.09\n9.523810\n88.888889\n1.587302\n\n\n\n\n5 rows × 41 columns\n# the above piece of code is throwing a lot of features out for not being a numeric column. The resulting frame has 41 features, however, we would expect more.\n# upon a bit of debugging, we found out that --  df._get_numeric_data() -- a Pandas function -- has a minor bug in inferring which columns are numeric when the first value of a column is a missing value, i.e., 'NaN' value. \n# This meant that some columns that are numeric were removed from the dataset even though they are numeric. \n\n\n# this next piece of code is addressing that now and also fills in the missing values with the mean() value of a column\n\nfor column_name, column in df.items():\n    try:\n        df[column_name] = df[column_name].fillna(df[column_name].mean())\n    except:\n        print(\"Column:\", column_name, \" is not a numeric column\")\n\nColumn: Code  is not a numeric column\nColumn: Area/INDICATOR  is not a numeric column\nColumn: Inner/ Outer London  is not a numeric column\nColumn: Largest migrant population by country of birth (2013)  is not a numeric column\nColumn: Second largest migrant population by country of birth (2013)  is not a numeric column\nColumn: Third largest migrant population by country of birth (2013)  is not a numeric column\nColumn: Political control in council  is not a numeric column\n# get only numeric columns\nnumericColumns = df._get_numeric_data()\nnumericColumns.head()\n\n\n\n\n\n\n\n\nGLA Population Estimate 2013\nGLA Household Estimate 2013\nInland Area (Hectares)\nPopulation density (per hectare) 2013\nAverage Age, 2013\nProportion of population aged 0-15, 2013\nProportion of population of working-age, 2013\nProportion of population aged 65 and over, 2013\n% of resident population born abroad (2013)\n% of largest migrant population (2013)\n...\nFemale life expectancy, (2010-12)\nTeenage conception rate (2012)\nLife satisfaction score 2012-13 (out of 10)\nWorthwhileness score 2012-13 (out of 10)\nHappiness score 2012-13 (out of 10)\nAnxiety score 2012-13 (out of 10)\nProportion of seats won by Conservatives in 2014 election\nProportion of seats won by Labour in 2014 election\nProportion of seats won by Lib Dems in 2014 election\nTurnout at 2014 local elections\n\n\n\n\n0\n8000\n4514.371383\n290.4\n27.525868\n41.303887\n7.948036\n77.541617\n14.510348\n35.883871\n5.194357\n...\n83.809375\n25.728125\n8.10\n8.23\n7.44\n3.284688\n32.854444\n56.615819\n6.598065\n39.054783\n\n\n1\n195600\n73261.408580\n3610.8\n54.160527\n33.228935\n26.072939\n63.835021\n10.092040\n35.789474\n5.814975\n...\n82.000000\n35.400000\n7.06\n7.57\n6.97\n3.300000\n0.000000\n100.000000\n0.000000\n38.160000\n\n\n2\n370000\n141385.794900\n8674.8\n42.651374\n36.896246\n20.886408\n65.505593\n13.607999\n35.854342\n2.234626\n...\n84.500000\n14.700000\n7.35\n7.79\n7.27\n2.630000\n50.793651\n42.857143\n1.587302\n41.100000\n\n\n3\n236500\n94701.226400\n6058.1\n39.044243\n38.883039\n20.282830\n63.146450\n16.570720\n16.450216\n2.169910\n...\n84.400000\n25.800000\n7.47\n7.75\n7.21\n3.220000\n71.428571\n23.809524\n0.000000\n39.054783\n\n\n4\n320200\n114318.553900\n4323.3\n74.063670\n35.262694\n20.462585\n68.714872\n10.822543\n53.307393\n10.717273\n...\n84.500000\n19.600000\n7.23\n7.32\n7.09\n3.330000\n9.523810\n88.888889\n1.587302\n33.000000\n\n\n\n\n5 rows × 69 columns\nNow we have 69 columns. Looks much better than 41, we can move on!",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Lab: Working with multiple models</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_5/IM939_Lab_5_1.html#data-exploration-and-wrangling",
    "href": "content/labs/Lab_5/IM939_Lab_5_1.html#data-exploration-and-wrangling",
    "title": "21  Lab: Working with multiple models",
    "section": "21.2 Multidimensional scaling",
    "text": "Warning\n\n\n\nReflect here whether the mean() replacements for missing values taht we did above is sensible and/or would work for each and every column here. You might need more sophisticated methods for filling in some of the missing values, e.g., using a model-based approach to ‘predict’ a value.\n\nfrom sklearn.metrics import euclidean_distances\n\n# keep place names and store them in a variable\nplaceNames = df[\"Area/INDICATOR\"]\n\n# if we hadn't done it, we could have filled in the missing values also here.\n# numericColumns = numericColumns.fillna(numericColumns.mean())\n\n# let's centralize the data\nnumericColumns -= numericColumns.mean()\n\nCheck to make sure everything looks ok.\n\nnumericColumns.head()\n\n\n\n\n\n\n\n\nGLA Population Estimate 2013\nGLA Household Estimate 2013\nInland Area (Hectares)\nPopulation density (per hectare) 2013\nAverage Age, 2013\nProportion of population aged 0-15, 2013\nProportion of population of working-age, 2013\nProportion of population aged 65 and over, 2013\n% of resident population born abroad (2013)\n% of largest migrant population (2013)\n...\nFemale life expectancy, (2010-12)\nTeenage conception rate (2012)\nLife satisfaction score 2012-13 (out of 10)\nWorthwhileness score 2012-13 (out of 10)\nHappiness score 2012-13 (out of 10)\nAnxiety score 2012-13 (out of 10)\nProportion of seats won by Conservatives in 2014 election\nProportion of seats won by Labour in 2014 election\nProportion of seats won by Lib Dems in 2014 election\nTurnout at 2014 local elections\n\n\n\n\n0\n-247760.606061\n-97761.616805\n-4473.681818\n-43.279630\n5.426932\n-11.500067\n8.480871\n3.019196\n0.000000\n0.000000\n...\n-1.421085e-14\n-3.552714e-15\n0.816364\n0.651212\n0.23303\n8.881784e-16\n0.000000\n0.000000\n-8.881784e-16\n7.105427e-15\n\n\n1\n-60160.606061\n-29014.579608\n-1153.281818\n-16.644971\n-2.648021\n6.624837\n-5.225725\n-1.399112\n-0.094397\n0.620617\n...\n-1.809375e+00\n9.671875e+00\n-0.223636\n-0.008788\n-0.23697\n1.531250e-02\n-32.854444\n43.384181\n-6.598065e+00\n-8.947826e-01\n\n\n2\n114239.393939\n39109.806712\n3910.718182\n-28.154125\n1.019290\n1.438305\n-3.555153\n2.116847\n-0.029529\n-2.959732\n...\n6.906250e-01\n-1.102813e+01\n0.066364\n0.211212\n0.06303\n-6.546875e-01\n17.939207\n-13.758676\n-5.010764e+00\n2.045217e+00\n\n\n3\n-19260.606061\n-7574.761788\n1294.018182\n-31.761255\n3.006083\n0.834727\n-5.914296\n5.079569\n-19.433655\n-3.024448\n...\n5.906250e-01\n7.187500e-02\n0.186364\n0.171212\n0.00303\n-6.468750e-02\n38.574127\n-32.806295\n-6.598065e+00\n7.105427e-15\n\n\n4\n64439.393939\n12042.565712\n-440.781818\n3.258171\n-0.614262\n1.014482\n-0.345874\n-0.668608\n17.423522\n5.522915\n...\n6.906250e-01\n-6.128125e+00\n-0.053636\n-0.258788\n-0.11697\n4.531250e-02\n-23.330634\n32.273070\n-5.010764e+00\n-6.054783e+00\n\n\n\n\n5 rows × 69 columns\n\n\n\nWe can plot out our many dimension space by uncommenting the code below (also note down how long does this take).\n\n#import seaborn as sns\n#sns_plot = sns.pairplot(numericColumns)\n#sns_plot.savefig(\"figs/output.png\")\n\nGiven that this takes quite a while (around 10 minutes), this is the image that would result from uncommenting and running the code above.\n\n\n\nDimension reduction will help us here!\n\n21.2 Multidimensional scaling\nWe could apply various different types of dimension reduction here. We are specifically going to capture the dissimilarity in the data using multidimensional scaling. We will need a distance matrix to start here.\n\nfrom sklearn import manifold\n\n# Here, we compute the euclidean distances between the columns by passing the same data twice\n# the resulting data matrix will now have the pairwise distances between the boroughs.\n# CAUTION: note that we are now building a distance matrix in a high-dimensional data space\n# remember the Curse of Dimensionality -- we need to be cautious with the distance values\ndistMatrix = euclidean_distances(numericColumns, numericColumns)\n\n# for instance, typing distMatrix.shape on the console gives:\n# Out[115]: (38, 38) # i.e., the number of rows\n\n# first we generate an MDS object and extract the projections\nmds = manifold.MDS(n_components = 2, max_iter=3000, n_init=1, dissimilarity=\"precomputed\", normalized_stress=False)\nY = mds.fit_transform(distMatrix)\n\nTo interpret what is happening, let us plot the boroughs on the projected two dimensional space.\n\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots()\nfig.set_size_inches(15, 15)\nplt.suptitle('MDS on only London boroughs')\nax.scatter(Y[:, 0], Y[:, 1], c=\"#D06B36\", s = 100, alpha = 0.8, linewidth=0)\n\nfor i, txt in enumerate(placeNames):\n    ax.annotate(txt, (Y[:, 0][i],Y[:, 1][i]))\n\n\n\n\n\n\n\n\nHere, we are projecting all the numeric variables, so it is difficult to get a sense of what these components represent and how to interpret the visualisation. We may want to project only a subset of features.\n\n\n\n\n\n\nFeature selection\n\n\n\nFeature selection is not straightforward and it is always a decision made by us informed by something (i.e. measures, literature –refer to the slides from last week). Below we will be embedding (machine learners’ word for projection) two different sets of features for you to compare.\n\n\n\n21.2.1 Feature selection: Happiness\nIn the example below, we are selecting happiness metrics. Pulling these out of our data and carrying out more multidimensional scaling can help us see how the boroughs differ in happiness.\n\n\n\n\n\n\nNote\n\n\n\nThe decision of selecting the features Life satisfaction score 2012-13 (out of 10), Worthwhileness score 2012-13 (out of 10) and Happiness score 2012-13 (out of 10) is not based on any numerical decision, but it is based on the semantics of the variables. In other words, these three variables provide different perspectives to describe how happy people are in the different neighbourhoods.\n\n\n\n# get the data columns relating to emotions and feelings\ndataOnEmotions = numericColumns[[\"Life satisfaction score 2012-13 (out of 10)\", \"Worthwhileness score 2012-13 (out of 10)\",\"Happiness score 2012-13 (out of 10)\"]]\n\n# a new distance matrix to represent \"emotional distance\"s\ndistMatrix2 = euclidean_distances(dataOnEmotions, dataOnEmotions)\n\n# compute a new \"embedding\" (machine learners' word for projection)\nY2 = mds.fit_transform(distMatrix2)\n\n# let's look at the results\nfig, ax = plt.subplots()\nfig.set_size_inches(15, 15)\nplt.suptitle('An \\\"emotional\\\" look at London boroughs')\nax.scatter(Y2[:, 0], Y2[:, 1], c=\"#D06B36\", s = 100, alpha = 0.8, linewidth=0)\n\nfor i, txt in enumerate(placeNames):\n    ax.annotate(txt, (Y2[:, 0][i],Y2[:, 1][i]))\n\n\n\n\n\n\n\n\nThe location of the different boroughs on the 2 dimensional multidimensional scaling space from the happiness metrics is\n\nresults_fixed = Y2.copy()\nprint(results_fixed)\n\n[[-0.96842689 -0.45475988]\n [ 0.30884742  0.15179769]\n [-0.16691283 -0.15803682]\n [-0.10177693 -0.25768786]\n [ 0.05785895  0.27427657]\n [-0.19834493 -0.41277366]\n [ 0.16569283  0.13333937]\n [ 0.24063752  0.21942675]\n [-0.18913393  0.08113099]\n [-0.1598099   0.04904532]\n [ 0.13658172  0.1767544 ]\n [ 0.19499445  0.26719731]\n [ 0.29875164  0.0257805 ]\n [ 0.11141595  0.13492429]\n [ 0.04459044 -0.14066626]\n [-0.02600529 -0.16833037]\n [-0.06882347 -0.1317033 ]\n [-0.03540934 -0.0644621 ]\n [ 0.22277681  0.50326906]\n [-0.38808697 -0.47422441]\n [ 0.1254132   0.01958298]\n [ 0.18626664  0.30951748]\n [-0.07094936  0.07075891]\n [ 0.07928372  0.10396892]\n [ 0.06468159 -0.04256096]\n [ 0.12678319 -0.10240873]\n [-0.07785182 -0.21537067]\n [-0.0153771  -0.065743  ]\n [ 0.05253965  0.05991075]\n [-0.02558256 -0.03995683]\n [-0.10504585 -0.05529746]\n [ 0.0201215  -0.01035603]\n [ 0.16029994  0.21365705]]\n\n\n\nprint(results_fixed.shape)\n\n(33, 2)\n\n\nWe may want to look at if the general happiness rating captures the position of the boroughs. To do this, we need to assign colours based on the binned happiness score.\n\nimport numpy as np\n\ncolorMappingValuesHappiness = np.asarray(dataOnEmotions[[\"Life satisfaction score 2012-13 (out of 10)\"]]).flatten()\ncolorMappingValuesHappiness.shape\n\n(33,)\n\n\n\n\n\ncolorMappingValuesHappiness\n#c = colorMappingValuesCrime, cmap = plt.cm.Greens\n\narray([ 0.81636364, -0.22363636,  0.06636364,  0.18636364, -0.05363636,\n        0.34636364, -0.06363636, -0.28363636, -0.04363636, -0.10363636,\n       -0.12363636, -0.21363636, -0.05363636, -0.08363636,  0.05636364,\n        0.11636364,  0.06636364,  0.01636364, -0.20363636,  0.39636364,\n        0.00636364, -0.19363636, -0.05363636, -0.10363636, -0.06363636,\n       -0.00363636,  0.13636364, -0.01363636, -0.03363636, -0.00363636,\n       -0.04363636, -0.05363636, -0.19363636])\n\n\nFinally, we can plot this. What can you see?\n\n# let's look at the results\nfig, ax = plt.subplots()\nfig.set_size_inches(15, 15)\nplt.suptitle('An \\\"emotional\\\" look at London boroughs')\n#ax.scatter(results_fixed[:, 0], results_fixed[:, 1], c = colorMappingValuesHappiness, cmap='viridis')\nplt.scatter(results_fixed[:, 0], results_fixed[:, 1], c = colorMappingValuesHappiness, s = 100, cmap=plt.cm.Greens)\n\nfor i, txt in enumerate(placeNames):\n    ax.annotate(txt, (results_fixed[:, 0][i],results_fixed[:, 1][i]))\n\n\n\n\n\n\n\n\n\n\n21.2.2 Feature selection: diversity\nSimilarly, we are now selecting features based on diveristy.\n\n# get the data columns relating to indicators that we think are related to \"diversity\" in one way or the other\ndataOnDiversity = numericColumns[[\"Proportion of population aged 0-15, 2013\", \"Proportion of population of working-age, 2013\", \"Proportion of population aged 65 and over, 2013\", \"% of population from BAME groups (2013)\", \"% people aged 3+ whose main language is not English (2011 census)\"]]\n\n# a new distance matrix to represent distances in \"diversity\"\ndistMatrix3 = euclidean_distances(dataOnDiversity, dataOnDiversity)\n\nmds = manifold.MDS(n_components = 2, max_iter=3000, n_init=1, dissimilarity=\"precomputed\", normalized_stress = False)\nY = mds.fit_transform(distMatrix3)\n\n# Visualising the data.\nfig, ax = plt.subplots()\nfig.set_size_inches(15, 15)\nplt.suptitle('A \\\"diversity\\\" look at London boroughs')\nax.scatter(Y[:, 0], Y[:, 1], s = 100, c = colorMappingValuesHappiness, cmap=plt.cm.Greens)\n\nfor i, txt in enumerate(placeNames):\n    ax.annotate(txt, (Y[:, 0][i],Y[:, 1][i]))\n\n\n\n\n\n\n\n\n\n\n\n21.3 It is now your turn!\n\n\n\n\n\n\nIt is your turn now\n\n\n\nFirst task:\nThis looks very different to the one above on “emotion” related variables. Our job now is to relate these two projections to one another. Do you see similarities? Do you see clusters of boroughs? Can you reflect on how you can relate and combine these two maps conceptually?\nSecond task:\nCan you think of and then generate other maps that you can produce with this data? Have a look at the variables once again and try to produce new “perspectives” to the data and see what they have to say.\nAlso think of visualisations to help you here, can you colour them with a different variable? What would that change?\n\n\n\n\n\n\n\n\n\nPage, Scott E. 2018. “Why ‘Many-Model Thinkers’ Make Better Decisions.” Harvard Business Review, November. https://hbr.org/2018/11/why-many-model-thinkers-make-better-decisions.",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Lab: Working with multiple models</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_5/IM939_Lab_5_1.html#multidimensional-scaling",
    "href": "content/labs/Lab_5/IM939_Lab_5_1.html#multidimensional-scaling",
    "title": "21  Lab: Working with multiple models",
    "section": "",
    "text": "We could apply various different types of dimension reduction here. We are specifically going to capture the dissimilarity in the data using multidimensional scaling. We will need a distance matrix to start here.\n\nfrom sklearn import manifold\n\n# Here, we compute the euclidean distances between the columns by passing the same data twice\n# the resulting data matrix will now have the pairwise distances between the boroughs.\n# CAUTION: note that we are now building a distance matrix in a high-dimensional data space\n# remember the Curse of Dimensionality -- we need to be cautious with the distance values\ndistMatrix = euclidean_distances(numericColumns, numericColumns)\n\n# for instance, typing distMatrix.shape on the console gives:\n# Out[115]: (38, 38) # i.e., the number of rows\n\n# first we generate an MDS object and extract the projections\nmds = manifold.MDS(n_components = 2, max_iter=3000, n_init=1, dissimilarity=\"precomputed\", normalized_stress=False)\nY = mds.fit_transform(distMatrix)\n\nTo interpret what is happening, let us plot the boroughs on the projected two dimensional space.\n\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots()\nfig.set_size_inches(15, 15)\nplt.suptitle('MDS on only London boroughs')\nax.scatter(Y[:, 0], Y[:, 1], c=\"#D06B36\", s = 100, alpha = 0.8, linewidth=0)\n\nfor i, txt in enumerate(placeNames):\n    ax.annotate(txt, (Y[:, 0][i],Y[:, 1][i]))\n\n\n\n\n\n\n\n\nHere, we are projecting all the numeric variables, so it is difficult to get a sense of what these components represent and how to interpret the visualisation. We may want to project only a subset of features.\n\n\n\n\n\n\nFeature selection\n\n\n\nFeature selection is not straightforward and it is always a decision made by us informed by something (i.e. measures, literature –refer to the slides from last week). Below we will be embedding (machine learners’ word for projection) two different sets of features for you to compare.\n\n\n\n21.2.1 Feature selection: Happiness\nIn the example below, we are selecting happiness metrics. Pulling these out of our data and carrying out more multidimensional scaling can help us see how the boroughs differ in happiness.\n\n\n\n\n\n\nNote\n\n\n\nThe decision of selecting the features Life satisfaction score 2012-13 (out of 10), Worthwhileness score 2012-13 (out of 10) and Happiness score 2012-13 (out of 10) is not based on any numerical decision, but it is based on the semantics of the variables. In other words, these three variables provide different perspectives to describe how happy people are in the different neighbourhoods.\n\n\n\n# get the data columns relating to emotions and feelings\ndataOnEmotions = numericColumns[[\"Life satisfaction score 2012-13 (out of 10)\", \"Worthwhileness score 2012-13 (out of 10)\",\"Happiness score 2012-13 (out of 10)\"]]\n\n# a new distance matrix to represent \"emotional distance\"s\ndistMatrix2 = euclidean_distances(dataOnEmotions, dataOnEmotions)\n\n# compute a new \"embedding\" (machine learners' word for projection)\nY2 = mds.fit_transform(distMatrix2)\n\n# let's look at the results\nfig, ax = plt.subplots()\nfig.set_size_inches(15, 15)\nplt.suptitle('An \\\"emotional\\\" look at London boroughs')\nax.scatter(Y2[:, 0], Y2[:, 1], c=\"#D06B36\", s = 100, alpha = 0.8, linewidth=0)\n\nfor i, txt in enumerate(placeNames):\n    ax.annotate(txt, (Y2[:, 0][i],Y2[:, 1][i]))\n\n\n\n\n\n\n\n\nThe location of the different boroughs on the 2 dimensional multidimensional scaling space from the happiness metrics is\n\nresults_fixed = Y2.copy()\nprint(results_fixed)\n\n[[-0.96842689 -0.45475988]\n [ 0.30884742  0.15179769]\n [-0.16691283 -0.15803682]\n [-0.10177693 -0.25768786]\n [ 0.05785895  0.27427657]\n [-0.19834493 -0.41277366]\n [ 0.16569283  0.13333937]\n [ 0.24063752  0.21942675]\n [-0.18913393  0.08113099]\n [-0.1598099   0.04904532]\n [ 0.13658172  0.1767544 ]\n [ 0.19499445  0.26719731]\n [ 0.29875164  0.0257805 ]\n [ 0.11141595  0.13492429]\n [ 0.04459044 -0.14066626]\n [-0.02600529 -0.16833037]\n [-0.06882347 -0.1317033 ]\n [-0.03540934 -0.0644621 ]\n [ 0.22277681  0.50326906]\n [-0.38808697 -0.47422441]\n [ 0.1254132   0.01958298]\n [ 0.18626664  0.30951748]\n [-0.07094936  0.07075891]\n [ 0.07928372  0.10396892]\n [ 0.06468159 -0.04256096]\n [ 0.12678319 -0.10240873]\n [-0.07785182 -0.21537067]\n [-0.0153771  -0.065743  ]\n [ 0.05253965  0.05991075]\n [-0.02558256 -0.03995683]\n [-0.10504585 -0.05529746]\n [ 0.0201215  -0.01035603]\n [ 0.16029994  0.21365705]]\n\n\n\nprint(results_fixed.shape)\n\n(33, 2)\n\n\nWe may want to look at if the general happiness rating captures the position of the boroughs. To do this, we need to assign colours based on the binned happiness score.\n\nimport numpy as np\n\ncolorMappingValuesHappiness = np.asarray(dataOnEmotions[[\"Life satisfaction score 2012-13 (out of 10)\"]]).flatten()\ncolorMappingValuesHappiness.shape\n\n(33,)\n\n\n\n\n\ncolorMappingValuesHappiness\n#c = colorMappingValuesCrime, cmap = plt.cm.Greens\n\narray([ 0.81636364, -0.22363636,  0.06636364,  0.18636364, -0.05363636,\n        0.34636364, -0.06363636, -0.28363636, -0.04363636, -0.10363636,\n       -0.12363636, -0.21363636, -0.05363636, -0.08363636,  0.05636364,\n        0.11636364,  0.06636364,  0.01636364, -0.20363636,  0.39636364,\n        0.00636364, -0.19363636, -0.05363636, -0.10363636, -0.06363636,\n       -0.00363636,  0.13636364, -0.01363636, -0.03363636, -0.00363636,\n       -0.04363636, -0.05363636, -0.19363636])\n\n\nFinally, we can plot this. What can you see?\n\n# let's look at the results\nfig, ax = plt.subplots()\nfig.set_size_inches(15, 15)\nplt.suptitle('An \\\"emotional\\\" look at London boroughs')\n#ax.scatter(results_fixed[:, 0], results_fixed[:, 1], c = colorMappingValuesHappiness, cmap='viridis')\nplt.scatter(results_fixed[:, 0], results_fixed[:, 1], c = colorMappingValuesHappiness, s = 100, cmap=plt.cm.Greens)\n\nfor i, txt in enumerate(placeNames):\n    ax.annotate(txt, (results_fixed[:, 0][i],results_fixed[:, 1][i]))\n\n\n\n\n\n\n\n\n\n\n21.2.2 Feature selection: diversity\nSimilarly, we are now selecting features based on diveristy.\n\n# get the data columns relating to indicators that we think are related to \"diversity\" in one way or the other\ndataOnDiversity = numericColumns[[\"Proportion of population aged 0-15, 2013\", \"Proportion of population of working-age, 2013\", \"Proportion of population aged 65 and over, 2013\", \"% of population from BAME groups (2013)\", \"% people aged 3+ whose main language is not English (2011 census)\"]]\n\n# a new distance matrix to represent distances in \"diversity\"\ndistMatrix3 = euclidean_distances(dataOnDiversity, dataOnDiversity)\n\nmds = manifold.MDS(n_components = 2, max_iter=3000, n_init=1, dissimilarity=\"precomputed\", normalized_stress = False)\nY = mds.fit_transform(distMatrix3)\n\n# Visualising the data.\nfig, ax = plt.subplots()\nfig.set_size_inches(15, 15)\nplt.suptitle('A \\\"diversity\\\" look at London boroughs')\nax.scatter(Y[:, 0], Y[:, 1], s = 100, c = colorMappingValuesHappiness, cmap=plt.cm.Greens)\n\nfor i, txt in enumerate(placeNames):\n    ax.annotate(txt, (Y[:, 0][i],Y[:, 1][i]))",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Lab: Working with multiple models</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_5/IM939_Lab_5_1.html#it-is-now-your-turn",
    "href": "content/labs/Lab_5/IM939_Lab_5_1.html#it-is-now-your-turn",
    "title": "21  Lab: Working with multiple models",
    "section": "21.3 It is now your turn!",
    "text": "21.3 It is now your turn!\n\n\n\n\n\n\nIt is your turn now\n\n\n\nFirst task:\nThis looks very different to the one above on “emotion” related variables. Our job now is to relate these two projections to one another. Do you see similarities? Do you see clusters of boroughs? Can you reflect on how you can relate and combine these two maps conceptually?\nSecond task:\nCan you think of and then generate other maps that you can produce with this data? Have a look at the variables once again and try to produce new “perspectives” to the data and see what they have to say.\nAlso think of visualisations to help you here, can you colour them with a different variable? What would that change?",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Lab: Working with multiple models</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_5/IM939_Lab_5_2.html",
    "href": "content/labs/Lab_5/IM939_Lab_5_2.html",
    "title": "22  Lab: Clustering and Ground Truth",
    "section": "",
    "text": "22.1 Data Wrangling\nimport pandas as pd\n\ndf = pd.read_csv('data/wine.csv')\nLook at our data.\ndf.head()\n\n\n\n\n\n\n\n\nClass label\nAlcohol\nMalic acid\nAsh\nAlcalinity of ash\nMagnesium\nTotal phenols\nFlavanoids\nNonflavanoid phenols\nProanthocyanins\nColor intensity\nHue\nOD280/OD315 of diluted wines\nProline\n\n\n\n\n0\n1\n14.23\n1.71\n2.43\n15.6\n127\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065\n\n\n1\n1\n13.20\n1.78\n2.14\n11.2\n100\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050\n\n\n2\n1\n13.16\n2.36\n2.67\n18.6\n101\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185\n\n\n3\n1\n14.37\n1.95\n2.50\n16.8\n113\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480\n\n\n4\n1\n13.24\n2.59\n2.87\n21.0\n118\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735\nThere is a column called Class label that gives us the ground truth. The wines come from three different cultivars. Knowing the actual grouping helps us to identify how well our methods can capture this ground truth.\nFollowing our process above, we should first get a sense of our data.\ndf.describe()\n\n\n\n\n\n\n\n\nClass label\nAlcohol\nMalic acid\nAsh\nAlcalinity of ash\nMagnesium\nTotal phenols\nFlavanoids\nNonflavanoid phenols\nProanthocyanins\nColor intensity\nHue\nOD280/OD315 of diluted wines\nProline\n\n\n\n\ncount\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n\n\nmean\n1.938202\n13.000618\n2.336348\n2.366517\n19.494944\n99.741573\n2.295112\n2.029270\n0.361854\n1.590899\n5.058090\n0.957449\n2.611685\n746.893258\n\n\nstd\n0.775035\n0.811827\n1.117146\n0.274344\n3.339564\n14.282484\n0.625851\n0.998859\n0.124453\n0.572359\n2.318286\n0.228572\n0.709990\n314.907474\n\n\nmin\n1.000000\n11.030000\n0.740000\n1.360000\n10.600000\n70.000000\n0.980000\n0.340000\n0.130000\n0.410000\n1.280000\n0.480000\n1.270000\n278.000000\n\n\n25%\n1.000000\n12.362500\n1.602500\n2.210000\n17.200000\n88.000000\n1.742500\n1.205000\n0.270000\n1.250000\n3.220000\n0.782500\n1.937500\n500.500000\n\n\n50%\n2.000000\n13.050000\n1.865000\n2.360000\n19.500000\n98.000000\n2.355000\n2.135000\n0.340000\n1.555000\n4.690000\n0.965000\n2.780000\n673.500000\n\n\n75%\n3.000000\n13.677500\n3.082500\n2.557500\n21.500000\n107.000000\n2.800000\n2.875000\n0.437500\n1.950000\n6.200000\n1.120000\n3.170000\n985.000000\n\n\nmax\n3.000000\n14.830000\n5.800000\n3.230000\n30.000000\n162.000000\n3.880000\n5.080000\n0.660000\n3.580000\n13.000000\n1.710000\n4.000000\n1680.000000\nNo missing data. The scales of our features vary (e.g., Magnesium is in the 100s whereas Hue is in the low single digits).\nHow about our feature distributions?\n# Reshape from wide to long format, so we can easily view all variables in a single plot.\ndf_long = df.melt(id_vars='Class label')\n\nimport seaborn as sns\n\n# Create seaborn violin plot\nmy_plot = sns.violinplot(data = df_long, x = 'variable', y = 'value')\n\n# Rotate x-axis labels\nmy_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=90)\n\n[Text(0, 0, 'Alcohol'),\n Text(1, 0, 'Malic acid'),\n Text(2, 0, 'Ash'),\n Text(3, 0, 'Alcalinity of ash'),\n Text(4, 0, 'Magnesium'),\n Text(5, 0, 'Total phenols'),\n Text(6, 0, 'Flavanoids'),\n Text(7, 0, 'Nonflavanoid phenols'),\n Text(8, 0, 'Proanthocyanins'),\n Text(9, 0, 'Color intensity'),\n Text(10, 0, 'Hue'),\n Text(11, 0, 'OD280/OD315 of diluted wines'),\n Text(12, 0, 'Proline ')]\nAs you can see, the scale of the different variables diffes enormously (this is specially true with Proline -refer to the output after df.describe()), so it makes sense to normalise our data.",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Lab: Clustering and Ground Truth</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_5/IM939_Lab_5_2.html#data-wrangling",
    "href": "content/labs/Lab_5/IM939_Lab_5_2.html#data-wrangling",
    "title": "22  Lab: Clustering and Ground Truth",
    "section": "",
    "text": "22.1.1 Normalise data\n\n\n\n\n\n\nWarning\n\n\n\nPlease note that Class label is a numerical column that in fact describes the ground truth categories, so it doesn’t make sense to normalise that variable. Therefore, we should exclude Class label from the normalisation.\n\n\n\n# Store labels as a variable before removing them from the dataframe\nclass_labels = df[\"Class label\"]\n\ndf_subset = df.iloc[:,1:]\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# create a scaler object\nscaler = MinMaxScaler()\n\ndf_norm = pd.DataFrame(scaler.fit_transform(df_subset), columns = df_subset.columns)\n\n# Re-add class labels in the first position\ndf_norm.insert(0, \"Class label\", class_labels)\n\ndf_norm\n\n\n\n\n\n\n\n\nClass label\nAlcohol\nMalic acid\nAsh\nAlcalinity of ash\nMagnesium\nTotal phenols\nFlavanoids\nNonflavanoid phenols\nProanthocyanins\nColor intensity\nHue\nOD280/OD315 of diluted wines\nProline\n\n\n\n\n0\n1\n0.842105\n0.191700\n0.572193\n0.257732\n0.619565\n0.627586\n0.573840\n0.283019\n0.593060\n0.372014\n0.455285\n0.970696\n0.561341\n\n\n1\n1\n0.571053\n0.205534\n0.417112\n0.030928\n0.326087\n0.575862\n0.510549\n0.245283\n0.274448\n0.264505\n0.463415\n0.780220\n0.550642\n\n\n2\n1\n0.560526\n0.320158\n0.700535\n0.412371\n0.336957\n0.627586\n0.611814\n0.320755\n0.757098\n0.375427\n0.447154\n0.695971\n0.646933\n\n\n3\n1\n0.878947\n0.239130\n0.609626\n0.319588\n0.467391\n0.989655\n0.664557\n0.207547\n0.558360\n0.556314\n0.308943\n0.798535\n0.857347\n\n\n4\n1\n0.581579\n0.365613\n0.807487\n0.536082\n0.521739\n0.627586\n0.495781\n0.490566\n0.444795\n0.259386\n0.455285\n0.608059\n0.325963\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n173\n3\n0.705263\n0.970356\n0.582888\n0.510309\n0.271739\n0.241379\n0.056962\n0.735849\n0.205047\n0.547782\n0.130081\n0.172161\n0.329529\n\n\n174\n3\n0.623684\n0.626482\n0.598930\n0.639175\n0.347826\n0.282759\n0.086498\n0.566038\n0.315457\n0.513652\n0.178862\n0.106227\n0.336662\n\n\n175\n3\n0.589474\n0.699605\n0.481283\n0.484536\n0.543478\n0.210345\n0.073840\n0.566038\n0.296530\n0.761092\n0.089431\n0.106227\n0.397290\n\n\n176\n3\n0.563158\n0.365613\n0.540107\n0.484536\n0.543478\n0.231034\n0.071730\n0.754717\n0.331230\n0.684300\n0.097561\n0.128205\n0.400856\n\n\n177\n3\n0.815789\n0.664032\n0.737968\n0.716495\n0.282609\n0.368966\n0.088608\n0.811321\n0.296530\n0.675768\n0.105691\n0.120879\n0.201141\n\n\n\n\n178 rows × 14 columns\n\n\n\n\n# Reshape from wide to long format\ndf_long = df_norm.melt(id_vars='Class label')\n\n# Create seaborn violin plot\nmy_plot = sns.violinplot(data = df_long, x = 'variable', y = 'value')\n\n# Rotate x-axis labels\nmy_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=90)\n\n\n[Text(0, 0, 'Alcohol'),\n Text(1, 0, 'Malic acid'),\n Text(2, 0, 'Ash'),\n Text(3, 0, 'Alcalinity of ash'),\n Text(4, 0, 'Magnesium'),\n Text(5, 0, 'Total phenols'),\n Text(6, 0, 'Flavanoids'),\n Text(7, 0, 'Nonflavanoid phenols'),\n Text(8, 0, 'Proanthocyanins'),\n Text(9, 0, 'Color intensity'),\n Text(10, 0, 'Hue'),\n Text(11, 0, 'OD280/OD315 of diluted wines'),\n Text(12, 0, 'Proline ')]\n\n\n\n\n\n\n\n\n\nAre there any patterns?\nHow about a pairplot?\n\nsns.pairplot(data = df_norm.iloc[:,1:])\n\n\n\n\n\n\n\n\nHmm, a few interesting correlations. Some of our variables are skewed. We could apply some PCA here to look at fewer dimension or even log transform some of the skewed variables.",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Lab: Clustering and Ground Truth</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_5/IM939_Lab_5_2.html#cluster-analysis",
    "href": "content/labs/Lab_5/IM939_Lab_5_2.html#cluster-analysis",
    "title": "22  Lab: Clustering and Ground Truth",
    "section": "22.2 Cluster analysis",
    "text": "22.2 Cluster analysis\nFor now we will just run a kmeans cluster and then check our results against the ground truth.\n\n22.2.1 Determining the number of clusters\nLets decide how many clusters we need.\n\nfrom sklearn.cluster import KMeans\n\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k, n_init = 10)\n    \n    # Fit model to samples\n    model.fit(df.iloc[:,1:])\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n\nimport matplotlib.pyplot as plt\n\nplt.plot(ks, inertias, '-o', color='black')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n\n\n\n\n\n\n\n\nWhat happens if we use the normalised data instead?\n\nfrom sklearn.cluster import KMeans\n\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k, n_init = 10)\n    \n    # Fit model to samples\n    model.fit(df_norm.iloc[:,1:])\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n\nimport matplotlib.pyplot as plt\n\nplt.plot(ks, inertias, '-o', color='black')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPause for thought\n\n\n\nBoth of the graphs are the same. Is that what you would expect?\n\n\nThree clusters seems about right (and matches our number of origonal labels).\n\ndf['Class label'].value_counts()\n\nClass label\n2    71\n1    59\n3    48\nName: count, dtype: int64\n\n\n\n\n22.2.2 Computing the clusters\n\n# Create a KMeans instance with k clusters: model\nk_means = KMeans(n_clusters=3)\n\n# Fit model to samples\ndf_k_means = k_means.fit(df.iloc[:,1:])\n\ndf['Three clusters'] = pd.Series(df_k_means.predict(df.iloc[:,1:].values), index = df.index)\ndf\n\n\n\n\n\n\n\n\nClass label\nAlcohol\nMalic acid\nAsh\nAlcalinity of ash\nMagnesium\nTotal phenols\nFlavanoids\nNonflavanoid phenols\nProanthocyanins\nColor intensity\nHue\nOD280/OD315 of diluted wines\nProline\nThree clusters\n\n\n\n\n0\n1\n14.23\n1.71\n2.43\n15.6\n127\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065\n1\n\n\n1\n1\n13.20\n1.78\n2.14\n11.2\n100\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050\n1\n\n\n2\n1\n13.16\n2.36\n2.67\n18.6\n101\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185\n1\n\n\n3\n1\n14.37\n1.95\n2.50\n16.8\n113\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480\n1\n\n\n4\n1\n13.24\n2.59\n2.87\n21.0\n118\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n173\n3\n13.71\n5.65\n2.45\n20.5\n95\n1.68\n0.61\n0.52\n1.06\n7.70\n0.64\n1.74\n740\n2\n\n\n174\n3\n13.40\n3.91\n2.48\n23.0\n102\n1.80\n0.75\n0.43\n1.41\n7.30\n0.70\n1.56\n750\n2\n\n\n175\n3\n13.27\n4.28\n2.26\n20.0\n120\n1.59\n0.69\n0.43\n1.35\n10.20\n0.59\n1.56\n835\n2\n\n\n176\n3\n13.17\n2.59\n2.37\n20.0\n120\n1.65\n0.68\n0.53\n1.46\n9.30\n0.60\n1.62\n840\n2\n\n\n177\n3\n14.13\n4.10\n2.74\n24.5\n96\n2.05\n0.76\n0.56\n1.35\n9.20\n0.61\n1.60\n560\n0\n\n\n\n\n178 rows × 15 columns\n\n\n\n\n# Create a KMeans instance with k clusters: model\nk_means = KMeans(n_clusters=3)\n\n# Fit model to samples using normalised dataframe\ndf_k_means = k_means.fit(df_norm.iloc[:,1:])\n\ndf_norm['Three clusters'] = pd.Series(df_k_means.predict(df_norm.iloc[:,1:].values), index = df_norm.index)\ndf_norm\n\n\n\n\n\n\n\n\nClass label\nAlcohol\nMalic acid\nAsh\nAlcalinity of ash\nMagnesium\nTotal phenols\nFlavanoids\nNonflavanoid phenols\nProanthocyanins\nColor intensity\nHue\nOD280/OD315 of diluted wines\nProline\nThree clusters\n\n\n\n\n0\n1\n0.842105\n0.191700\n0.572193\n0.257732\n0.619565\n0.627586\n0.573840\n0.283019\n0.593060\n0.372014\n0.455285\n0.970696\n0.561341\n0\n\n\n1\n1\n0.571053\n0.205534\n0.417112\n0.030928\n0.326087\n0.575862\n0.510549\n0.245283\n0.274448\n0.264505\n0.463415\n0.780220\n0.550642\n0\n\n\n2\n1\n0.560526\n0.320158\n0.700535\n0.412371\n0.336957\n0.627586\n0.611814\n0.320755\n0.757098\n0.375427\n0.447154\n0.695971\n0.646933\n0\n\n\n3\n1\n0.878947\n0.239130\n0.609626\n0.319588\n0.467391\n0.989655\n0.664557\n0.207547\n0.558360\n0.556314\n0.308943\n0.798535\n0.857347\n0\n\n\n4\n1\n0.581579\n0.365613\n0.807487\n0.536082\n0.521739\n0.627586\n0.495781\n0.490566\n0.444795\n0.259386\n0.455285\n0.608059\n0.325963\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n173\n3\n0.705263\n0.970356\n0.582888\n0.510309\n0.271739\n0.241379\n0.056962\n0.735849\n0.205047\n0.547782\n0.130081\n0.172161\n0.329529\n1\n\n\n174\n3\n0.623684\n0.626482\n0.598930\n0.639175\n0.347826\n0.282759\n0.086498\n0.566038\n0.315457\n0.513652\n0.178862\n0.106227\n0.336662\n1\n\n\n175\n3\n0.589474\n0.699605\n0.481283\n0.484536\n0.543478\n0.210345\n0.073840\n0.566038\n0.296530\n0.761092\n0.089431\n0.106227\n0.397290\n1\n\n\n176\n3\n0.563158\n0.365613\n0.540107\n0.484536\n0.543478\n0.231034\n0.071730\n0.754717\n0.331230\n0.684300\n0.097561\n0.128205\n0.400856\n1\n\n\n177\n3\n0.815789\n0.664032\n0.737968\n0.716495\n0.282609\n0.368966\n0.088608\n0.811321\n0.296530\n0.675768\n0.105691\n0.120879\n0.201141\n1\n\n\n\n\n178 rows × 15 columns",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Lab: Clustering and Ground Truth</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_5/IM939_Lab_5_2.html#clusters-and-ground-truth",
    "href": "content/labs/Lab_5/IM939_Lab_5_2.html#clusters-and-ground-truth",
    "title": "22  Lab: Clustering and Ground Truth",
    "section": "22.3 Clusters and Ground Truth",
    "text": "22.3 Clusters and Ground Truth\nNow that we have created three clusters, we may ask ourselves: Do our cluster labels match our ground truth? Did our cluster model capture reality?\n\nct = pd.crosstab(df['Three clusters'], df['Class label'])\nct\n\n\n\n\n\n\n\nClass label\n1\n2\n3\n\n\nThree clusters\n\n\n\n\n\n\n\n0\n0\n50\n19\n\n\n1\n46\n1\n0\n\n\n2\n13\n20\n29\n\n\n\n\n\n\n\nAnd now, we will repeat the same with the normalised dataframe:\n\nct_norm = pd.crosstab(df_norm['Three clusters'], df_norm['Class label'])\nct_norm\n\n\n\n\n\n\n\nClass label\n1\n2\n3\n\n\nThree clusters\n\n\n\n\n\n\n\n0\n59\n2\n0\n\n\n1\n0\n7\n48\n\n\n2\n0\n62\n0\n\n\n\n\n\n\n\nIt might be easier to see as a stacked plot (see this post).\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nct.plot.bar(stacked=True)\nplt.legend(title='Class label')\nplt.title(label = 'Clusters and ground truth (original dataset)')\n\nText(0.5, 1.0, 'Clusters and ground truth (original dataset)')\n\n\n\n\n\n\n\n\n\nAnd now, we repeat the same, but using the normalised dataset:\n\n# use the normalised dataset\nct_norm.plot.bar(stacked=True)\nplt.legend(title='Class label')\nplt.title(label = 'Clusters and ground truth (normalised dataset)')\n\nText(0.5, 1.0, 'Clusters and ground truth (normalised dataset)')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow has the kmeans model done compared to our ground truth?\n\n\n\n\nCheck the difference between the clusters using the original dataframe and the normalised dataframe.\nReflect on the meaning of the labels: Are the labels from ground truth and cluster labels related somehow? Are the labels meaningful?\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe need to be really careful here. We notice that it is not easily possible to compare the known class labels to clustering labels. The reason is that the clustering algorithm labels are just arbitrary and not assigned to any deterministic criteria. Each time you run the algorithm, you might get a different id for the labels. The reason is that the label itself doesn’t actually mean anything, what is important is the list of items that are in the same cluster and their relations.\n\n\n\n22.3.1 Principal Components Analysis\nA way to come over this ambiguity and evaluate the results is to look at a visualisations of the results and compare. But this brings in the question of what type of visualisation to use for looking at the clusters. An immediate alternative is to use scatterplots. However, it is not clear which axis to use for clustering. A common method to apply at this stage is to make use of PCA to get a 2D plane where we can project the data points and visualise them over this projection.\n\n# We will be excluding the column `Class label` located in the first position from the PCA.\ndf.iloc[:,1:14]\n\n\n\n\n\n\n\n\nAlcohol\nMalic acid\nAsh\nAlcalinity of ash\nMagnesium\nTotal phenols\nFlavanoids\nNonflavanoid phenols\nProanthocyanins\nColor intensity\nHue\nOD280/OD315 of diluted wines\nProline\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n173\n13.71\n5.65\n2.45\n20.5\n95\n1.68\n0.61\n0.52\n1.06\n7.70\n0.64\n1.74\n740\n\n\n174\n13.40\n3.91\n2.48\n23.0\n102\n1.80\n0.75\n0.43\n1.41\n7.30\n0.70\n1.56\n750\n\n\n175\n13.27\n4.28\n2.26\n20.0\n120\n1.59\n0.69\n0.43\n1.35\n10.20\n0.59\n1.56\n835\n\n\n176\n13.17\n2.59\n2.37\n20.0\n120\n1.65\n0.68\n0.53\n1.46\n9.30\n0.60\n1.62\n840\n\n\n177\n14.13\n4.10\n2.74\n24.5\n96\n2.05\n0.76\n0.56\n1.35\n9.20\n0.61\n1.60\n560\n\n\n\n\n178 rows × 13 columns\n\n\n\nAnd now, with the normalised dataset:\n\n# We will be excluding the column `Class label` located in the first position from the PCA.\ndf_norm.iloc[:,1:14]\n\n\n\n\n\n\n\n\nAlcohol\nMalic acid\nAsh\nAlcalinity of ash\nMagnesium\nTotal phenols\nFlavanoids\nNonflavanoid phenols\nProanthocyanins\nColor intensity\nHue\nOD280/OD315 of diluted wines\nProline\n\n\n\n\n0\n0.842105\n0.191700\n0.572193\n0.257732\n0.619565\n0.627586\n0.573840\n0.283019\n0.593060\n0.372014\n0.455285\n0.970696\n0.561341\n\n\n1\n0.571053\n0.205534\n0.417112\n0.030928\n0.326087\n0.575862\n0.510549\n0.245283\n0.274448\n0.264505\n0.463415\n0.780220\n0.550642\n\n\n2\n0.560526\n0.320158\n0.700535\n0.412371\n0.336957\n0.627586\n0.611814\n0.320755\n0.757098\n0.375427\n0.447154\n0.695971\n0.646933\n\n\n3\n0.878947\n0.239130\n0.609626\n0.319588\n0.467391\n0.989655\n0.664557\n0.207547\n0.558360\n0.556314\n0.308943\n0.798535\n0.857347\n\n\n4\n0.581579\n0.365613\n0.807487\n0.536082\n0.521739\n0.627586\n0.495781\n0.490566\n0.444795\n0.259386\n0.455285\n0.608059\n0.325963\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n173\n0.705263\n0.970356\n0.582888\n0.510309\n0.271739\n0.241379\n0.056962\n0.735849\n0.205047\n0.547782\n0.130081\n0.172161\n0.329529\n\n\n174\n0.623684\n0.626482\n0.598930\n0.639175\n0.347826\n0.282759\n0.086498\n0.566038\n0.315457\n0.513652\n0.178862\n0.106227\n0.336662\n\n\n175\n0.589474\n0.699605\n0.481283\n0.484536\n0.543478\n0.210345\n0.073840\n0.566038\n0.296530\n0.761092\n0.089431\n0.106227\n0.397290\n\n\n176\n0.563158\n0.365613\n0.540107\n0.484536\n0.543478\n0.231034\n0.071730\n0.754717\n0.331230\n0.684300\n0.097561\n0.128205\n0.400856\n\n\n177\n0.815789\n0.664032\n0.737968\n0.716495\n0.282609\n0.368966\n0.088608\n0.811321\n0.296530\n0.675768\n0.105691\n0.120879\n0.201141\n\n\n\n\n178 rows × 13 columns\n\n\n\n\nfrom sklearn.decomposition import PCA\n\nn_components = 2\n\npca = PCA(n_components=n_components)\ndf_pca = pca.fit(df.iloc[:,1:14])\ndf_pca_vals = df_pca.transform(df.iloc[:,1:14])\n\nAgain, we are repeating the same, but using the normalised dataframe:\n\n# We are repeating the same, with the normalised dataframe.\n\nfrom sklearn.decomposition import PCA\n\nn_components = 2\n\npca = PCA(n_components=n_components)\ndf_pca_norm = pca.fit(df_norm.iloc[:,1:14])\ndf_pca_norm_vals = df_pca_norm.transform(df_norm.iloc[:,1:14])\n\nGrab our projections and plot along with our cluster names.\n\n# Store components as a new column in the original dataframe.\ndf['c1'] = [item[0] for item in df_pca_vals]\ndf['c2'] = [item[1] for item in df_pca_vals]\n\nax = sns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Class label')\nax.set_title('Known labels visualised over PCs (Original data)')\n\nText(0.5, 1.0, 'Known labels visualised over PCs (Original data)')\n\n\n\n\n\n\n\n\n\nAnd the same, with the normalised dataframe:\n\n# We are repeating the same, with the normalised dataframe.\n\n# Store components as a new column in the normalised dataframe.\ndf_norm['c1'] = [item[0] for item in df_pca_norm_vals]\ndf_norm['c2'] = [item[1] for item in df_pca_norm_vals]\n\nax = sns.scatterplot(data = df_norm, x = 'c1', y = 'c2', hue = 'Class label')\nax.set_title('Known labels visualised over PCs (Normalised data)')\n\nText(0.5, 1.0, 'Known labels visualised over PCs (Normalised data)')\n\n\n\n\n\n\n\n\n\nIn the figure above, we colored the points based on the actual labels, we observe that there has been several misclassifications in the figure above (i.e., in the algorithm’s results). So one may choose to use an alternative algorithm or devise a better distance metric.\n\nax = sns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Three clusters')\nax.set_title('Results of the algorithm visualised over PCs (Original data)')\n\nText(0.5, 1.0, 'Results of the algorithm visualised over PCs (Original data)')\n\n\n\n\n\n\n\n\n\n\n# We are repeating the same, with the normalised dataframe.\nax = sns.scatterplot(data = df_norm, x = 'c1', y = 'c2', hue = 'Three clusters')\nax.set_title('Results of the algorithm visualised over PCs (Normalised data)')\n\nText(0.5, 1.0, 'Results of the algorithm visualised over PCs (Normalised data)')\n\n\n\n\n\n\n\n\n\nThis shows the parallelism between the clustering algorithm and PCA. By looking at the PCA loadings, we can find out what the x-axis mean and try to interpret the clusters (We leave this as an additional exercise for those interested).\nHow might your interpret the above plots? Did the kmeans model identify the ground truth?\nHow robust is our clustering? It may be that the kmeans algorithm becamse stuck or that a few outliers have biased the clustering.\nTwo ways to check are:\n\nRunning the model multiple times with different initial values.\nRemoving some data and running the modelling multiple times.\n\n\n\n22.3.2 Running the model multiple times\nRun the below cell a few times. What do you see?\n\n# Create a KMeans instance with k clusters: model\nk_means = KMeans(n_clusters=3, init='random', n_init = 10)\n\n# Fit model to samples\ndf_k_means = k_means.fit(df.iloc[:,1:14])\n\ndf['Three clusters'] = pd.Series(df_k_means.predict(df.iloc[:,1:14].values), index = df.index)\n\nax = sns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Three clusters')\nax.set_title('Results of the algorithm visualised over PCs (Original data)')\n\nText(0.5, 1.0, 'Results of the algorithm visualised over PCs (Original data)')\n\n\n\n\n\n\n\n\n\n\n\n# Create a KMeans instance with k clusters: model\nk_means = KMeans(n_clusters=3, init='random', n_init = 10)\n\n# Fit model to samples\ndf_norm_k_means = k_means.fit(df_norm.iloc[:,1:14])\n\ndf_norm['Three clusters'] = pd.Series(df_norm_k_means.predict(df_norm.iloc[:,1:14].values), index = df.index)\n\nax = sns.scatterplot(data = df_norm, x = 'c1', y = 'c2', hue = 'Three clusters')\nax.set_title('Results of the algorithm visualised over PCs (Normalised data)')\n\nText(0.5, 1.0, 'Results of the algorithm visualised over PCs (Normalised data)')\n\n\n\n\n\n\n\n\n\n\n\n22.3.3 Removing some data\nHow about with only 80% of the data?\n\ndf_sample = df.sample(frac=0.8, replace=False)\n\n# Create a KMeans instance with k clusters: model\nk_means = KMeans(n_clusters=3, init='random', n_init = 10)\n\n# Fit model to samples\ndf_k_means = k_means.fit(df_sample.iloc[:,1:14])\n\ndf_sample['Three clusters'] = pd.Series(df_k_means.predict(df_sample.iloc[:,1:14].values), index = df_sample.index)\n\nax = sns.scatterplot(data = df_sample, x = 'c1', y = 'c2', hue = 'Three clusters')\nax.set_title('Results of the algorithm visualised over PCs')\n\nText(0.5, 1.0, 'Results of the algorithm visualised over PCs')\n\n\n\n\n\n\n\n\n\nWe may want to automate the process of resampling the data or rerunning the model then perhaps plotting the different inertia values or creating different plots.\nDo you think our clustering algorithm is stable and provide similiar results even when some data is removed or the initial values are random?\nIf so, then is our algorithm capturing the ground truth?\n\n\n\n\nCortez, Paulo, A Cerdeira, F Almeida, T Matos, and J. Reis. 2009. “Wine Quality.” UCI Machine Learning Repository. https://doi.org/10.24432/C56S3T.",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Lab: Clustering and Ground Truth</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_5/IM939_Lab_5_3.html",
    "href": "content/labs/Lab_5/IM939_Lab_5_3.html",
    "title": "23  Lab: Cross validation",
    "section": "",
    "text": "Details of the crime dataset are here.\nWe are going to examine the data, fit and then cross-validate a regression model.\n\nimport pandas as pd\ndf = pd.read_csv('data/censusCrimeClean.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncommunityname\nfold\npopulation\nhouseholdsize\nracepctblack\nracePctWhite\nracePctAsian\nracePctHisp\nagePct12t21\nagePct12t29\n...\nNumStreet\nPctForeignBorn\nPctBornSameState\nPctSameHouse85\nPctSameCity85\nPctSameState85\nLandArea\nPopDens\nPctUsePubTrans\nViolentCrimesPerPop\n\n\n\n\n0\nLakewoodcity\n1\n0.19\n0.33\n0.02\n0.90\n0.12\n0.17\n0.34\n0.47\n...\n0.0\n0.12\n0.42\n0.50\n0.51\n0.64\n0.12\n0.26\n0.20\n0.20\n\n\n1\nTukwilacity\n1\n0.00\n0.16\n0.12\n0.74\n0.45\n0.07\n0.26\n0.59\n...\n0.0\n0.21\n0.50\n0.34\n0.60\n0.52\n0.02\n0.12\n0.45\n0.67\n\n\n2\nAberdeentown\n1\n0.00\n0.42\n0.49\n0.56\n0.17\n0.04\n0.39\n0.47\n...\n0.0\n0.14\n0.49\n0.54\n0.67\n0.56\n0.01\n0.21\n0.02\n0.43\n\n\n3\nWillingborotownship\n1\n0.04\n0.77\n1.00\n0.08\n0.12\n0.10\n0.51\n0.50\n...\n0.0\n0.19\n0.30\n0.73\n0.64\n0.65\n0.02\n0.39\n0.28\n0.12\n\n\n4\nBethlehemtownship\n1\n0.01\n0.55\n0.02\n0.95\n0.09\n0.05\n0.38\n0.38\n...\n0.0\n0.11\n0.72\n0.64\n0.61\n0.53\n0.04\n0.09\n0.02\n0.03\n\n\n\n\n5 rows × 102 columns\n\n\n\nOne hundred features. Too many for us to visualise at once.\nInstead, we can pick out particular variables and carry out a linear regression. To make our work simple we will look at ViolentCrimesPerPop as our dependent variable and medIncome as our indpendent variable.\nWe may wonder if there is more violent crime in low income areas.\nLet us create a new dataframe containing our regression variables. We do not have to do this I find it makes our work clearer.\n\ndf_reg = df[['communityname', 'medIncome', 'ViolentCrimesPerPop']]\ndf_reg\n\n\n\n\n\n\n\n\ncommunityname\nmedIncome\nViolentCrimesPerPop\n\n\n\n\n0\nLakewoodcity\n0.37\n0.20\n\n\n1\nTukwilacity\n0.31\n0.67\n\n\n2\nAberdeentown\n0.30\n0.43\n\n\n3\nWillingborotownship\n0.58\n0.12\n\n\n4\nBethlehemtownship\n0.50\n0.03\n\n\n...\n...\n...\n...\n\n\n1989\nTempleTerracecity\n0.42\n0.09\n\n\n1990\nSeasidecity\n0.28\n0.45\n\n\n1991\nWaterburytown\n0.31\n0.23\n\n\n1992\nWalthamcity\n0.44\n0.19\n\n\n1993\nOntariocity\n0.40\n0.48\n\n\n\n\n1994 rows × 3 columns\n\n\n\nPlot our data (a nice page on plotting regressions with seaborn is here).\n\nimport seaborn as sns\nsns.jointplot(data = df[['medIncome', 'ViolentCrimesPerPop']], \n              x = 'ViolentCrimesPerPop', \n              y = 'medIncome', kind='reg',\n              marker = '.')\n\n\n\n\n\n\n\n\nWe may want to z-transform or log these scores as they are heavily skewed.\n\nimport numpy as np\n\n# some values are 0 so 0.1 is added to prevent log giving us infinity\n# there may be a better way to do this!\ndf_reg.loc[:, 'ViolentCrimesPerPop_log'] = np.log(df_reg['ViolentCrimesPerPop'] + 0.1)\ndf_reg.loc[:,'medIncome_log'] = np.log(df_reg['medIncome'] + 0.1)\n\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_54664/3488182522.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_reg.loc[:, 'ViolentCrimesPerPop_log'] = np.log(df_reg['ViolentCrimesPerPop'] + 0.1)\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_54664/3488182522.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_reg.loc[:,'medIncome_log'] = np.log(df_reg['medIncome'] + 0.1)\n\n\n\ndf_reg\n\n\n\n\n\n\n\n\ncommunityname\nmedIncome\nViolentCrimesPerPop\nViolentCrimesPerPop_log\nmedIncome_log\n\n\n\n\n0\nLakewoodcity\n0.37\n0.20\n-1.203973\n-0.755023\n\n\n1\nTukwilacity\n0.31\n0.67\n-0.261365\n-0.891598\n\n\n2\nAberdeentown\n0.30\n0.43\n-0.634878\n-0.916291\n\n\n3\nWillingborotownship\n0.58\n0.12\n-1.514128\n-0.385662\n\n\n4\nBethlehemtownship\n0.50\n0.03\n-2.040221\n-0.510826\n\n\n...\n...\n...\n...\n...\n...\n\n\n1989\nTempleTerracecity\n0.42\n0.09\n-1.660731\n-0.653926\n\n\n1990\nSeasidecity\n0.28\n0.45\n-0.597837\n-0.967584\n\n\n1991\nWaterburytown\n0.31\n0.23\n-1.108663\n-0.891598\n\n\n1992\nWalthamcity\n0.44\n0.19\n-1.237874\n-0.616186\n\n\n1993\nOntariocity\n0.40\n0.48\n-0.544727\n-0.693147\n\n\n\n\n1994 rows × 5 columns\n\n\n\n\nimport seaborn as sns\nsns.jointplot(data = df_reg[['medIncome_log', 'ViolentCrimesPerPop_log']], \n              x = 'ViolentCrimesPerPop_log', \n              y = 'medIncome_log', kind='reg',\n              marker = '.')\n\n\n\n\n\n\n\n\nIs log transforming our variables the right thing to do here?\nFit our regression to the log transformed data.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\nx = df_reg[['ViolentCrimesPerPop_log']]\ny = df_reg[['medIncome_log']]\n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\ny_hat = model.predict(x)\nplt.plot(x, y,'o', alpha = 0.5)\nplt.plot(x, y_hat, 'r', alpha = 0.5)\n\nplt.xlabel('Violent Crimes Per Population')\nplt.ylabel('Median Income')\n\nprint (\"MSE:\", metrics.mean_squared_error(y_hat, y))\nprint (\"R^2:\", metrics.r2_score(y, y_hat))\nprint (\"var:\", y.var())\n\nMSE: 0.1531885348757034\nR^2: 0.22763497704356928\nvar: medIncome_log    0.198436\ndtype: float64\n\n\n\n\n\n\n\n\n\nHas our log transformation distorted the pattern in the data?\n\nx = df_reg[['ViolentCrimesPerPop']]\ny = df_reg[['medIncome']]\n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\ny_hat = model.predict(x)\nplt.plot(x, y,'o', alpha = 0.5)\nplt.plot(x, y_hat, 'r', alpha = 0.5)\n\nplt.xlabel('Violent Crimes Per Population')\nplt.ylabel('Median Income')\n\nprint (\"MSE:\", metrics.mean_squared_error(y_hat, y))\nprint (\"R^2:\", metrics.r2_score(y, y_hat))\nprint (\"var:\", y.var())\n\nMSE: 0.03592636778157073\nR^2: 0.17996313165549482\nvar: medIncome    0.043833\ndtype: float64\n\n\n\n\n\n\n\n\n\nWhat is the relationship between violent crime and median income? Why might this be?\nAssuming the log data is fine, have we overfit the model? Remember that a good model (which accurately models the relationship between violent crimes per population) need to be robust when faced with new data.\nKfold cross validation splits data into train and test subsets. We can then fit the regression to the training set and see how well it does for the test set.\n\nfrom sklearn.model_selection import KFold\n\nX = df_reg[['ViolentCrimesPerPop']]\ny = df_reg[['medIncome']]\n\n# get four splits, Each split contains a \n# test series and a train series.\nkf = KFold(n_splits=4)\n\n\n# lists to store our statistics\nr_vals = []\nMSEs = []\nmedIncome_coef = []\n\nfor train_index, test_index in kf.split(X):\n    # fit our model and extract statistics\n    model = LinearRegression()\n    model.fit(X.iloc[train_index], y.iloc[train_index])\n    y_hat = model.predict(X.iloc[test_index])\n    \n    MSEs.append(metrics.mean_squared_error(y.iloc[test_index], y_hat))\n    medIncome_coef.append(model.coef_[0][0])\n    r_vals.append(metrics.r2_score(y.iloc[test_index], y_hat))\n\n\ndata = {'MSE' : MSEs, 'medIncome coefficient' : medIncome_coef, 'r squared' : r_vals}\npd.DataFrame(data)\n\n\n\n\n\n\n\n\nMSE\nmedIncome coefficient\nr squared\n\n\n\n\n0\n0.035727\n-0.403609\n0.130479\n\n\n1\n0.035904\n-0.389344\n0.162820\n\n\n2\n0.040777\n-0.353379\n0.200139\n\n\n3\n0.032255\n-0.378883\n0.182403\n\n\n\n\n\n\n\nDoes our model produce similiar coefficients with subsets of the data?\nWe can do this using an inbuild sklearn function (see here).\n\nfrom sklearn.model_selection import cross_val_score\nx = df_reg[['ViolentCrimesPerPop']]\ny = df_reg[['medIncome']]\n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\nprint(cross_val_score(model, x, y, cv=4))\n\n[0.13047946 0.16281953 0.20013867 0.18240261]\n\n\nWhat do these values tell us about our model and data?\nYou might want to carry out multiple regression with more than one predictor variable, or reduce the number of dimensions, or perhaps address different questions using a clustering algorithm instead with all or a subset of features.",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Lab: Cross validation</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_5/IM939_lab_5_Exercise.html",
    "href": "content/labs/Lab_5/IM939_lab_5_Exercise.html",
    "title": "24  IM939 lab 5 - Exercise",
    "section": "",
    "text": "The exercise this week is a chance to apply the code you have been delving into over these past week.\nChoose one of the datasets from a previous week:\n\nCrime Census\nLondon Borough\nWine\nIris\nGlobal warming\n\nOr, if you are feeling confident, another dataset from the sklearn datasets.\nnote Please do refer to the sklean and pandas documentation if you get stuck.\nRead in the data using pandas.\nLook at the first few rows. Get a feel for the structure of the data.\nDeal with missing values, if any.\nCreate a summary of the data. Plot any particular features or groups of features which you think are of interest.\nSettle on a possible question you want to answer. What might you be able to learn from your dataset?\nDecide on your initial analysis. Remember, we have covered:\n\nLinear regressions\nDimension reduction\nClustering\n\nWhich method will best allow you to tackle your question?\nApply your chosen analysis method below. Please do refer to and copy and paste code from previous weeks.\nCan you visualise your result?\nYou may want to use a:\n\nScatterplot\nHistogram\nAny other plot, such as those in the seaborn example library.\n\nAre you able to check if your method is robust (e.g., kfold test of regressions or cluster stability checks)? Perhaps do that below.\nHmm, what have you learned?\nYou may want to consider if you could convince a friend of your conclusion. Perhaps another type of analysis is needed or there are issues with the analysis you chose above!\nUse the space below to explore a bit more.",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>IM939 lab 5 - Exercise</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-06.html",
    "href": "content/sessions/session-06.html",
    "title": "25  Introduction",
    "section": "",
    "text": "25.1 Highlights of the lecture\nIn the session, we will discuss causality and when and to what extent it can be expected and observed, and while also discussing the notion of confounding. We will then discuss statistical traps such as Simpson’s paradox, regression to the mean, as well as touching upon the discussions of around the null-hypothesis testing process and the new statistics. We will also look at how visualisations can deceive and what we need to be careful about when representing data visually, as well as some of the cognitive biases that might have implications on how inferences and decisions based on data are made.",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-06.html#practical-lab-session",
    "href": "content/sessions/session-06.html#practical-lab-session",
    "title": "25  Introduction",
    "section": "25.2 Practical Lab Session",
    "text": "25.2 Practical Lab Session\nIn the practical session, we will explore some examples of how such traps could be encountered in practice. We will then spend some considerable time on exploring data visualisations and making better decisions in designing effective visual representations to support our reasoning.",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-06.html#reading-lists-resources",
    "href": "content/sessions/session-06.html#reading-lists-resources",
    "title": "25  Introduction",
    "section": "25.3 Reading lists & Resources",
    "text": "25.3 Reading lists & Resources\n\n25.3.1 Required reading\n\nThe New Statistics Debate: Here is an easy read from Geoff Cummings on the “New Statistics” where there is also a link to his really fun to watch “Dance of p-values” video: Cumming , Geoff. “Mind Your Confidence Interval: How Statistics Skew Research Results.” The Conversation, 22 Oct. 2020. [link] (see the optional reading for Cumming’s already transformational paper)\nSimpson’s paradox: You can read the first two sections of this rather practical paper with very good examples of how one can observe the paradox in research settings but also in real settings: Kievit, R., Frankenhuis, W.E., Waldorp, L. and Borsboom, D., 2013. Simpson’s paradox in psychological science: a practical guide. Frontiers in psychology, 4, p.513. [online paper]\nOn confounding: An accessible introduction to confounding effects with some practical advice on how to approach and avoid them: Skelly, A.C., Dettori, J.R. and Brodt, E.D., 2012. Assessing bias: the importance of considering confounding. Evidence-based spine-care journal, 3(1), p.9. [pdf]\nOn cognitive biases in visualisation: Valdez, A.C., Ziefle, M. and Sedlmair, M., 2018. Studying Biases in Visualization Research: Framework and Methods. In Cognitive Biases in Visualizations (pp. 13-27). Springer, Cham. [pdf]\n\nThis paper refers to the cognitive bias cheat sheet here which is a useful visual exploration of the biases: https://busterbenson.com/piles/cognitive-biases/\n\n\n\n\n25.3.2 Optional reading\n\nA causal modelling explanation to Simpson’s paradox by Judea Pearl: Pearl, J., 2014. Comment: understanding Simpson’s paradox. The American Statistician, 68(1), pp.8-13. [pdf]\nFor a more informative and academic coverage of the New Statistics, this is Cumming’s very influential article which also forms the basis of his book: Cumming, G. (2014) ‘The New Statistics: Why and How’, Psychological Science, 25(1), pp. 7–29. doi: 10.1177/0956797613504966. [pdf]\nOn regression to the mean: Barnett, A.G., Van Der Pols, J.C. and Dobson, A.J., 2005. Regression to the mean: what it is and how to deal with it. International journal of epidemiology, 34(1), pp.215-220. [pdf]\n\n\n\n25.3.3 Further reading\n\nA good read on p-values from the highly interesting book “Reinhart, A., 2015. Statistics done wrong: The woefully complete guide. No starch press.”: https://www.statisticsdonewrong.com/data-analysis.html\nA fun and short example of causation/correlation relation: Matthews, R., 2000. Storks deliver babies (p= 0.008). Teaching Statistics, 22(2), pp.36-38. [pdf]",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_1-illusions.html",
    "href": "content/labs/Lab_6/IM939_Lab_6_1-illusions.html",
    "title": "26  Lab: Illusions",
    "section": "",
    "text": "26.1 Clustering illusion\nThe cluster illusion refers to the natural tendency to wrongly see patterns or clusters in random data. You can find details of the clustering illusion here, here, and here.\nWe will try to replicate the image on the aside by generating a random dataset of 700 points with random x and y coordinates, and visualising them on a scatterplot to see if we can see any false clustering:\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport altair as alt\n\n# Set number of observations in the dataframe that we are about to create.\nn = 700\n\n# Generate a random dataframe.\nd = {'x': np.random.uniform(0, 100, n), 'y': np.random.uniform(0, 100, n)}\ndf = pd.DataFrame(d)\n\n# Create a scatterplot with seaborn.\nsns.relplot(data = df, x = 'x', y = 'y')\nAnd now the same but using altair instead of seaborn. The syntax for building this type in Altair is pretty straight forward.\n# Basic scatterplot in Altair\nalt.Chart(df).mark_circle(size=5).encode(\n    x='x',\n    y='y')\nTo remove those pesky lines we need to specify we want an x and y axis without grid lines.\nalt.Chart(df).mark_circle(size=5).encode(\n    alt.X('x', axis=alt.Axis(grid=False)),\n    alt.Y('y', axis=alt.Axis(grid=False)))\nWe will do a lot of altering axis and colors in altair. We do this by specifying alt.axistype and then passing various options.\nalt.Chart(df).mark_circle(size=5).encode(\n    alt.X('x', axis=alt.Axis(grid=False, title='Height')),\n    alt.Y('y', axis=alt.Axis(grid=False, title='Weight')))",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Lab: Illusions</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_1-illusions.html#clustering-illusion",
    "href": "content/labs/Lab_6/IM939_Lab_6_1-illusions.html#clustering-illusion",
    "title": "26  Lab: Illusions",
    "section": "",
    "text": "1,000 points randomly distributed inside a square, showing apparent clusters and empty spaces\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDo you see any clustering the in the above plots? What about if we give names to the different columns?\n\n\n\n\n\n26.1.1 Streaks\nAnother example of the clustering illusion is the idea of ‘streaks’, which consists of (wrongly) identifying a pattern from a small sample and extrapolate out.\n\n\n\nLet’s imagine you roll a dice. What are the odds of it being 6? And if we do a second roll? And a third? And if we repeat it 10 times? Or conversely, would you be able to predict what the next dice roll would be after 10 attemtps?\nLet’s figure it out!\n\nn_rolls = 10\n\n# Generate a dataframe where we randomly get a number from 1-6 per n rounds.\nd = {'round': np.linspace(1,n_rolls,n_rolls), 'roll': np.random.randint(1,7,n_rolls)}\ndf_dice = pd.DataFrame(d)\n\ndf_dice\n\n\n\n\n\n\n\n\nround\nroll\n\n\n\n\n0\n1.0\n3\n\n\n1\n2.0\n3\n\n\n2\n3.0\n1\n\n\n3\n4.0\n4\n\n\n4\n5.0\n6\n\n\n5\n6.0\n1\n\n\n6\n7.0\n3\n\n\n7\n8.0\n1\n\n\n8\n9.0\n5\n\n\n9\n10.0\n2\n\n\n\n\n\n\n\n\nsns.scatterplot(data=df_dice, x='round', y='roll')\n\n&lt;Axes: xlabel='round', ylabel='roll'&gt;\n\n\n\n\n\n\n\n\n\n\nalt.Chart(df_dice).mark_circle(size=20).encode(\n    alt.X('round', axis=alt.Axis(grid=False)),\n    alt.Y('roll', axis=alt.Axis(grid=False)))\n\n\n\n\n\n\n\nEach number on the dice will occur the same number of times. Any patterns you see are due to extrapolating based on a small sample. We can check that though by rolling the ‘dice’ 1,000,000 times.\n\nn_rolls = 1000000\nd = {'round': np.linspace(1,n_rolls,n_rolls), 'roll': np.random.randint(1,7,n_rolls)}\ndf_dice_many = pd.DataFrame(d)\n\ndf_dice_many.groupby('roll').count()\n\n\n\n\n\n\n\n\nround\n\n\nroll\n\n\n\n\n\n1\n166875\n\n\n2\n166654\n\n\n3\n166284\n\n\n4\n166441\n\n\n5\n166753\n\n\n6\n166993",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Lab: Illusions</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_1-illusions.html#weber-fechner-law",
    "href": "content/labs/Lab_6/IM939_Lab_6_1-illusions.html#weber-fechner-law",
    "title": "26  Lab: Illusions",
    "section": "26.2 Weber-Fechner Law",
    "text": "26.2 Weber-Fechner Law\n\n‘The Weber-Fechner Law is a famous finding of early psychophysics indicating that differences between stimuli are detected on a logarithmic scale. It takes more additional millimeters of radius to discern two larger circles than two smaller circles. This type of bias is probably one of the most researched biases in visualization research.’\n– (Calero Valdez, Ziefle, and Sedlmair 2018)\n\nLet us see if we can create a plot to demonstrate it.\nWe will load in the car crashes dataset from seaborn. Documentation of the data is here.\n\ndf_crashes = sns.load_dataset('car_crashes')\ndf_crashes.head()\n\n\n\n\n\n\n\n\ntotal\nspeeding\nalcohol\nnot_distracted\nno_previous\nins_premium\nins_losses\nabbrev\n\n\n\n\n0\n18.8\n7.332\n5.640\n18.048\n15.040\n784.55\n145.08\nAL\n\n\n1\n18.1\n7.421\n4.525\n16.290\n17.014\n1053.48\n133.93\nAK\n\n\n2\n18.6\n6.510\n5.208\n15.624\n17.856\n899.47\n110.35\nAZ\n\n\n3\n22.4\n4.032\n5.824\n21.056\n21.280\n827.34\n142.39\nAR\n\n\n4\n12.0\n4.200\n3.360\n10.920\n10.680\n878.41\n165.63\nCA\n\n\n\n\n\n\n\nTo illustrate this ‘illusion’ we will plot the percentage of drivers speeding, percentage of alcohol impaired and set the size of the point equal to the percentage of drivers not previously involves in any accident. Each point is an american state.\nAre there any relationships or patterns in the data?\n\nsns.scatterplot(data=df_crashes, x='speeding', y='alcohol', size='no_previous')\n\n&lt;Axes: xlabel='speeding', ylabel='alcohol'&gt;\n\n\n\n\n\n\n\n\n\nIs it easier to distinguish the different sizes in the below plot?\n\nsns.scatterplot(data=df_crashes,\n                x='speeding',\n                y='alcohol',\n                size='no_previous',\n                sizes=(10,40))\n\n&lt;Axes: xlabel='speeding', ylabel='alcohol'&gt;\n\n\n\n\n\n\n\n\n\nHow about this one?\n\nsns.scatterplot(data=df_crashes,\n                x='speeding',\n                y='alcohol',\n                size='no_previous',\n                sizes=(40,70))\n\n&lt;Axes: xlabel='speeding', ylabel='alcohol'&gt;\n\n\n\n\n\n\n\n\n\nThe values are the same. We have just changed the range of sizes.\nWe can do much the same in altair.\n\nalt.Chart(df_crashes).mark_circle().encode(\n    x='speeding',\n    y='alcohol',\n    size='no_previous'\n)\n\n\n\n\n\n\n\n\nalt.Chart(df_crashes).mark_circle().encode(\n    x='speeding',\n    y='alcohol',\n    size = alt.Size('no_previous', scale=alt.Scale(range=[10,40]))\n)\n\n\n\n\n\n\n\n\nalt.Chart(df_crashes).mark_circle().encode(\n    x='speeding',\n    y='alcohol',\n    size = alt.Size('no_previous', scale=alt.Scale(range=[40,70]))\n)\n\n\n\n\n\n\n\nHave you come across any other illusions? If so, try and plot them out. I sometimes find it easier to understand these things through creating simple illustrations of my own.\n\n\n\n\nCalero Valdez, André, Martina Ziefle, and Michael Sedlmair. 2018. “Studying Biases in Visualization Research: Framework and Methods.” In Cognitive Biases in Visualizations, edited by Geoffrey Ellis, 13–27. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-95831-6_2.",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Lab: Illusions</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html",
    "href": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html",
    "title": "27  Lab: Axes manipulation",
    "section": "",
    "text": "27.1 Data wrangling\nWe are going to use polls from the 2020 USA presidential election. As before, we load and examine the data.\nimport pandas as pd \nimport seaborn as sns\nimport altair as alt \n\ndf_polls = pd.read_csv('data/presidential_poll_averages_2020.csv')\ndf_polls.head()\n\n\n\n\n\n\n\n\ncycle\nstate\nmodeldate\ncandidate_name\npct_estimate\npct_trend_adjusted\n\n\n\n\n0\n2020\nWyoming\n11/3/2020\nJoseph R. Biden Jr.\n30.81486\n30.82599\n\n\n1\n2020\nWisconsin\n11/3/2020\nJoseph R. Biden Jr.\n52.12642\n52.09584\n\n\n2\n2020\nWest Virginia\n11/3/2020\nJoseph R. Biden Jr.\n33.49125\n33.51517\n\n\n3\n2020\nWashington\n11/3/2020\nJoseph R. Biden Jr.\n59.34201\n59.39408\n\n\n4\n2020\nVirginia\n11/3/2020\nJoseph R. Biden Jr.\n53.74120\n53.72101\nFor our analysis, we are going to pick estimates from 11/3/2020 for the swing states of Florida, Texas, Arizona, Michigan, Minnesota and Pennsylvania.\ndf_nov = df_polls[\n    (df_polls.modeldate == '11/3/2020')\n]\n\ndf_nov = df_nov[\n    (df_nov.candidate_name == 'Joseph R. Biden Jr.') |\n    (df_nov.candidate_name == 'Donald Trump')\n]\n\ndf_swing = df_nov[\n    (df_nov['state'] == 'Florida') |\n    (df_nov['state'] == 'Texas' ) |\n    (df_nov['state'] == 'Arizona' ) |\n    (df_nov['state'] == 'Michigan' ) |\n    (df_nov['state'] == 'Minnesota' ) |\n    (df_nov['state'] == 'Pennsylvania' ) \n]\n\ndf_swing\n\n\n\n\n\n\n\n\ncycle\nstate\nmodeldate\ncandidate_name\npct_estimate\npct_trend_adjusted\n\n\n\n\n7\n2020\nTexas\n11/3/2020\nJoseph R. Biden Jr.\n47.46643\n47.44781\n\n\n12\n2020\nPennsylvania\n11/3/2020\nJoseph R. Biden Jr.\n50.22000\n50.20422\n\n\n30\n2020\nMinnesota\n11/3/2020\nJoseph R. Biden Jr.\n51.86992\n51.84517\n\n\n31\n2020\nMichigan\n11/3/2020\nJoseph R. Biden Jr.\n51.17806\n51.15482\n\n\n46\n2020\nFlorida\n11/3/2020\nJoseph R. Biden Jr.\n49.09162\n49.08035\n\n\n53\n2020\nArizona\n11/3/2020\nJoseph R. Biden Jr.\n48.72237\n48.70539\n\n\n63\n2020\nTexas\n11/3/2020\nDonald Trump\n48.57118\n48.58794\n\n\n68\n2020\nPennsylvania\n11/3/2020\nDonald Trump\n45.57216\n45.55034\n\n\n86\n2020\nMinnesota\n11/3/2020\nDonald Trump\n42.63638\n42.66826\n\n\n87\n2020\nMichigan\n11/3/2020\nDonald Trump\n43.20577\n43.23326\n\n\n102\n2020\nFlorida\n11/3/2020\nDonald Trump\n46.68101\n46.61909\n\n\n109\n2020\nArizona\n11/3/2020\nDonald Trump\n46.11074\n46.10181",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Lab: Axes manipulation</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#default-barplot",
    "href": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#default-barplot",
    "title": "27  Lab: Axes manipulation",
    "section": "27.2 Default barplot",
    "text": "27.2 Default barplot\nWe can look at the relative performance of the candidates within each state using a nested bar plot.\n\nax = sns.barplot(\n    data = df_swing, \n    x = 'state', \n    y = 'pct_estimate', \n    hue = 'candidate_name')",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Lab: Axes manipulation</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#altering-the-axes",
    "href": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#altering-the-axes",
    "title": "27  Lab: Axes manipulation",
    "section": "27.3 Altering the axes",
    "text": "27.3 Altering the axes\nAltering the axis increases the distance between the bars. Some might say that is misleading.\n\nax = sns.barplot(\n    data = df_swing, \n    x = 'state', \n    y = 'pct_estimate', \n    hue = 'candidate_name')\n\nax.set(ylim=(41, 52))\n\n[(41.0, 52.0)]\n\n\n\n\n\n\n\n\n\nWhat do you think?\nHow about if we instead put the data on the full 0 to 100 scale?\n\nax = sns.barplot(\n    data = df_swing, \n    x = 'state', \n    y = 'pct_estimate', \n    hue = 'candidate_name')\n\nax.set(ylim=(0, 100))\n\n[(0.0, 100.0)]\n\n\n\n\n\n\n\n\n\nWe can do the same thing in Altair.\n\nalt.Chart(df_swing).mark_bar().encode(\n    x='candidate_name',\n    y='pct_estimate',\n    color='candidate_name',\n    column = alt.Column('state:O', spacing = 5, header = alt.Header(labelOrient = \"bottom\")),\n)\n\n\n\n\n\n\n\nNote the need for the alt column. What happens if you do not provide an alt column?\nPassing the domain option to the scale of the Y axis allows us to choose the y axis range.\n\nalt.Chart(df_swing).mark_bar(clip=True).encode(\n    x='candidate_name',\n    y=alt.Y('pct_estimate', scale=alt.Scale(domain=[42,53])),\n    color='candidate_name',\n    column = alt.Column('state:O', spacing = 5, header = alt.Header(labelOrient = \"bottom\")),\n)",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Lab: Axes manipulation</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#altering-the-proportions",
    "href": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#altering-the-proportions",
    "title": "27  Lab: Axes manipulation",
    "section": "27.4 Altering the proportions",
    "text": "27.4 Altering the proportions\nWe can even be a bit tricky and stretch out the difference.\n\nalt.Chart(df_swing).mark_bar(clip=True).encode(\n    x='candidate_name',\n    y=alt.Y('pct_estimate', scale=alt.Scale(domain=[42,53])),\n    color='candidate_name',\n    column = alt.Column('state:O', spacing = 5, header = alt.Header(labelOrient = \"bottom\")),\n).properties(\n    width=20,\n    height=600\n)",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Lab: Axes manipulation</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#default-line-plot",
    "href": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#default-line-plot",
    "title": "27  Lab: Axes manipulation",
    "section": "27.5 Default line plot",
    "text": "27.5 Default line plot\nIt is not just bar plot that you can have fun with. Line plots are another interesting example.\nFor our simple line plot, we will need the poll data for a single state.\n\ndf_texas = df_polls[\n    df_polls['state'] == 'Texas'\n]\n\ndf_texas_bt = df_texas[\n    (df_texas['candidate_name'] == 'Donald Trump') |\n    (df_texas['candidate_name'] == 'Joseph R. Biden Jr.')\n]\n\ndf_texas_bt.head()\n\n\n\n\n\n\n\n\ncycle\nstate\nmodeldate\ncandidate_name\npct_estimate\npct_trend_adjusted\n\n\n\n\n7\n2020\nTexas\n11/3/2020\nJoseph R. Biden Jr.\n47.46643\n47.44781\n\n\n63\n2020\nTexas\n11/3/2020\nDonald Trump\n48.57118\n48.58794\n\n\n231\n2020\nTexas\n11/2/2020\nJoseph R. Biden Jr.\n47.46643\n47.44781\n\n\n287\n2020\nTexas\n11/2/2020\nDonald Trump\n48.57118\n48.58794\n\n\n455\n2020\nTexas\n11/1/2020\nJoseph R. Biden Jr.\n47.45590\n47.43400\n\n\n\n\n\n\n\nThe modeldate column is a string (object) and not date time. So we need to change that: we will create a new datetime column called date.\n\ndf_texas_bt['date'] = pd.to_datetime(df_texas_bt.loc[:,'modeldate'], format='%m/%d/%Y').copy()\n\ndf_texas_bt.dtypes\n\ncycle                          int64\nstate                         object\nmodeldate                     object\ncandidate_name                object\npct_estimate                 float64\npct_trend_adjusted           float64\ndate                  datetime64[ns]\ndtype: object\n\n\nCreate our line plot.\n\nalt.Chart(df_texas_bt).mark_line().encode(\n    y=alt.Y('pct_estimate', scale=alt.Scale(domain=[42,53])),\n    x='date',\n    color='candidate_name')\n\n\n\n\n\n\n\nSometimes multiple axis are used for each line, or in a combined line and bar plot.\nThe example here uses a dataframe with a column for each line. Our data does not have that.\n\ndf_texas_bt\nour_df = df_texas_bt[['candidate_name', 'pct_estimate', 'date']]\nour_df\n\n\n\n\n\n\n\n\ncandidate_name\npct_estimate\ndate\n\n\n\n\n7\nJoseph R. Biden Jr.\n47.46643\n2020-11-03\n\n\n63\nDonald Trump\n48.57118\n2020-11-03\n\n\n231\nJoseph R. Biden Jr.\n47.46643\n2020-11-02\n\n\n287\nDonald Trump\n48.57118\n2020-11-02\n\n\n455\nJoseph R. Biden Jr.\n47.45590\n2020-11-01\n\n\n...\n...\n...\n...\n\n\n28931\nDonald Trump\n49.09724\n2020-02-29\n\n\n28963\nJoseph R. Biden Jr.\n45.30901\n2020-02-28\n\n\n28995\nDonald Trump\n49.09676\n2020-02-28\n\n\n29027\nJoseph R. Biden Jr.\n45.30089\n2020-02-27\n\n\n29058\nDonald Trump\n49.07925\n2020-02-27\n\n\n\n\n502 rows × 3 columns\n\n\n\nPivot table allows us to reshape our dataframe.\n\nour_df = pd.pivot_table(our_df, index=['date'], columns = 'candidate_name')\nour_df.columns = our_df.columns.to_series().str.join('_')\nour_df.head()\n\n\n\n\n\n\n\n\npct_estimate_Donald Trump\npct_estimate_Joseph R. Biden Jr.\n\n\ndate\n\n\n\n\n\n\n2020-02-27\n49.07925\n45.30089\n\n\n2020-02-28\n49.09676\n45.30901\n\n\n2020-02-29\n49.09724\n45.30896\n\n\n2020-03-01\n49.09724\n45.30895\n\n\n2020-03-02\n48.91861\n45.37694\n\n\n\n\n\n\n\nDate here is the dataframe index. We want it to be a column.\n\nour_df['date1'] = our_df.index\nour_df.columns = ['Trump', 'Biden', 'date1']\nour_df.head()\n\n\n\n\n\n\n\n\nTrump\nBiden\ndate1\n\n\ndate\n\n\n\n\n\n\n\n2020-02-27\n49.07925\n45.30089\n2020-02-27\n\n\n2020-02-28\n49.09676\n45.30901\n2020-02-28\n\n\n2020-02-29\n49.09724\n45.30896\n2020-02-29\n\n\n2020-03-01\n49.09724\n45.30895\n2020-03-01\n\n\n2020-03-02\n48.91861\n45.37694\n2020-03-02\n\n\n\n\n\n\n\nCreating our new plot, to fool all those people who expect Trump to win in Texas.\n\nbase = alt.Chart(our_df).encode(\n        alt.X('date1')\n)\n\nline_A = base.mark_line(color='#5276A7').encode(\n    alt.Y('Trump', axis=alt.Axis(titleColor='#5276A7'), scale=alt.Scale(domain=[42,53]))\n)\n\nline_B = base.mark_line(color='#F18727').encode(\n    alt.Y('Biden', axis=alt.Axis(titleColor='#F18727'), scale=alt.Scale(domain=[35,53]))\n)\n\nalt.layer(line_A, line_B).resolve_scale(y='independent')\n\n\n\n\n\n\n\nDid you see what I did there?\nOf course, mixed axis plots are rarely purely line plots. Instead they can be mixes of different axis. For these and other plotting mistakes, the economist has a nice article here. You may want to try some of these plots with this data set or the world indicators dataset from a few weeks ago.",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Lab: Axes manipulation</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_3-Choropleths.html",
    "href": "content/labs/Lab_6/IM939_Lab_6_3-Choropleths.html",
    "title": "28  Lab: Choropleth Maps",
    "section": "",
    "text": "28.1 Data preparations\nWe will be loading two datasets:\nimport geopandas as gpd \nimport pandas as pd\nimport altair as alt\n\ngeo_states = gpd.read_file('data/gz_2010_us_040_00_500k.json')\ndf_polls = pd.read_csv('data/presidential_poll_averages_2020.csv')\nLet’s explore the data first:\ngeo_states.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.6065, -70.82374 4...\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11578, -88.67563 ...\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.0577 44.99743, -104.25014 44.99...\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.0506 37.0004, -114.05 36.95777,...\nThis seems like a regular data frame, but there’s a feature that stands out from the others: geometry. This feature contains the coordinates thar define the polygons (or multipolygons) for every region in the map, in this case, every State in the USA. This is also an indicator that we are not using a regular dataframe, but a particular type of dataframe called GeoDataFrame:\ntype(geo_states)\n\ngeopandas.geodataframe.GeoDataFrame\nBecause this is a geospatial dataframe, we can visualise it as a map. In this case, we are going to use Altair to create a map using the AlbersUsa projection.\nalt.Chart(geo_states, title='US states').mark_geoshape().encode(\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\nAnd now the polls’ result:\ndf_polls\n\n\n\n\n\n\n\n\ncycle\nstate\nmodeldate\ncandidate_name\npct_estimate\npct_trend_adjusted\n\n\n\n\n0\n2020\nWyoming\n11/3/2020\nJoseph R. Biden Jr.\n30.81486\n30.82599\n\n\n1\n2020\nWisconsin\n11/3/2020\nJoseph R. Biden Jr.\n52.12642\n52.09584\n\n\n2\n2020\nWest Virginia\n11/3/2020\nJoseph R. Biden Jr.\n33.49125\n33.51517\n\n\n3\n2020\nWashington\n11/3/2020\nJoseph R. Biden Jr.\n59.34201\n59.39408\n\n\n4\n2020\nVirginia\n11/3/2020\nJoseph R. Biden Jr.\n53.74120\n53.72101\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n29080\n2020\nConnecticut\n2/27/2020\nDonald Trump\n33.66370\n34.58325\n\n\n29081\n2020\nColorado\n2/27/2020\nDonald Trump\n44.27899\n44.07662\n\n\n29082\n2020\nCalifornia\n2/27/2020\nDonald Trump\n34.66504\n34.69761\n\n\n29083\n2020\nArizona\n2/27/2020\nDonald Trump\n47.79450\n48.07208\n\n\n29084\n2020\nAlabama\n2/27/2020\nDonald Trump\n59.15000\n59.14228\n\n\n\n\n29085 rows × 6 columns\nAs you can see, modeldate has different dates. Let’s double check that:\ndf_polls.modeldate.unique()\n\narray(['11/3/2020', '11/2/2020', '11/1/2020', '10/31/2020', '10/30/2020',\n       '10/29/2020', '10/28/2020', '10/27/2020', '10/26/2020',\n       '10/25/2020', '10/24/2020', '10/23/2020', '10/22/2020',\n       '10/21/2020', '10/20/2020', '10/19/2020', '10/18/2020',\n       '10/17/2020', '10/16/2020', '10/15/2020', '10/14/2020',\n       '10/13/2020', '10/12/2020', '10/11/2020', '10/10/2020',\n       '10/9/2020', '10/8/2020', '10/7/2020', '10/6/2020', '10/5/2020',\n       '10/4/2020', '10/3/2020', '10/2/2020', '10/1/2020', '9/30/2020',\n       '9/29/2020', '9/28/2020', '9/27/2020', '9/26/2020', '9/25/2020',\n       '9/24/2020', '9/23/2020', '9/22/2020', '9/21/2020', '9/20/2020',\n       '9/19/2020', '9/18/2020', '9/17/2020', '9/16/2020', '9/15/2020',\n       '9/14/2020', '9/13/2020', '9/12/2020', '9/11/2020', '9/10/2020',\n       '9/9/2020', '9/8/2020', '9/7/2020', '9/6/2020', '9/5/2020',\n       '9/4/2020', '9/3/2020', '9/2/2020', '9/1/2020', '8/31/2020',\n       '8/30/2020', '8/29/2020', '8/28/2020', '8/27/2020', '8/26/2020',\n       '8/25/2020', '8/24/2020', '8/23/2020', '8/22/2020', '8/21/2020',\n       '8/20/2020', '8/19/2020', '8/18/2020', '8/17/2020', '8/16/2020',\n       '8/15/2020', '8/14/2020', '8/13/2020', '8/12/2020', '8/11/2020',\n       '8/10/2020', '8/9/2020', '8/8/2020', '8/7/2020', '8/6/2020',\n       '8/5/2020', '8/4/2020', '8/3/2020', '8/2/2020', '8/1/2020',\n       '7/31/2020', '7/30/2020', '7/29/2020', '7/28/2020', '7/27/2020',\n       '7/26/2020', '7/25/2020', '7/24/2020', '7/23/2020', '7/22/2020',\n       '7/21/2020', '7/20/2020', '7/19/2020', '7/18/2020', '7/17/2020',\n       '7/16/2020', '7/15/2020', '7/14/2020', '7/13/2020', '7/12/2020',\n       '7/11/2020', '7/10/2020', '7/9/2020', '7/8/2020', '7/7/2020',\n       '7/6/2020', '7/5/2020', '7/4/2020', '7/3/2020', '7/2/2020',\n       '7/1/2020', '6/30/2020', '6/29/2020', '6/28/2020', '6/27/2020',\n       '6/26/2020', '6/25/2020', '6/24/2020', '6/23/2020', '6/22/2020',\n       '6/21/2020', '6/20/2020', '6/19/2020', '6/18/2020', '6/17/2020',\n       '6/16/2020', '6/15/2020', '6/14/2020', '6/13/2020', '6/12/2020',\n       '6/11/2020', '6/10/2020', '6/9/2020', '6/8/2020', '6/7/2020',\n       '6/6/2020', '6/5/2020', '6/4/2020', '6/3/2020', '6/2/2020',\n       '6/1/2020', '5/31/2020', '5/30/2020', '5/29/2020', '5/28/2020',\n       '5/27/2020', '5/26/2020', '5/25/2020', '5/24/2020', '5/23/2020',\n       '5/22/2020', '5/21/2020', '5/20/2020', '5/19/2020', '5/18/2020',\n       '5/17/2020', '5/16/2020', '5/15/2020', '5/14/2020', '5/13/2020',\n       '5/12/2020', '5/11/2020', '5/10/2020', '5/9/2020', '5/8/2020',\n       '5/7/2020', '5/6/2020', '5/5/2020', '5/4/2020', '5/3/2020',\n       '5/2/2020', '5/1/2020', '4/30/2020', '4/29/2020', '4/28/2020',\n       '4/27/2020', '4/26/2020', '4/25/2020', '4/24/2020', '4/23/2020',\n       '4/22/2020', '4/21/2020', '4/20/2020', '4/19/2020', '4/18/2020',\n       '4/17/2020', '4/16/2020', '4/15/2020', '4/14/2020', '4/13/2020',\n       '4/12/2020', '4/11/2020', '4/10/2020', '4/9/2020', '4/8/2020',\n       '4/7/2020', '4/6/2020', '4/5/2020', '4/4/2020', '4/3/2020',\n       '4/2/2020', '4/1/2020', '3/31/2020', '3/30/2020', '3/29/2020',\n       '3/28/2020', '3/27/2020', '3/26/2020', '3/25/2020', '3/24/2020',\n       '3/23/2020', '3/22/2020', '3/21/2020', '3/20/2020', '3/19/2020',\n       '3/18/2020', '3/17/2020', '3/16/2020', '3/15/2020', '3/14/2020',\n       '3/13/2020', '3/12/2020', '3/11/2020', '3/10/2020', '3/9/2020',\n       '3/8/2020', '3/7/2020', '3/6/2020', '3/5/2020', '3/4/2020',\n       '3/3/2020', '3/2/2020', '3/1/2020', '2/29/2020', '2/28/2020',\n       '2/27/2020'], dtype=object)",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lab: Choropleth Maps</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_3-Choropleths.html#data-preparations",
    "href": "content/labs/Lab_6/IM939_Lab_6_3-Choropleths.html#data-preparations",
    "title": "28  Lab: Choropleth Maps",
    "section": "",
    "text": "geo_states: contains the geospatial polygons of the states in America, but does not contain any data about USA elections;\ndf_polls: the polling data we used in the last notebook, but does not have any geospatial polygons (you can find more information about every variable here).\n\n\n\n\n\n\n\n\n\n\n\n\n\n28.1.1 Filtering\nThat means, that we will need to filter our poll data to a specific date, in this case 11/2/2020\n\ndf_nov = df_polls[\n    (df_polls.modeldate == '11/3/2020')\n]\n\ndf_nov_states = df_nov[\n    (df_nov.candidate_name == 'Donald Trump') |\n    (df_nov.candidate_name == 'Joseph R. Biden Jr.')\n]\n\ndf_nov_states\n\n\n\n\n\n\n\n\ncycle\nstate\nmodeldate\ncandidate_name\npct_estimate\npct_trend_adjusted\n\n\n\n\n0\n2020\nWyoming\n11/3/2020\nJoseph R. Biden Jr.\n30.81486\n30.82599\n\n\n1\n2020\nWisconsin\n11/3/2020\nJoseph R. Biden Jr.\n52.12642\n52.09584\n\n\n2\n2020\nWest Virginia\n11/3/2020\nJoseph R. Biden Jr.\n33.49125\n33.51517\n\n\n3\n2020\nWashington\n11/3/2020\nJoseph R. Biden Jr.\n59.34201\n59.39408\n\n\n4\n2020\nVirginia\n11/3/2020\nJoseph R. Biden Jr.\n53.74120\n53.72101\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n107\n2020\nCalifornia\n11/3/2020\nDonald Trump\n32.28521\n32.43615\n\n\n108\n2020\nArkansas\n11/3/2020\nDonald Trump\n58.39097\n58.94886\n\n\n109\n2020\nArizona\n11/3/2020\nDonald Trump\n46.11074\n46.10181\n\n\n110\n2020\nAlaska\n11/3/2020\nDonald Trump\n50.99835\n51.23236\n\n\n111\n2020\nAlabama\n11/3/2020\nDonald Trump\n57.36153\n57.36126\n\n\n\n\n112 rows × 6 columns\n\n\n\n\n\n28.1.2 Computing percentages\nWe want to put the percentage estimates for each candidate onto the map. First, let us create a dataframe containing the data for each candidate.\n\n# Create separate data frame for Trump and Biden\ntrump_data = df_nov_states[\n    df_nov_states.candidate_name == 'Donald Trump'\n]\n\nbiden_data = df_nov_states[\n    df_nov_states.candidate_name == 'Joseph R. Biden Jr.'\n]\n\n\n\n28.1.3 Joining data\nAs we have seen before, we have two datasets that partially address our needs: geo_states contains the geospatial polygons of the states in America, but lacks data about USA elections; df_polls contains data about USA elections but lacks geometry.\nWe will need to combine both (joining) to create a (geospatial)dataframe that contains geometry AND polling data so we can create a choropleth map capable of answering our question: who is winning the elections?\nTo do so, we need to join both dataframes using a common feature. Our spatial and poll data have the name of the state in common, but their columns have different names.\nWe could rename the columns names, and then join them with pd.merge() but instead, we are going to use a less destructive way.\nWe can join the geospatial data and poll data using pd.merge() while providing different column names by using left_on for the left data (usually the geodataframe) and right_on for the right dataframe. We will be using this method, as it doesn’t require to rename columns.\n\n# Add the poll data (divided in two data frames) to a single geospatial dataframe.\ngeo_states_trump = geo_states.merge(\n    trump_data, left_on = 'NAME', right_on = 'state')\n\ngeo_states_biden = geo_states.merge(\n    biden_data, left_on = 'NAME', right_on = 'state')\n\n\ngeo_states_trump.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\ncycle\nstate\nmodeldate\ncandidate_name\npct_estimate\npct_trend_adjusted\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n2020\nMaine\n11/3/2020\nDonald Trump\n40.34410\n40.31588\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.6065, -70.82374 4...\n2020\nMassachusetts\n11/3/2020\nDonald Trump\n28.56164\n28.86275\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11578, -88.67563 ...\n2020\nMichigan\n11/3/2020\nDonald Trump\n43.20577\n43.23326\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.0577 44.99743, -104.25014 44.99...\n2020\nMontana\n11/3/2020\nDonald Trump\n49.74744\n49.78661\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.0506 37.0004, -114.05 36.95777,...\n2020\nNevada\n11/3/2020\nDonald Trump\n44.32982\n44.36094\n\n\n\n\n\n\n\n\ngeo_states_biden.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\ncycle\nstate\nmodeldate\ncandidate_name\npct_estimate\npct_trend_adjusted\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n2020\nMaine\n11/3/2020\nJoseph R. Biden Jr.\n53.31518\n53.32106\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.6065, -70.82374 4...\n2020\nMassachusetts\n11/3/2020\nJoseph R. Biden Jr.\n64.36328\n64.62505\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11578, -88.67563 ...\n2020\nMichigan\n11/3/2020\nJoseph R. Biden Jr.\n51.17806\n51.15482\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.0577 44.99743, -104.25014 44.99...\n2020\nMontana\n11/3/2020\nJoseph R. Biden Jr.\n45.34418\n45.36695\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.0506 37.0004, -114.05 36.95777,...\n2020\nNevada\n11/3/2020\nJoseph R. Biden Jr.\n49.62386\n49.65657\n\n\n\n\n\n\n\nJoe Biden is clearly winning. Can we make it look like he is not?",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lab: Choropleth Maps</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_3-Choropleths.html#data-visualisation",
    "href": "content/labs/Lab_6/IM939_Lab_6_3-Choropleths.html#data-visualisation",
    "title": "28  Lab: Choropleth Maps",
    "section": "28.2 Data visualisation",
    "text": "28.2 Data visualisation\nWe can plot this specifying the feature to use for our colour.\n\nalt.Chart(geo_states_trump, title='Poll estimate for Donald Trump on 11/3/2020').mark_geoshape().encode(\n    color='pct_estimate',\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\n\n\nFigure 28.1\n\n\n\n\n\n28.2.1 Binning\nTo smooth out any differences we can bin our data.\nIn the case below, we will be binning based on a single value (step):\n\nalt.Chart(geo_states_trump, title='Poll estimate for Donald Trump on 11/3/2020').mark_geoshape().encode(\n    alt.Color('pct_estimate', bin=alt.Bin(step=35)),\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nHow would you interpret the plot above? What would change if we change the value of the step?\n\n\nWhat about if we increase the binstep so we have more bins?\n\nalt.Chart(geo_states_trump, title='Poll estimate for Donald Trump on 11/3/2020').mark_geoshape().encode(\n    alt.Color('pct_estimate', bin=alt.Bin(step=5)),\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nTry different step sizes for the bins and consider how bins can shape our interpretation of the data. What would happen if plots with different bin sizes were placed side to side?\n\n\nTo add further confusion, what happens when we log scale the data?\n\nalt.Chart(geo_states_trump, title='Poll estimate for Donald Trump on 11/3/2020').mark_geoshape().encode(\n    alt.Color('pct_estimate', bin=alt.Bin(step=5), scale=alt.Scale(type='log')),\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\nvs\n\nalt.Chart(geo_states_biden, title='Poll estimate for Joe Biden on 11/3/2020').mark_geoshape().encode(\n    alt.Color('pct_estimate', bin=alt.Bin(step=5), scale=alt.Scale(type='log')),\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\nWhat is happening here?!?!\n\n\n28.2.2 Colour palettes\nNext up, what about the colours we use and the range of values assigned to each color? Code inspired by/taken from here.\n\nalt.Chart(geo_states_trump, title='Poll estimate for Donal Trump on 11/3/2020').mark_geoshape().encode(\n    alt.Color('pct_estimate',\n    scale=alt.Scale(type=\"linear\",\n              domain=[10, 40, 50, 55, 60, 61, 62],\n                          range=[\"#414487\",\"#414487\",\n                                 \"#355f8d\",\"#355f8d\",\n                                 \"#2a788e\",\n                                 \"#fde725\",\"#fde725\"])),\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\nCompare that with\n\nalt.Chart(geo_states_trump, title='Poll estimate for Donald Trump on 11/3/2020').mark_geoshape().encode(\n    alt.Color('pct_estimate',\n    scale=alt.Scale(type=\"linear\",\n              domain=[10, 20, 30, 35, 68, 70, 100],\n                          range=[\"#414487\",\"#414487\",\n                                 \"#7ad151\",\"#7ad151\",\n                                 \"#bddf26\",\n                                 \"#fde725\",\"#fde725\"])),\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\n\n\n28.2.3 Legends\nMy goodness! So what have we played around with?\n\nTransforming our scale using log\nBinning our data to smooth out variances\nAltering our colour scheme and the ranges for each colour\n\n… what about if we remove the legend?\n\nalt.Chart(geo_states_trump, title='Poll estimate for Donald Trump on 11/3/2020').mark_geoshape().encode(\n    alt.Color('pct_estimate',\n    scale=alt.Scale(type=\"linear\",\n              domain=[10, 20, 30, 35, 68, 70, 100],\n                          range=[\"#414487\",\"#414487\",\n                                 \"#7ad151\",\"#7ad151\",\n                                 \"#bddf26\",\n                                 \"#fde725\",\"#fde725\"]),\n                                 legend=None),\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\nGood luck trying to interpret that. Though we often see maps without legends and with questionable colour schemes on TV.\n\n\n\n\n\n\nFood for thought\n\n\n\nHow do you think choropleths should be displayed? What information does a use need to understand the message communicated in these plots?",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Lab: Choropleth Maps</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html",
    "href": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html",
    "title": "29  Exercise: Data visualisation",
    "section": "",
    "text": "29.1 Setup\nLoad any libraries and datasets needed for your visualisation.",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Exercise: Data visualisation</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#data-wrangling",
    "href": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#data-wrangling",
    "title": "29  Exercise: Data visualisation",
    "section": "29.2 Data wrangling",
    "text": "29.2 Data wrangling\nSubset or clean your data",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Exercise: Data visualisation</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#analysis",
    "href": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#analysis",
    "title": "29  Exercise: Data visualisation",
    "section": "29.3 Analysis",
    "text": "29.3 Analysis\nCarry out an analysis if required. E.g., are you running a PCA or other dimension reduction, or a linear regression to plot?",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Exercise: Data visualisation</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#visualisation",
    "href": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#visualisation",
    "title": "29  Exercise: Data visualisation",
    "section": "29.4 Visualisation",
    "text": "29.4 Visualisation\nCreate a visualisation which clearly show the trend you would like to show in your data.",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Exercise: Data visualisation</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#discussion",
    "href": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#discussion",
    "title": "29  Exercise: Data visualisation",
    "section": "29.5 Discussion",
    "text": "29.5 Discussion\nWhy did you choose this visualisation? Do you think other will clearly see the trend you have identified?\nCreate a visualisation of this trend which you think will mislead the user?\nHow do you think this will mislead the user?",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Exercise: Data visualisation</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html",
    "href": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html",
    "title": "30  IM939 - Lab 6 Part 5 Simpson’s Paradox",
    "section": "",
    "text": "30.1 1 Data\nHere are the key information from the dataset documentation file (every time I use “” below it is a cite from the dataset file). ### Abstract: “The State of California Department of Developmental Services (DDS) is responsible for allocating funds that support over 250,000 developmentally-disabled residents (e.g., intellectual disability, cerebral palsy, autism, etc.), called here also consumers. The dataset represents a sample of 1,000 of these consumers. Biographical characteristics and expenditure data (i.e., the dollar amount the State spends on each consumer in supporting these individuals and their families) are included in the data set for each consumer.",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>IM939 - Lab 6 Part 5 Simpson's Paradox</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#data",
    "href": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#data",
    "title": "30  IM939 - Lab 6 Part 5 Simpson’s Paradox",
    "section": "",
    "text": "30.1.1 Source:\nThe data set originates from DDS’s “Client Master File.” In order to remain in compliance with California State Legislation, the data have been altered to protect the rights and privacy of specific individual consumers. The data set is designed to represent a sample of 1,000 DDS consumers.\n\n\n30.1.2 Variable Descriptions:\nA header line contains the name of the variables. There are no missing values.\nId: 5-digit, unique identification code for each consumer (similar to a social security number)\nAge Cohort: Binned age variable represented as six age cohorts (0-5, 6-12, 13-17, 18-21, 22-50, and 51+)\nAge: Unbinned age variable\nGender: Male or Female\nExpenditures: Dollar amount of annual expenditures spent on each consumer\nEthnicity: Eight ethnic groups (American Indian, Asian, Black, Hispanic, Multi-race, Native Hawaiian, Other, and White non-Hispanic).\n\n\n30.1.3 Research problem\nThe data set and case study are based on a real-life scenario where there was a claim of discrimination based on ethnicity. The exercise highlights the importance of performing rigorous statistical analysis and how data interpretations can accurately inform or misguide decision makers.” (Taylor, Mickel 2014)",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>IM939 - Lab 6 Part 5 Simpson's Paradox</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#reading-the-dataset",
    "href": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#reading-the-dataset",
    "title": "30  IM939 - Lab 6 Part 5 Simpson’s Paradox",
    "section": "30.2 2 Reading the dataset",
    "text": "30.2 2 Reading the dataset\nYou should know the Pandas library already from the lab 1 with James. Here we are going to use it to explore the data and for pivot tables. In the folder you downloaded from the Moodle you have a dataset called ‘Lab 6 - Paradox Dataset’.\n\nimport pandas as pd\ndf = pd.read_excel('data/Paradox_Dataset.xlsx')\n\nA reminder: anything with a pd. prefix comes from pandas. This is particulary useful for preventing a module from overwriting inbuilt Python functionality.\nLet’s have a look at our dataset\n\ndf\n\n\n\n\n\n\n\n\nId\nAgeCohort\nAge\nGender\nExpenditures\nEthnicity\n\n\n\n\n0\n10210\n13-17\n17\nFemale\n2113\nWhite not Hispanic\n\n\n1\n10409\n22-50\n37\nMale\n41924\nWhite not Hispanic\n\n\n2\n10486\n0-5\n3\nMale\n1454\nHispanic\n\n\n3\n10538\n18-21\n19\nFemale\n6400\nHispanic\n\n\n4\n10568\n13-17\n13\nMale\n4412\nWhite not Hispanic\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n99622\n51 +\n86\nFemale\n57055\nWhite not Hispanic\n\n\n996\n99715\n18-21\n20\nMale\n7494\nHispanic\n\n\n997\n99718\n13-17\n17\nFemale\n3673\nMulti Race\n\n\n998\n99791\n6-12\n10\nMale\n3638\nHispanic\n\n\n999\n99898\n22-50\n23\nMale\n26702\nWhite not Hispanic\n\n\n\n\n1000 rows × 6 columns\n\n\n\nWe have 6 columns (variables) in 1000 rows. Let’s see what type of object is our dataset and what types of objects are in the dataset.\n\ntype(df)\n\npandas.core.frame.DataFrame",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>IM939 - Lab 6 Part 5 Simpson's Paradox</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#exploring-data",
    "href": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#exploring-data",
    "title": "30  IM939 - Lab 6 Part 5 Simpson’s Paradox",
    "section": "30.3 3 Exploring data",
    "text": "30.3 3 Exploring data\n\n30.3.1 Missing values\nLet’s check if we have any missing data\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 6 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   Id            1000 non-null   int64 \n 1   AgeCohort     1000 non-null   object\n 2   Age           1000 non-null   int64 \n 3   Gender        1000 non-null   object\n 4   Expenditures  1000 non-null   int64 \n 5   Ethnicity     1000 non-null   object\ndtypes: int64(3), object(3)\nmemory usage: 47.0+ KB\n\n\nThe above tables shows that we have 1000 observations for each of 6 columns.\nLet’s see if there are any unexpected values.\n\nimport numpy as np\nnp.unique(df.AgeCohort)\n\narray([' 0-5', ' 51 +', '13-17', '18-21', '22-50', '6-12'], dtype=object)\n\n\n\nnp.unique(df.Age)\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 48, 51, 52, 53, 54,\n       55, 56, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73,\n       74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 94,\n       95])\n\n\n\nnp.unique(df.Gender)\n\narray(['Female', 'Male'], dtype=object)\n\n\n\nnp.unique(df.Ethnicity)\n\narray(['American Indian', 'Asian', 'Black', 'Hispanic', 'Multi Race',\n       'Native Hawaiian', 'Other', 'White not Hispanic'], dtype=object)\n\n\nThere aren’t any unexpected values in neither of these 4 variables. We didn’t run this command for Expenditures on purpose, as this would return us too many values. An easier way to check this variable would be just a boxplot.\n\ndf.boxplot(column=['Age'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\ndf.boxplot(column=['Expenditures'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nLet’s see a summary of data types we have here.\n\n\n30.3.2 Data types\n\ndf.dtypes\n\nId               int64\nAgeCohort       object\nAge              int64\nGender          object\nExpenditures     int64\nEthnicity       object\ndtype: object\n\n\nWe are creating a new categorical column cat_AgeCohort that would make our work a bit easier later. You can read more here\n\ndf['cat_AgeCohort'] = pd.Categorical(df['AgeCohort'], \n                                     ordered=True, \n                                     categories=['0-5', '6-12', '13-17', '18-21', '22-50', '51 +'])\n\nHere int64 mean ‘a 64-bit integer’ and ‘object’ are strings. This gives you also a hint they are different types of variables. The ‘bit’ refers to how long and precise the number is. Pandas uses data types from numpy (pandas documentation, numpy documentation). In our dataset three variables are numeric: Id, age are ordinal variables, Expenditures is a scale variable. AgeCohort is categorical and Gender and Ethnicity are nominal.\nFor that reason ‘data.describe’ will bring us a summary of numeric variables only.\n\ndf.describe()\n\n\n\n\n\n\n\n\nId\nAge\nExpenditures\n\n\n\n\ncount\n1000.000000\n1000.000000\n1000.000000\n\n\nmean\n54662.846000\n22.800000\n18065.786000\n\n\nstd\n25643.673401\n18.462038\n19542.830884\n\n\nmin\n10210.000000\n0.000000\n222.000000\n\n\n25%\n31808.750000\n12.000000\n2898.750000\n\n\n50%\n55384.500000\n18.000000\n7026.000000\n\n\n75%\n76134.750000\n26.000000\n37712.750000\n\n\nmax\n99898.000000\n95.000000\n75098.000000\n\n\n\n\n\n\n\nIt doesn’t make sense to plot not numeric variables or ids. That’s why we are going to just plot age and expenditures.\n\ndf.plot(x = 'Age', y = 'Expenditures', kind='scatter')\n\n&lt;Axes: xlabel='Age', ylabel='Expenditures'&gt;\n\n\n\n\n\n\n\n\n\nThe pattern of data is very interesting, expecially around x-values of ca. 25. The research paper can bring us more clarification.\n\n\n30.3.3 Age\nThe crucial factor in this case study is age: “As consumers get older, their financial needs increase as they move out of their parent’s home, etc. Therefore, it is expected that expenditures for older consumers will be higher than for the younger consumers”\nIn the dataset we have two age variables that both refer to the same information - age of consumers. They are saved as two distinct data types: binned ‘AgeCohort’ and unbinned ‘Age’.\nAge categories If you look at the binned one you will notice that the categories are somewhat interesting:\n\ndf[['Age']].plot(kind='hist', ec='black')\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\n\n\n\n\ndf[\"AgeCohort\"].describe() #we will receive the output for the categorical variable \"AgeCohort\"\ndf['cat_AgeCohort'].describe()\n\ncount       812\nunique        4\ntop       22-50\nfreq        226\nName: cat_AgeCohort, dtype: object\n\n\nHere we will run a bar plot of age categories.\n\ndf['cat_AgeCohort'].value_counts().plot(kind=\"bar\")\n\n&lt;Axes: xlabel='cat_AgeCohort'&gt;\n\n\n\n\n\n\n\n\n\nThe default order of plot elements is ‘value count’. For the age variable it might be more useful to look at the order chronologically.\n\n#using sns.countplot from seaborn we will plot AgeCohort\n#the order in plotting this variable is really crucial, we want to have sorted by age categories\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#here is without sorting / ordering\n#sns.countplot(x=\"AgeCohort\", data=df)\n\n#here we plot the variable with sorting\nsns.countplot(x=\"cat_AgeCohort\", data=df)\n\n#You can try playing with the commands below too:\n#sns.countplot(x=\"AgeCohort\", data=df, order=df['AgeCohort'].value_counts().index)\n#sns.countplot(x=\"AgeCohort\", data=df, order=['0-5', '6-12', '13-17', '18-21', '22-50','51+'])\n\n&lt;Axes: xlabel='cat_AgeCohort', ylabel='count'&gt;\n\n\n\n\n\n\n\n\n\nWhy would the data be binned in such “uneven” categories like ‘0-5 years’, ‘6-12’ and ‘22-50’? Instead of even categories e.g. ‘0-10’, ‘11-20’, ‘21-30’ etc. or every 5 years ‘0-5’, ‘6-10’ etc.?\nHere the age cohorts were allocated based on the theory, rather than based on data (this way we would have even number of people in each category) or based on logical age categories, e.g. every 5 or 10 years.\nThe authors explain: “The cohorts are established based on the amount of financial support typically required during a particular life phase (…) The 0-5 cohort (preschool age) has the fewest needs and requires the least amount of funding (…) Those in the 51+ cohort have the most needs and require the most amount of funding”. You can read in more details in the paper.",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>IM939 - Lab 6 Part 5 Simpson's Paradox</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#exploratory-analysis",
    "href": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#exploratory-analysis",
    "title": "30  IM939 - Lab 6 Part 5 Simpson’s Paradox",
    "section": "30.4 4 Exploratory analysis",
    "text": "30.4 4 Exploratory analysis\nThe research question is: are any demographics discriminated in distributions of the funds?\nFollowing the authors: “Discrimination exists in this case study if the amount of expenditures for a typical person in a group of consumers that share a common attribute (e.g., gender, ethnicity, etc.) is significantly different when compared to a typical person in another group. For example, discrimination based on gender would occur if the expenditures for a typical female are less than the amount for a typical male.”\nWe are going to examine the data using plots for categorical data and pivot tables (cross-tables) with means. “Pivot table reports are particularly useful in narrowing down larger data sets or analyzing relationships between data points.” Pivot tables will help you understand what is Simpson’s Paradox.\n\n30.4.1 Age x expenditures\nLet’s see how expenditures are distributed across age groups.\nWe are going to use a swarm plot which I believe works well here to notice the paradox and “the points are adjusted (only along the categorical axis) so that they don’t overlap. This gives a better representation of the distribution of values, but it does not scale well to large numbers of observations. A swarm plot can be drawn on its own, but it is also a good complement to a box or violin plot in cases where you want to show all observations along with some representation of the underlying distribution.” Read more here\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.catplot(x=\"AgeCohort\", y=\"Expenditures\", kind=\"swarm\", data=df)\n#you can also do a boxplot if you change kind=\"box\"\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 83.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 35.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 76.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 58.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 14.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 84.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 85.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 42.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 80.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 64.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 21.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 86.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n\n\n\n\n30.4.2 Ethnicity\nEthnicity could be another discriminating factor. Let’s check this here too by plotting expenditures by ethnicity.\nThese groups reflect the demographic profile of the State of California.\n\nsns.catplot(x=\"Ethnicity\", y=\"Expenditures\", kind=\"swarm\", data=df)\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 50.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 64.6% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 10.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 11.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 25.6% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 58.6% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 69.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 18.6% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 23.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 34.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n\n\n\n\n30.4.3 Gender\nGender could have been another discriminating factor (as gender based discrimination is also very common). It is not the case here. See below plots to confirm these. We are plotting expenditures by gender.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#sns.catplot(x=\"Gender\", y=\"Expenditures\", kind=\"swarm\", data=df)\n#you can create even a nicer plot than for ethnicity, using tips here https://seaborn.pydata.org/tutorial/categorical.html\n#It's a combination of swarmplot and violin plot to show each observation along with a summary of the distribution\n\ng = sns.catplot(x=\"Gender\", y=\"Expenditures\", kind=\"violin\", inner=None, data=df)\nsns.swarmplot(x=\"Gender\", y=\"Expenditures\", color=\"k\", size=3, data=df, ax=g.ax)\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 12.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 10.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n&lt;Axes: xlabel='Gender', ylabel='Expenditures'&gt;\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 13.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3370: UserWarning: 10.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n\n\n\n\n30.4.4 Mean Expenditures\nThis was a quick visual analysis. Let’s check means to see how it looks like by age, ethnicity and gender. Why would it be also good to check medians here?\n\nimport pandas as pd\nimport numpy as np\n\n#By default the aggreggate function is mean\n\nnp.round(df.pivot_table(index=['Ethnicity'], values=['Expenditures']), 2)\n\n\n\n\n\n\n\n\nExpenditures\n\n\nEthnicity\n\n\n\n\n\nAmerican Indian\n36438.25\n\n\nAsian\n18392.37\n\n\nBlack\n20884.59\n\n\nHispanic\n11065.57\n\n\nMulti Race\n4456.73\n\n\nNative Hawaiian\n42782.33\n\n\nOther\n3316.50\n\n\nWhite not Hispanic\n24697.55\n\n\n\n\n\n\n\n\nnp.round(df.pivot_table(index=['Gender'], values=['Expenditures']), 2)\n\n\n\n\n\n\n\n\nExpenditures\n\n\nGender\n\n\n\n\n\nFemale\n18129.61\n\n\nMale\n18001.20\n\n\n\n\n\n\n\n\nnp.round(df.pivot_table(index=['cat_AgeCohort'], values=['Expenditures']), 2)\n\n\n\n\n\n\n\n\nExpenditures\n\n\ncat_AgeCohort\n\n\n\n\n\n6-12\n2226.86\n\n\n13-17\n3922.61\n\n\n18-21\n9888.54\n\n\n22-50\n40209.28\n\n\n\n\n\n\n\nWhat do these tables tell us? There is much discrepnacy in average results for ethnicity and age cohort. If we look at gender - there aren’t many differences.\nPlease remember that in this case study “the needs for consumers increase as they become older which results in higher expenditures”. This would explain age discrepancies a bit, but what about ethnicity?",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>IM939 - Lab 6 Part 5 Simpson's Paradox</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#explanation",
    "href": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#explanation",
    "title": "30  IM939 - Lab 6 Part 5 Simpson’s Paradox",
    "section": "32.1 Explanation",
    "text": "32.1 Explanation\n“This exercise is based on a real-life case in California. The situation involved an alleged case of discrimination privileging White non-Hispanics over Hispanics in the allocation of funds to over 250,000 developmentally-disabled California residents.\nA number of years ago, an allegation of discrimination was made and supported by a univariate analysis that examined average annual expenditures on consumers by ethnicity. The analysis revealed that the average annual expenditures on Hispanic consumers was approximately one-third (⅓) of the average expenditures on White non-Hispanic consumers. (…) A bivariate analysis examining ethnicity and age (divided into six age cohorts) revealed that ethnic discrimination did not exist. Moreover, in all but one of the age cohorts, the trend reversed where the average annual expenditures on White non-Hispanic consumers were less than the expenditures on Hispanic consumers.”(Taylor, Mickel 2014)\nWhen running the simple table with aggregated data, the discrimination in this case appared evident. After running a few more detailed tables, it appears to be no evidence of discrimination based on this sample and the variables collected.",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>IM939 - Lab 6 Part 5 Simpson's Paradox</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#takeaways",
    "href": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#takeaways",
    "title": "30  IM939 - Lab 6 Part 5 Simpson’s Paradox",
    "section": "32.2 Takeaways",
    "text": "32.2 Takeaways\nThe example above concerns a crucial topic of discrimination. As you can see, data and statistics alone won’t give us the anwser. First results might give us a confusing result. Critical thinking is essential when working with data, in order to account for reasons not evident at the first sight. The authors remind us the following: 1) “outcome of important decisions (such as discrimination claims) are often heavily influenced by statistics and how an incomplete analysis may lead to poor decision making” 2) “importance of identifying and analyzing all sources of specific variation (i.e., potential influential factors) in statistical analyses”. This is something we already discussed in previous weeks, but it is never enough to stress it out”\n\n32.2.1 *Additional Links\nSome links regarding categorical data in Python for those interested:\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html#description\nhttps://pandas.pydata.org/pandas-docs/version/0.23.1/generated/pandas.DataFrame.plot.bar.html\nhttps://seaborn.pydata.org/tutorial/categorical.html\nhttps://seaborn.pydata.org/generated/seaborn.countplot.html",
    "crumbs": [
      "Recognising and avoiding traps",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>IM939 - Lab 6 Part 5 Simpson's Paradox</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-07.html",
    "href": "content/sessions/session-07.html",
    "title": "31  Introduction",
    "section": "",
    "text": "31.1 Reading lists & Resources",
    "crumbs": [
      "Data Science & Society",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-07.html#reading-lists-resources",
    "href": "content/sessions/session-07.html#reading-lists-resources",
    "title": "31  Introduction",
    "section": "",
    "text": "31.1.1 Required reading\n\nMichele Willson (2016): Algorithms (and the) everyday, Information, Communication & Society [pdf]",
    "crumbs": [
      "Data Science & Society",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part1.html",
    "href": "content/labs/Lab_7/IM939_Lab7-Part1.html",
    "title": "32  Lab: Hate crimes",
    "section": "",
    "text": "32.1 Datasets\nThis week we will work with the following datasets:",
    "crumbs": [
      "Data Science & Society",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Lab: Hate crimes</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part1.html#datasets",
    "href": "content/labs/Lab_7/IM939_Lab7-Part1.html#datasets",
    "title": "32  Lab: Hate crimes",
    "section": "",
    "text": "Hate crimes\nWorld Bank Indicators Dataset\nOffice for National Statistics (ONS): Gender Pay Gap\n\n\n32.1.1 Further datasets\n\nOECD Poverty gap\nPoverty & Equity Data Portal: From Organisation for Economic Co-operation and Development (OECD) or from the WorldBank\nNHS: multiple files. The NHS inequality challenge https://www.nuffieldtrust.org.uk/project/nhs-visual-data-challenge \n\nHealth state life expectancies by Index of Multiple Deprivation (IMD 2015 and IMD 2019): England, all ages multiple publications\n\n\n\n\n32.1.2 Additional Readings\n\nIndicators - critical reviews: The Poverty of Statistics and the Statistics of Poverty: https://www.tandfonline.com/doi/full/10.1080/01436590903321844?src=recsys\nIndicators in global health: arguments: indicators are usually comprehensible to a small group of experts. Why use indicators then? “Because indicators used in global HIV finance offer openings for engagement to promote accountability (…) some indicators and data truly are better than others, and as they were all created by humans, they all can be deconstructed and remade in other forms” Davis, S. (2020). The Uncounted: Politics of Data in Global Health, Cambridge. doi:10.1017/9781108649544",
    "crumbs": [
      "Data Science & Society",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Lab: Hate crimes</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part1.html#hate-crimes",
    "href": "content/labs/Lab_7/IM939_Lab7-Part1.html#hate-crimes",
    "title": "32  Lab: Hate crimes",
    "section": "32.2 Hate Crimes",
    "text": "32.2 Hate Crimes\nIn this notebook we will be using the Hate Crimes dataset from Fivethirtyeight, which was used in the story Higher Rates Of Hate Crimes Are Tied To Income Inequality.\n\n32.2.1 Variables:\n\n\n\n\n\n\n\nHeader\nDefinition\n\n\n\n\nNAME\nState name\n\n\nmedian_household_income\nMedian household income, 2016\n\n\nshare_unemployed_seasonal\nShare of the population that is unemployed (seasonally adjusted), Sept. 2016\n\n\nshare_population_in_metro_areas\nShare of the population that lives in metropolitan areas, 2015\n\n\nshare_population_with_high_school_degree\nShare of adults 25 and older with a high-school degree, 2009\n\n\nshare_non_citizen\nShare of the population that are not U.S. citizens, 2015\n\n\nshare_white_poverty\nShare of white residents who are living in poverty, 2015\n\n\ngini_index\nGini Index, 2015\n\n\nshare_non_white\nShare of the population that is not white, 2015\n\n\nshare_voters_voted_trump\nShare of 2016 U.S. presidential voters who voted for Donald Trump\n\n\nhate_crimes_per_100k_splc\nHate crimes per 100,000 population, Southern Poverty Law Center, Nov. 9-18, 2016\n\n\navg_hatecrimes_per_100k_fbi\nAverage annual hate crimes per 100,000 population, FBI, 2010-2015\n\n\n\n\n\nGini Index: measures income inequality. Gini Index values can range between 0 and 1, where 0 indicates perfect equality and everyone has the same income, and 1 indicates perfect inequality. You can read more about Gini Index here: https://databank.worldbank.org/metadataglossary/world-development-indicators/series/SI.POV.GINI\n\n\n\nMap of income inequality Gini coefficients by country (%). Based on World Bank data for 2019. Source: Wikipedia",
    "crumbs": [
      "Data Science & Society",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Lab: Hate crimes</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part1.html#data-exploration",
    "href": "content/labs/Lab_7/IM939_Lab7-Part1.html#data-exploration",
    "title": "32  Lab: Hate crimes",
    "section": "32.3 Data exploration",
    "text": "32.3 Data exploration\n\n\n\n\n\n\nSelect the IM939 environment before you begin\n\n\n\nWe will again be using Altair and Geopandas this week. If you are using the course’s virtual environment, this should be installed for you the first time you set up your environment for the module. Refer to Appendix A for instructions on how to set up your environment.\n\n\n\nimport pandas as pd\ndf = pd.read_excel('data/hate_Crimes_v2.xlsx')\n\nA reminder: anything with a pd. prefix comes from pandas (since pd is the alias we have created for the pandas library). This is particulary useful for preventing a module from overwriting inbuilt Python functionality.\nLet’s have a look at our dataset\n\n# Retrieve the last ten rows of the df dataframe\ndf.tail(10)\n\n\n\n\n\n\n\n\nNAME\nmedian_household_income\nshare_unemployed_seasonal\nshare_population_in_metro_areas\nshare_population_with_high_school_degree\nshare_non_citizen\nshare_white_poverty\ngini_index\nshare_non_white\nshare_voters_voted_trump\nhate_crimes_per_100k_splc\navg_hatecrimes_per_100k_fbi\n\n\n\n\n41\nSouth Dakota\n53053\n0.035\n0.51\n0.899\nNaN\n0.08\n0.442\n0.17\n0.62\n0.00\n3.30\n\n\n42\nTennessee\n43716\n0.057\n0.82\n0.831\n0.04\n0.13\n0.468\n0.27\n0.61\n0.19\n3.13\n\n\n43\nTexas\n53875\n0.042\n0.92\n0.799\n0.11\n0.08\n0.469\n0.56\n0.53\n0.21\n0.75\n\n\n44\nUtah\n63383\n0.036\n0.82\n0.904\n0.04\n0.08\n0.419\n0.19\n0.47\n0.13\n2.38\n\n\n45\nVermont\n60708\n0.037\n0.35\n0.910\n0.01\n0.10\n0.444\n0.06\n0.33\n0.32\n1.90\n\n\n46\nVirginia\n66155\n0.043\n0.89\n0.866\n0.06\n0.07\n0.459\n0.38\n0.45\n0.36\n1.72\n\n\n47\nWashington\n59068\n0.052\n0.86\n0.897\n0.08\n0.09\n0.441\n0.31\n0.38\n0.67\n3.81\n\n\n48\nWest Virginia\n39552\n0.073\n0.55\n0.828\n0.01\n0.14\n0.451\n0.07\n0.69\n0.32\n2.03\n\n\n49\nWisconsin\n58080\n0.043\n0.69\n0.898\n0.03\n0.09\n0.430\n0.22\n0.48\n0.22\n1.12\n\n\n50\nWyoming\n55690\n0.040\n0.31\n0.918\n0.02\n0.09\n0.423\n0.15\n0.70\n0.00\n0.26\n\n\n\n\n\n\n\n\n# Is df indeed a DataFrame, let's do a quick check\ntype(df)\n\npandas.core.frame.DataFrame\n\n\n\n# What about the data type (Dtype) of the columns in df. We better also be aware of these to help us understand about manipulating them effectively\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 51 entries, 0 to 50\nData columns (total 12 columns):\n #   Column                                    Non-Null Count  Dtype  \n---  ------                                    --------------  -----  \n 0   NAME                                      51 non-null     object \n 1   median_household_income                   51 non-null     int64  \n 2   share_unemployed_seasonal                 51 non-null     float64\n 3   share_population_in_metro_areas           51 non-null     float64\n 4   share_population_with_high_school_degree  51 non-null     float64\n 5   share_non_citizen                         48 non-null     float64\n 6   share_white_poverty                       51 non-null     float64\n 7   gini_index                                51 non-null     float64\n 8   share_non_white                           51 non-null     float64\n 9   share_voters_voted_trump                  51 non-null     float64\n 10  hate_crimes_per_100k_splc                 51 non-null     float64\n 11  avg_hatecrimes_per_100k_fbi               51 non-null     float64\ndtypes: float64(10), int64(1), object(1)\nmemory usage: 4.9+ KB\n\n\n\n32.3.1 Missing values\nLet’s explore the dataset\nThe above tables shows that we have some missing data for some of states, as did df.tail(10) earlier on. Let’s check again.\n\ndf.isna().sum()\n\nNAME                                        0\nmedian_household_income                     0\nshare_unemployed_seasonal                   0\nshare_population_in_metro_areas             0\nshare_population_with_high_school_degree    0\nshare_non_citizen                           3\nshare_white_poverty                         0\ngini_index                                  0\nshare_non_white                             0\nshare_voters_voted_trump                    0\nhate_crimes_per_100k_splc                   0\navg_hatecrimes_per_100k_fbi                 0\ndtype: int64\n\n\nHmmm, the column ‘share_non_citizen’ does indeed have some missing data. How about the column NAME (state names). Is that looking ok?\n\nimport numpy as np\nnp.unique(df.NAME)\n\narray(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n       'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n       'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n       'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n       'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n       'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n       'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n       'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee',\n       'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n       'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object)\n\n\nThere aren’t any unexpected values in ‘NAME’.\n\n# And how many states do we have the data for?\ncount_states = df['NAME'].nunique()\nprint(count_states)\n\n51\n\n\nOh…one extra state! Which one is it? And is it a state? Even if you don’t get into investigating this immediately, if you realise that this entry is a particularly interesting one down the line in your analysis, you may wish to dig deeper into the context!",
    "crumbs": [
      "Data Science & Society",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Lab: Hate crimes</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part1.html#mapping-hate-crime-across-the-usa",
    "href": "content/labs/Lab_7/IM939_Lab7-Part1.html#mapping-hate-crime-across-the-usa",
    "title": "32  Lab: Hate crimes",
    "section": "32.4 Mapping hate crime across the USA",
    "text": "32.4 Mapping hate crime across the USA\n\n#We need  the geospatial polygons of the states in America  \nimport geopandas as gpd \nimport pandas as pd\nimport altair as alt\n\n# Read geospatial data as geospatial data frame -gdf\ngdf = gpd.read_file('data/gz_2010_us_040_00_500k.json')\ngdf.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\n\n\n\n\n\n\n\n\n# Checking what type geo_states is...\ntype(gdf)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\nAs with the previous week, we have got a column called geometry that would allow us to project the data in a 2D map format.\n\n# Calling the Altair alias (alt) to help us create the map of USA - more technically speaking, \n# creating a Chart object using Altair with the following properties\nalt.Chart(gdf, title='US states').mark_geoshape().encode(\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\n\n# Add the data\n# You might recall that the df DataFrame and the geostates GeoDataFrame both have a NAME column\ngeo_states = gdf.merge(df, on='NAME')\n\ngeo_states.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\nmedian_household_income\nshare_unemployed_seasonal\nshare_population_in_metro_areas\nshare_population_with_high_school_degree\nshare_non_citizen\nshare_white_poverty\ngini_index\nshare_non_white\nshare_voters_voted_trump\nhate_crimes_per_100k_splc\navg_hatecrimes_per_100k_fbi\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n51710\n0.044\n0.54\n0.902\nNaN\n0.12\n0.437\n0.09\n0.45\n0.61\n2.62\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n63151\n0.046\n0.97\n0.890\n0.09\n0.08\n0.475\n0.27\n0.34\n0.63\n4.80\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n52005\n0.050\n0.87\n0.879\n0.04\n0.09\n0.451\n0.24\n0.48\n0.40\n3.20\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n51102\n0.041\n0.34\n0.908\n0.01\n0.10\n0.435\n0.10\n0.57\n0.49\n2.95\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\n49875\n0.067\n0.87\n0.839\n0.10\n0.08\n0.448\n0.50\n0.46\n0.14\n2.11\n\n\n\n\n\n\n\n\n# Let's take a look at the merged GeoDataFrame\ngeo_states.head(10)\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\nmedian_household_income\nshare_unemployed_seasonal\nshare_population_in_metro_areas\nshare_population_with_high_school_degree\nshare_non_citizen\nshare_white_poverty\ngini_index\nshare_non_white\nshare_voters_voted_trump\nhate_crimes_per_100k_splc\navg_hatecrimes_per_100k_fbi\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n51710\n0.044\n0.54\n0.902\nNaN\n0.12\n0.437\n0.09\n0.45\n0.61\n2.62\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n63151\n0.046\n0.97\n0.890\n0.09\n0.08\n0.475\n0.27\n0.34\n0.63\n4.80\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n52005\n0.050\n0.87\n0.879\n0.04\n0.09\n0.451\n0.24\n0.48\n0.40\n3.20\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n51102\n0.041\n0.34\n0.908\n0.01\n0.10\n0.435\n0.10\n0.57\n0.49\n2.95\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\n49875\n0.067\n0.87\n0.839\n0.10\n0.08\n0.448\n0.50\n0.46\n0.14\n2.11\n\n\n5\n0400000US34\n34\nNew Jersey\n\n7354.220\nPOLYGON ((-75.52684 39.65571, -75.52634 39.656...\n65243\n0.056\n1.00\n0.874\n0.11\n0.07\n0.464\n0.44\n0.42\n0.07\n4.41\n\n\n6\n0400000US36\n36\nNew York\n\n47126.399\nMULTIPOLYGON (((-71.94356 41.28668, -71.92680 ...\n54310\n0.051\n0.94\n0.847\n0.10\n0.10\n0.499\n0.42\n0.37\n0.35\n3.10\n\n\n7\n0400000US37\n37\nNorth Carolina\n\n48617.905\nMULTIPOLYGON (((-82.60288 36.03983, -82.60074 ...\n46784\n0.058\n0.76\n0.843\n0.05\n0.10\n0.464\n0.38\n0.51\n0.24\n1.26\n\n\n8\n0400000US39\n39\nOhio\n\n40860.694\nMULTIPOLYGON (((-82.81349 41.72347, -82.81049 ...\n49644\n0.045\n0.75\n0.876\n0.03\n0.10\n0.452\n0.21\n0.52\n0.19\n3.24\n\n\n9\n0400000US42\n42\nPennsylvania\n\n44742.703\nPOLYGON ((-75.41504 39.80179, -75.42804 39.809...\n55173\n0.053\n0.87\n0.879\n0.03\n0.09\n0.461\n0.24\n0.49\n0.28\n0.43\n\n\n\n\n\n\n\nLet’s start making our visualisations and see if we can spot any trends or patterns\n\n# Let's first check how hate crimes looked pre-election \nchart_pre_election = alt.Chart(geo_states, title='PRE-election Hate crime per 100k').mark_geoshape().encode(\n    color='avg_hatecrimes_per_100k_fbi',\n    tooltip=['NAME', 'avg_hatecrimes_per_100k_fbi']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\nchart_pre_election\n\n\n\n\n\n\n\nAs you can see above, Altair has chosen a color for each US state based on the range of values in the avg_hatecrimes_per_100k_fbi column. We have also created a tooltip, so hover over the map and check the crime rates. Which ones are particularly high? average? low?\nAlso, is there a dark spot between Virginia and Maryland - What’s happening there? Remember: context matters for data analysis!\n\n# Ok, what about the post election status?\nchart_post_election = alt.Chart(geo_states, title='POST-election Hate crime per 100k').mark_geoshape().encode(\n    color='hate_crimes_per_100k_splc',\n    tooltip=['NAME', 'hate_crimes_per_100k_splc']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\nchart_post_election\n\n\n\n\n\n\n\n\n# Perhaps we can arrange the maps side-by-side to compare better?\npre_and_post_map = chart_pre_election | chart_post_election\npre_and_post_map\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOh, what’s happening here? We better investigate:\n\nIdentify why the maps (particularly) one of them looks so different now. Go back to the original maps to check. Go back to the description of the variables to check. Also, visit the source of the article.\nOnce you have identified the issue, can you find a way to address the issue?\n\n\n\n\n32.4.1 Exploring data\n\nimport seaborn as sns\nsns.pairplot(data = df.iloc[:,1:])\n\n\n\n\n\n\n\n\nThe above plot may be hard to read without squinting our eyes (and take a bit longer to run on some devices), but it’s definitely worth a closer look if you are able to. Check the histograms along the diagonal - what do they show about the distribution of each variable. For example, what does the gini_index distribution tell us? With respect to the scatter plots, some are more random while others show likely positive or negative correlations. You may wish to investigate what’s happening! And, you might remember (as we also discussed in the video recordings this week), correlation != causation!\n\n# Let's take a look at the income range in the country\ndf.boxplot(column=['median_household_income'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\n# And the average hatecrimes based on FBI data next (also, average over what? check the Variables description again to remind you if need be)\ndf.boxplot(column=['avg_hatecrimes_per_100k_fbi'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nWe may want to drop some states (remove them). See more here.\nLet us drop Hawaii (which is one of the states outside mainland USA)\n\n# Let's find out the index value of the state in the DataFrame df\ndf[df.NAME == 'Hawaii']\n\n\n\n\n\n\n\n\nNAME\nmedian_household_income\nshare_unemployed_seasonal\nshare_population_in_metro_areas\nshare_population_with_high_school_degree\nshare_non_citizen\nshare_white_poverty\ngini_index\nshare_non_white\nshare_voters_voted_trump\nhate_crimes_per_100k_splc\navg_hatecrimes_per_100k_fbi\n\n\n\n\n11\nHawaii\n71223\n0.034\n0.76\n0.904\n0.08\n0.07\n0.433\n0.81\n0.3\n0.0\n0.0\n\n\n\n\n\n\n\n\n# Let's look at a summary of numeric columns prior to dropping Hawaii\ndf.describe()\n\n\n\n\n\n\n\n\nmedian_household_income\nshare_unemployed_seasonal\nshare_population_in_metro_areas\nshare_population_with_high_school_degree\nshare_non_citizen\nshare_white_poverty\ngini_index\nshare_non_white\nshare_voters_voted_trump\nhate_crimes_per_100k_splc\navg_hatecrimes_per_100k_fbi\n\n\n\n\ncount\n51.000000\n51.000000\n51.000000\n51.000000\n48.000000\n51.000000\n51.000000\n51.000000\n51.00000\n51.000000\n51.000000\n\n\nmean\n55223.607843\n0.049569\n0.750196\n0.869118\n0.054583\n0.091765\n0.453765\n0.315686\n0.49000\n0.275686\n2.316863\n\n\nstd\n9208.478170\n0.010698\n0.181587\n0.034073\n0.031077\n0.024715\n0.020891\n0.164915\n0.11871\n0.256252\n1.729228\n\n\nmin\n35521.000000\n0.028000\n0.310000\n0.799000\n0.010000\n0.040000\n0.419000\n0.060000\n0.04000\n0.000000\n0.000000\n\n\n25%\n48657.000000\n0.042000\n0.630000\n0.840500\n0.030000\n0.075000\n0.440000\n0.195000\n0.41500\n0.125000\n1.270000\n\n\n50%\n54916.000000\n0.051000\n0.790000\n0.874000\n0.045000\n0.090000\n0.454000\n0.280000\n0.49000\n0.210000\n1.930000\n\n\n75%\n60719.000000\n0.057500\n0.895000\n0.898000\n0.080000\n0.100000\n0.466500\n0.420000\n0.57500\n0.340000\n3.165000\n\n\nmax\n76165.000000\n0.073000\n1.000000\n0.918000\n0.130000\n0.170000\n0.532000\n0.810000\n0.70000\n1.520000\n10.950000\n\n\n\n\n\n\n\n\n# Let's now drop Hawaii\ndf = df.drop(df.index[11])\n\n\n# Now check again for the statistical summary\ndf.describe()\n\n\n\n\n\n\n\n\nmedian_household_income\nshare_unemployed_seasonal\nshare_population_in_metro_areas\nshare_population_with_high_school_degree\nshare_non_citizen\nshare_white_poverty\ngini_index\nshare_non_white\nshare_voters_voted_trump\nhate_crimes_per_100k_splc\navg_hatecrimes_per_100k_fbi\n\n\n\n\ncount\n50.000000\n50.000000\n50.000000\n50.000000\n47.000000\n50.000000\n50.000000\n50.000000\n50.00000\n50.000000\n50.000000\n\n\nmean\n54903.620000\n0.049880\n0.750000\n0.868420\n0.054043\n0.092200\n0.454180\n0.305800\n0.49380\n0.281200\n2.363200\n\n\nstd\n9010.994814\n0.010571\n0.183425\n0.034049\n0.031184\n0.024767\n0.020889\n0.150551\n0.11674\n0.255779\n1.714502\n\n\nmin\n35521.000000\n0.028000\n0.310000\n0.799000\n0.010000\n0.040000\n0.419000\n0.060000\n0.04000\n0.000000\n0.260000\n\n\n25%\n48358.500000\n0.042250\n0.630000\n0.839750\n0.030000\n0.080000\n0.440000\n0.192500\n0.42000\n0.130000\n1.290000\n\n\n50%\n54613.000000\n0.051000\n0.790000\n0.874000\n0.040000\n0.090000\n0.454500\n0.275000\n0.49500\n0.215000\n1.980000\n\n\n75%\n60652.750000\n0.057750\n0.897500\n0.897750\n0.080000\n0.100000\n0.466750\n0.420000\n0.57750\n0.345000\n3.182500\n\n\nmax\n76165.000000\n0.073000\n1.000000\n0.918000\n0.130000\n0.170000\n0.532000\n0.630000\n0.70000\n1.520000\n10.950000\n\n\n\n\n\n\n\nThere seems to be some changes.\n\n# Let's dig in deeper to the correlation between median household income and hatecrimes based on FBI data\ndf.plot(x = 'avg_hatecrimes_per_100k_fbi', y = 'median_household_income', kind='scatter')\n\n&lt;Axes: xlabel='avg_hatecrimes_per_100k_fbi', ylabel='median_household_income'&gt;\n\n\n\n\n\n\n\n\n\n\n# And the relationship between median household income and hatecrimes based on SPLC data\ndf.plot(x = 'hate_crimes_per_100k_splc', y = 'median_household_income', kind='scatter')\n\n&lt;Axes: xlabel='hate_crimes_per_100k_splc', ylabel='median_household_income'&gt;\n\n\n\n\n\n\n\n\n\nHmmm, there doesn’t appear to be a strong (linear!) correlation, but surely there is a cluster and some outliers! That’s our cue - let’s find out which states might be outliers by using the standard deviation function ‘std’.\n\ndf[df.hate_crimes_per_100k_splc &gt; (np.std(df.hate_crimes_per_100k_splc) * 2.5)]\n\n\n\n\n\n\n\n\nNAME\nmedian_household_income\nshare_unemployed_seasonal\nshare_population_in_metro_areas\nshare_population_with_high_school_degree\nshare_non_citizen\nshare_white_poverty\ngini_index\nshare_non_white\nshare_voters_voted_trump\nhate_crimes_per_100k_splc\navg_hatecrimes_per_100k_fbi\n\n\n\n\n8\nDistrict of Columbia\n68277\n0.067\n1.00\n0.871\n0.11\n0.04\n0.532\n0.63\n0.04\n1.52\n10.95\n\n\n37\nOregon\n58875\n0.062\n0.87\n0.891\n0.07\n0.10\n0.449\n0.26\n0.41\n0.83\n3.39\n\n\n47\nWashington\n59068\n0.052\n0.86\n0.897\n0.08\n0.09\n0.441\n0.31\n0.38\n0.67\n3.81\n\n\n\n\n\n\n\nRemember that we discussed about ‘context’ earlier when we got 51 states? If your investigation paid off there, you can make better sense of the outliers here.\n\n# Let's try to make the outliers more obvious\nimport matplotlib.pyplot as plt\noutliers_df = df[df.hate_crimes_per_100k_splc &gt; (np.std(df.hate_crimes_per_100k_splc) * 2.5)]\ndf.plot(x = 'hate_crimes_per_100k_splc', y = 'median_household_income', kind='scatter')\nplt.scatter(outliers_df.hate_crimes_per_100k_splc, outliers_df.median_household_income ,c='red')\n\n&lt;matplotlib.collections.PathCollection at 0x16d8c8990&gt;\n\n\n\n\n\n\n\n\n\n\n# Let's create a pivot table to focus on specific columns of interest\ndf_pivot = df.pivot_table(index=['NAME'], values=['hate_crimes_per_100k_splc', 'avg_hatecrimes_per_100k_fbi', 'median_household_income'])\ndf_pivot\n\n\n\n\n\n\n\n\navg_hatecrimes_per_100k_fbi\nhate_crimes_per_100k_splc\nmedian_household_income\n\n\nNAME\n\n\n\n\n\n\n\nAlabama\n1.80\n0.12\n42278.0\n\n\nAlaska\n1.65\n0.14\n67629.0\n\n\nArizona\n3.41\n0.22\n49254.0\n\n\nArkansas\n0.86\n0.06\n44922.0\n\n\nCalifornia\n2.39\n0.25\n60487.0\n\n\nColorado\n2.80\n0.39\n60940.0\n\n\nConnecticut\n3.77\n0.33\n70161.0\n\n\nDelaware\n1.46\n0.32\n57522.0\n\n\nDistrict of Columbia\n10.95\n1.52\n68277.0\n\n\nFlorida\n0.69\n0.18\n46140.0\n\n\nGeorgia\n0.41\n0.12\n49555.0\n\n\nIdaho\n1.89\n0.12\n53438.0\n\n\nIllinois\n1.04\n0.19\n54916.0\n\n\nIndiana\n1.75\n0.24\n48060.0\n\n\nIowa\n0.56\n0.45\n57810.0\n\n\nKansas\n2.14\n0.10\n53444.0\n\n\nKentucky\n4.20\n0.32\n42786.0\n\n\nLouisiana\n1.34\n0.10\n42406.0\n\n\nMaine\n2.62\n0.61\n51710.0\n\n\nMaryland\n1.32\n0.37\n76165.0\n\n\nMassachusetts\n4.80\n0.63\n63151.0\n\n\nMichigan\n3.20\n0.40\n52005.0\n\n\nMinnesota\n3.61\n0.62\n67244.0\n\n\nMississippi\n0.62\n0.06\n35521.0\n\n\nMissouri\n1.90\n0.18\n56630.0\n\n\nMontana\n2.95\n0.49\n51102.0\n\n\nNebraska\n2.68\n0.15\n56870.0\n\n\nNevada\n2.11\n0.14\n49875.0\n\n\nNew Hampshire\n2.10\n0.15\n73397.0\n\n\nNew Jersey\n4.41\n0.07\n65243.0\n\n\nNew Mexico\n1.88\n0.29\n46686.0\n\n\nNew York\n3.10\n0.35\n54310.0\n\n\nNorth Carolina\n1.26\n0.24\n46784.0\n\n\nNorth Dakota\n4.74\n0.00\n60730.0\n\n\nOhio\n3.24\n0.19\n49644.0\n\n\nOklahoma\n1.08\n0.13\n47199.0\n\n\nOregon\n3.39\n0.83\n58875.0\n\n\nPennsylvania\n0.43\n0.28\n55173.0\n\n\nRhode Island\n1.28\n0.09\n58633.0\n\n\nSouth Carolina\n1.93\n0.20\n44929.0\n\n\nSouth Dakota\n3.30\n0.00\n53053.0\n\n\nTennessee\n3.13\n0.19\n43716.0\n\n\nTexas\n0.75\n0.21\n53875.0\n\n\nUtah\n2.38\n0.13\n63383.0\n\n\nVermont\n1.90\n0.32\n60708.0\n\n\nVirginia\n1.72\n0.36\n66155.0\n\n\nWashington\n3.81\n0.67\n59068.0\n\n\nWest Virginia\n2.03\n0.32\n39552.0\n\n\nWisconsin\n1.12\n0.22\n58080.0\n\n\nWyoming\n0.26\n0.00\n55690.0\n\n\n\n\n\n\n\n\n# the pivot table seems sorted by state names, let's sort by FBI hate crime data instead \ndf_pivot.sort_values(by=['avg_hatecrimes_per_100k_fbi'], ascending=False)\n\n\n\n\n\n\n\n\navg_hatecrimes_per_100k_fbi\nhate_crimes_per_100k_splc\nmedian_household_income\n\n\nNAME\n\n\n\n\n\n\n\nDistrict of Columbia\n10.95\n1.52\n68277.0\n\n\nMassachusetts\n4.80\n0.63\n63151.0\n\n\nNorth Dakota\n4.74\n0.00\n60730.0\n\n\nNew Jersey\n4.41\n0.07\n65243.0\n\n\nKentucky\n4.20\n0.32\n42786.0\n\n\nWashington\n3.81\n0.67\n59068.0\n\n\nConnecticut\n3.77\n0.33\n70161.0\n\n\nMinnesota\n3.61\n0.62\n67244.0\n\n\nArizona\n3.41\n0.22\n49254.0\n\n\nOregon\n3.39\n0.83\n58875.0\n\n\nSouth Dakota\n3.30\n0.00\n53053.0\n\n\nOhio\n3.24\n0.19\n49644.0\n\n\nMichigan\n3.20\n0.40\n52005.0\n\n\nTennessee\n3.13\n0.19\n43716.0\n\n\nNew York\n3.10\n0.35\n54310.0\n\n\nMontana\n2.95\n0.49\n51102.0\n\n\nColorado\n2.80\n0.39\n60940.0\n\n\nNebraska\n2.68\n0.15\n56870.0\n\n\nMaine\n2.62\n0.61\n51710.0\n\n\nCalifornia\n2.39\n0.25\n60487.0\n\n\nUtah\n2.38\n0.13\n63383.0\n\n\nKansas\n2.14\n0.10\n53444.0\n\n\nNevada\n2.11\n0.14\n49875.0\n\n\nNew Hampshire\n2.10\n0.15\n73397.0\n\n\nWest Virginia\n2.03\n0.32\n39552.0\n\n\nSouth Carolina\n1.93\n0.20\n44929.0\n\n\nVermont\n1.90\n0.32\n60708.0\n\n\nMissouri\n1.90\n0.18\n56630.0\n\n\nIdaho\n1.89\n0.12\n53438.0\n\n\nNew Mexico\n1.88\n0.29\n46686.0\n\n\nAlabama\n1.80\n0.12\n42278.0\n\n\nIndiana\n1.75\n0.24\n48060.0\n\n\nVirginia\n1.72\n0.36\n66155.0\n\n\nAlaska\n1.65\n0.14\n67629.0\n\n\nDelaware\n1.46\n0.32\n57522.0\n\n\nLouisiana\n1.34\n0.10\n42406.0\n\n\nMaryland\n1.32\n0.37\n76165.0\n\n\nRhode Island\n1.28\n0.09\n58633.0\n\n\nNorth Carolina\n1.26\n0.24\n46784.0\n\n\nWisconsin\n1.12\n0.22\n58080.0\n\n\nOklahoma\n1.08\n0.13\n47199.0\n\n\nIllinois\n1.04\n0.19\n54916.0\n\n\nArkansas\n0.86\n0.06\n44922.0\n\n\nTexas\n0.75\n0.21\n53875.0\n\n\nFlorida\n0.69\n0.18\n46140.0\n\n\nMississippi\n0.62\n0.06\n35521.0\n\n\nIowa\n0.56\n0.45\n57810.0\n\n\nPennsylvania\n0.43\n0.28\n55173.0\n\n\nGeorgia\n0.41\n0.12\n49555.0\n\n\nWyoming\n0.26\n0.00\n55690.0\n\n\n\n\n\n\n\n\n# Let's standardise our data before we attempt further modelling using the data\nfrom sklearn import preprocessing\nimport numpy as np\n\n# Get the column names first\ndf_stand = df[['median_household_income','share_unemployed_seasonal', 'share_population_in_metro_areas'\n               , 'share_population_with_high_school_degree', 'share_non_citizen', 'share_white_poverty', 'gini_index'\n               , 'share_non_white', 'share_voters_voted_trump', 'hate_crimes_per_100k_splc', 'avg_hatecrimes_per_100k_fbi']]\nnames = df_stand.columns\n\n# Create the Scaler object for standardising the data\nscaler = preprocessing.StandardScaler()\n\n# Fit our data on the scaler object\ndf2 = scaler.fit_transform(df_stand)\n\n# check what type is df2\ntype(df2)\n\nnumpy.ndarray\n\n\n\n# Let's convert the numpy array into a DataFrame before further processing\ndf2 = pd.DataFrame(df2, columns=names)\ndf2.tail(10)\n\n\n\n\n\n\n\n\nmedian_household_income\nshare_unemployed_seasonal\nshare_population_in_metro_areas\nshare_population_with_high_school_degree\nshare_non_citizen\nshare_white_poverty\ngini_index\nshare_non_white\nshare_voters_voted_trump\nhate_crimes_per_100k_splc\navg_hatecrimes_per_100k_fbi\n\n\n\n\n40\n-0.207459\n-1.421951\n-1.321717\n0.907229\nNaN\n-0.497582\n-0.588999\n-0.911176\n1.092014\n-1.110547\n0.551945\n\n\n41\n-1.254157\n0.680396\n0.385501\n-1.110154\n-0.455183\n1.541689\n0.668306\n-0.240207\n1.005484\n-0.360177\n0.451784\n\n\n42\n-0.115311\n-0.753023\n0.936216\n-2.059511\n1.813836\n-0.497582\n0.716664\n1.705604\n0.313240\n-0.281191\n-0.950467\n\n\n43\n0.950557\n-1.326390\n0.385501\n1.055566\n-0.455183\n-0.497582\n-1.701230\n-0.776982\n-0.205942\n-0.597136\n0.009898\n\n\n44\n0.650684\n-1.230829\n-2.202862\n1.233571\n-1.427620\n0.318126\n-0.492283\n-1.649243\n-1.417369\n0.153233\n-0.272909\n\n\n45\n1.261305\n-0.657461\n0.771002\n-0.071795\n0.193108\n-0.905436\n0.233085\n0.497859\n-0.379003\n0.311206\n-0.378961\n\n\n46\n0.466836\n0.202590\n0.605787\n0.847894\n0.841399\n-0.089728\n-0.637357\n0.028181\n-0.984716\n1.535493\n0.852428\n\n\n47\n-1.720951\n2.209376\n-1.101431\n-1.199157\n-1.427620\n1.949543\n-0.153778\n-1.582146\n1.697727\n0.153233\n-0.196315\n\n\n48\n0.356079\n-0.657461\n-0.330429\n0.877562\n-0.779329\n-0.089728\n-1.169293\n-0.575692\n-0.119412\n-0.241698\n-0.732470\n\n\n49\n0.088155\n-0.944145\n-2.423149\n1.470910\n-1.103475\n-0.089728\n-1.507798\n-1.045370\n1.784258\n-1.110547\n-1.239166\n\n\n\n\n\n\n\n\n# Now that our data has been standardised, let's look at the distribution across all columns\nax = sns.boxplot(data=df2, orient=\"h\")\n\n\n\n\n\n\n\n\n\nimport scipy.stats\n# Let's create a correlation matrix by computing the pairwise correlation of numerical columns, rounding correlation values to two places\ncorrMatrix = df2.corr(numeric_only=True).round(2)\nprint (corrMatrix)\n\n                                          median_household_income  \\\nmedian_household_income                                      1.00   \nshare_unemployed_seasonal                                   -0.34   \nshare_population_in_metro_areas                              0.29   \nshare_population_with_high_school_degree                     0.64   \nshare_non_citizen                                            0.28   \nshare_white_poverty                                         -0.82   \ngini_index                                                  -0.15   \nshare_non_white                                             -0.00   \nshare_voters_voted_trump                                    -0.57   \nhate_crimes_per_100k_splc                                    0.33   \navg_hatecrimes_per_100k_fbi                                  0.32   \n\n                                          share_unemployed_seasonal  \\\nmedian_household_income                                       -0.34   \nshare_unemployed_seasonal                                      1.00   \nshare_population_in_metro_areas                                0.37   \nshare_population_with_high_school_degree                      -0.61   \nshare_non_citizen                                              0.31   \nshare_white_poverty                                            0.19   \ngini_index                                                     0.53   \nshare_non_white                                                0.59   \nshare_voters_voted_trump                                      -0.21   \nhate_crimes_per_100k_splc                                      0.18   \navg_hatecrimes_per_100k_fbi                                    0.07   \n\n                                          share_population_in_metro_areas  \\\nmedian_household_income                                              0.29   \nshare_unemployed_seasonal                                            0.37   \nshare_population_in_metro_areas                                      1.00   \nshare_population_with_high_school_degree                            -0.27   \nshare_non_citizen                                                    0.75   \nshare_white_poverty                                                 -0.39   \ngini_index                                                           0.52   \nshare_non_white                                                      0.60   \nshare_voters_voted_trump                                            -0.58   \nhate_crimes_per_100k_splc                                            0.26   \navg_hatecrimes_per_100k_fbi                                          0.21   \n\n                                          share_population_with_high_school_degree  \\\nmedian_household_income                                                       0.64   \nshare_unemployed_seasonal                                                    -0.61   \nshare_population_in_metro_areas                                              -0.27   \nshare_population_with_high_school_degree                                      1.00   \nshare_non_citizen                                                            -0.30   \nshare_white_poverty                                                          -0.48   \ngini_index                                                                   -0.58   \nshare_non_white                                                              -0.56   \nshare_voters_voted_trump                                                     -0.13   \nhate_crimes_per_100k_splc                                                     0.21   \navg_hatecrimes_per_100k_fbi                                                   0.16   \n\n                                          share_non_citizen  \\\nmedian_household_income                                0.28   \nshare_unemployed_seasonal                              0.31   \nshare_population_in_metro_areas                        0.75   \nshare_population_with_high_school_degree              -0.30   \nshare_non_citizen                                      1.00   \nshare_white_poverty                                   -0.38   \ngini_index                                             0.51   \nshare_non_white                                        0.76   \nshare_voters_voted_trump                              -0.62   \nhate_crimes_per_100k_splc                              0.28   \navg_hatecrimes_per_100k_fbi                            0.30   \n\n                                          share_white_poverty  gini_index  \\\nmedian_household_income                                 -0.82       -0.15   \nshare_unemployed_seasonal                                0.19        0.53   \nshare_population_in_metro_areas                         -0.39        0.52   \nshare_population_with_high_school_degree                -0.48       -0.58   \nshare_non_citizen                                       -0.38        0.51   \nshare_white_poverty                                      1.00        0.01   \ngini_index                                               0.01        1.00   \nshare_non_white                                         -0.24        0.59   \nshare_voters_voted_trump                                 0.54       -0.46   \nhate_crimes_per_100k_splc                               -0.26        0.38   \navg_hatecrimes_per_100k_fbi                             -0.26        0.42   \n\n                                          share_non_white  \\\nmedian_household_income                             -0.00   \nshare_unemployed_seasonal                            0.59   \nshare_population_in_metro_areas                      0.60   \nshare_population_with_high_school_degree            -0.56   \nshare_non_citizen                                    0.76   \nshare_white_poverty                                 -0.24   \ngini_index                                           0.59   \nshare_non_white                                      1.00   \nshare_voters_voted_trump                            -0.44   \nhate_crimes_per_100k_splc                            0.12   \navg_hatecrimes_per_100k_fbi                          0.08   \n\n                                          share_voters_voted_trump  \\\nmedian_household_income                                      -0.57   \nshare_unemployed_seasonal                                    -0.21   \nshare_population_in_metro_areas                              -0.58   \nshare_population_with_high_school_degree                     -0.13   \nshare_non_citizen                                            -0.62   \nshare_white_poverty                                           0.54   \ngini_index                                                   -0.46   \nshare_non_white                                              -0.44   \nshare_voters_voted_trump                                      1.00   \nhate_crimes_per_100k_splc                                    -0.69   \navg_hatecrimes_per_100k_fbi                                  -0.50   \n\n                                          hate_crimes_per_100k_splc  \\\nmedian_household_income                                        0.33   \nshare_unemployed_seasonal                                      0.18   \nshare_population_in_metro_areas                                0.26   \nshare_population_with_high_school_degree                       0.21   \nshare_non_citizen                                              0.28   \nshare_white_poverty                                           -0.26   \ngini_index                                                     0.38   \nshare_non_white                                                0.12   \nshare_voters_voted_trump                                      -0.69   \nhate_crimes_per_100k_splc                                      1.00   \navg_hatecrimes_per_100k_fbi                                    0.68   \n\n                                          avg_hatecrimes_per_100k_fbi  \nmedian_household_income                                          0.32  \nshare_unemployed_seasonal                                        0.07  \nshare_population_in_metro_areas                                  0.21  \nshare_population_with_high_school_degree                         0.16  \nshare_non_citizen                                                0.30  \nshare_white_poverty                                             -0.26  \ngini_index                                                       0.42  \nshare_non_white                                                  0.08  \nshare_voters_voted_trump                                        -0.50  \nhate_crimes_per_100k_splc                                        0.68  \navg_hatecrimes_per_100k_fbi                                      1.00  \n\n\nTime for reflection: Look at the positive and negative correlation values above. What do they suggest and how strong, weak or moderate is the correlation.\n\nimport pandas as pd\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n\n# Let's create a heatmap to visualse the pairwise correlations for better understanding\ncorrMatrix = df2.corr(numeric_only=True).round(1)  #Rounding to (1) so it's easier to read given number of variables\nsn.heatmap(corrMatrix, annot=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Let's now perform a linear regression on our data\n# Try the commented code after you run this first\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\nx = df2[['median_household_income', 'share_population_with_high_school_degree', 'share_voters_voted_trump']]\ny = df2[['avg_hatecrimes_per_100k_fbi']]\n#What if we change the data (y variable)\n#y = df2[['hate_crimes_per_100k_splc']]\n\nest = LinearRegression(fit_intercept = True) \nest.fit(x, y)\n\nprint(\"Coefficients:\", est.coef_)\nprint (\"Intercept:\", est.intercept_)\n\n# Generate predictions from our linear regression model, and check the MSE, Rsquared and Variance measures to assess performance\ny_hat = est.predict(x)\nprint (\"MSE:\", metrics.mean_squared_error(y, y_hat))\nprint (\"R^2:\", metrics.r2_score(y, y_hat))\nprint (\"var:\", y.var())\n\nCoefficients: [[-0.0861606   0.15199564 -0.53470881]]\nIntercept: [2.12745505e-16]\nMSE: 0.7326374635746324\nR^2: 0.2673625364253678\nvar: avg_hatecrimes_per_100k_fbi    1.020408\ndtype: float64\n\n\nWhat do these values suggest?\n\n\n\n\n\n\nNote\n\n\n\nRemember the earlier note about the maps looking different? Were you able to identify what’s going there?\nLet’s revisit that part again\n\n\nThe maps did not share comparable scales. The first map was showing the annual crime rate per 100k residents, while the second map was showing the total incident numbers per 100k resident only for the 10-days following the 2016 election. How can we fix this discrepency?  Tweak the ??s below to visualise changes \n\n# We can generete two new \"features\" and add them to the DataFrame df \ndf['hate_crimes_per_100k_splc_perday'] = df['hate_crimes_per_100k_splc'] / ??\n# the 'avg_hatecrimes_per_100k_fbi' column is an annual incidence average between 2010- 15, so each data value is the number of incidences (per 100k residents) in an average year. \ndf['avg_hatecrimes_per_100k_fbi_perday'] = df['avg_hatecrimes_per_100k_fbi'] / ???\n\nSyntaxError: invalid syntax (2706068062.py, line 2)\n\n\n\n# Update geo_states\ngeo_states = geo_states.merge(df, on='????')\n\nKeyError: '????'\n\n\n\n# Let's plot again\n# First the PRE election map\npre_election_map = alt.Chart(geo_states, title='PRE-election Hate crime per 100k per day').mark_geoshape().encode(\n    alt.Color('avg_hatecrimes_per_100k_fbi_perday', scale=alt.Scale(domain=[0, 0.15])),\n    tooltip=['NAME', 'avg_hatecrimes_per_100k_fbi_perday']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\npost_election_map = alt.Chart(geo_states, title='POST-election Hate crime per 100k per day').mark_geoshape().encode(\n    alt.Color('hate_crimes_per_100k_splc_perday', scale=alt.Scale(domain=[0, 0.15])),\n    tooltip=['NAME', 'hate_crimes_per_100k_splc_perday']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\nnew_combined_map = pre_election_map | post_election_map\nnew_combined_map\n\nValueError: Unable to determine data type for the field \"avg_hatecrimes_per_100k_fbi_perday\"; verify that the field name is not misspelled. If you are referencing a field from a transform, also confirm that the data type is specified correctly.\n\n\nalt.HConcatChart(...)\n\n\nHow is that now?",
    "crumbs": [
      "Data Science & Society",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Lab: Hate crimes</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part2.html",
    "href": "content/labs/Lab_7/IM939_Lab7-Part2.html",
    "title": "33  Lab: Poverty and Inequality",
    "section": "",
    "text": "33.1 Source\nIn this lab, we will be using World Development Indicators dataset from the World Bank, which contains the following features:",
    "crumbs": [
      "Data Science & Society",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Lab: Poverty and Inequality</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part2.html#reading-the-dataset",
    "href": "content/labs/Lab_7/IM939_Lab7-Part2.html#reading-the-dataset",
    "title": "33  Lab: Poverty and Inequality",
    "section": "33.2 Reading the dataset",
    "text": "33.2 Reading the dataset\n\n%matplotlib inline\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_excel('data/WDI_countries_v2.xlsx', sheet_name='Data4')\n\nLet’s have a look at our dataset\n\ndf.head()\n\n\n\n\n\n\n\n\nCountry Code\nbirthrate\nDeathrate\nGNI\nLifeexp_female\nLifeexp_male\nNeonatal_death\n\n\n\n\n0\nAFG\n32.487\n6.423\n2260.0\n66.026\n63.047\n44503.0\n\n\n1\nALB\n11.780\n7.898\n13820.0\n80.167\n76.816\n243.0\n\n\n2\nDZA\n24.282\n4.716\n11450.0\n77.938\n75.494\n16407.0\n\n\n3\nAND\n7.200\n4.400\nNaN\nNaN\nNaN\n1.0\n\n\n4\nAGO\n40.729\n8.190\n6550.0\n63.666\n58.064\n35489.0\n\n\n\n\n\n\n\n\n33.2.1 Missing values\nLet’s check if we have any missing data\n\ndf.info()\ndf.isna().sum()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 216 entries, 0 to 215\nData columns (total 7 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   Country Code    216 non-null    object \n 1   birthrate       205 non-null    float64\n 2   Deathrate       205 non-null    float64\n 3   GNI             187 non-null    float64\n 4   Lifeexp_female  198 non-null    float64\n 5   Lifeexp_male    198 non-null    float64\n 6   Neonatal_death  193 non-null    float64\ndtypes: float64(6), object(1)\nmemory usage: 11.9+ KB\n\n\nCountry Code       0\nbirthrate         11\nDeathrate         11\nGNI               29\nLifeexp_female    18\nLifeexp_male      18\nNeonatal_death    23\ndtype: int64\n\n\nLooks like there are null values in all but one column (Country Code)\n\n# Let's look at the distribution of values in the birthrate and deathrate columns\ndf.boxplot(column=['birthrate', 'Deathrate'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\n# Let's look at the distribution of values in terms of male and female life expectancy\ndf.boxplot(column=['Lifeexp_female', 'Lifeexp_male'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\n# Let's look at the distribution of the Gross National Income (surely we expect to see outliers!)\ndf.boxplot(column=['GNI'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\n# So, as expected, the world's wealth is not distributed evenly, but what countries are the outliers as Mary Rouncefield asked?\n# We don't have the geographic coordinates as with our Hate Crime dataset to project on a map\n# But perhaps we can think of other ways to get this information. Following is one way of doing this. \n\n# Get the 25th and 75th percentile GNI values\nGNI_quartiles = df['GNI'].describe()[['25%', '75%']]\n\n# Calculate the IQR\nIQR_GNI = GNI_quartiles['75%'] - GNI_quartiles['25%']\n\n# Calculate the whiskers\nlb_GNI = GNI_quartiles['25%'] - 1.5 * IQR_GNI\nub_GNI = GNI_quartiles['75%'] + 1.5 * IQR_GNI\n\n# Retrieve the outliers\noutliers_GNI_df = df[(df['GNI'] &lt; lb_GNI) | (df['GNI'] &gt; ub_GNI)]\n\n# Get Country Codes for the outliers\noutliers_GNI_countries = outliers_GNI_df['Country Code']\noutliers_GNI_countries\n\n85     HKG\n92     IRL\n115    LUX\n116    MAC\n146    NOR\n158    QAT\n170    SGP\n187    CHE\n203    ARE\n205    USA\nName: Country Code, dtype: object\n\n\nPerhaps you can checkout this link if you are doubtful about the country names and codes\n\n# As with Part 1, let's standardise our data before we attempt further modelling using the data\nfrom sklearn import preprocessing\nimport numpy as np\nimport seaborn as sns\n\n# Get column names first\ndf_stand = df[['birthrate', 'Deathrate', 'GNI', 'Lifeexp_female', 'Lifeexp_male', 'Neonatal_death']]\nnames = df_stand.columns\n\n# Create the Scaler object\nscaler = preprocessing.StandardScaler()\n\n# Fit our data on the scaler object\ndf2 = scaler.fit_transform(df_stand)\n\n# Check what type is df2? (Do you recollect this from Part 1?)\ntype(df2)\n\nnumpy.ndarray\n\n\n\n# Let's convert the numpy array into a DataFrame before further processing\ndf2 = pd.DataFrame(df2, columns=names)\ndf2.tail()\n\n\n\n\n\n\n\n\nbirthrate\nDeathrate\nGNI\nLifeexp_female\nLifeexp_male\nNeonatal_death\n\n\n\n\n211\n-0.727171\n0.200024\nNaN\n0.994355\n0.807538\nNaN\n\n\n212\n0.982566\n-1.563870\n-0.661830\n0.051167\n0.262031\n-0.233989\n\n\n213\n1.101866\n-0.604926\nNaN\n-0.942333\n-0.798039\n0.209120\n\n\n214\n1.686551\n-0.425077\n-0.813823\n-1.114030\n-1.323007\n0.039069\n\n\n215\n1.124585\n0.117514\n-0.840505\n-1.604284\n-1.462458\n-0.026205\n\n\n\n\n\n\n\n\n# Now that our data has been standardised, let's look at the distribution across all columns\nax = sns.boxplot(data=df2, orient=\"h\")\n\n\n\n\n\n\n\n\n\n# There are clearly several outliers in some columns. Let's take a look at the numbers\ndf.describe()\n\n\n\n\n\n\n\n\nbirthrate\nDeathrate\nGNI\nLifeexp_female\nLifeexp_male\nNeonatal_death\n\n\n\n\ncount\n205.000000\n205.000000\n187.000000\n198.000000\n198.000000\n193.000000\n\n\nmean\n19.637580\n7.573941\n20630.427807\n75.193288\n70.323854\n12948.031088\n\n\nstd\n9.839573\n2.636414\n21044.240160\n7.870933\n7.419214\n48782.770706\n\n\nmin\n5.900000\n1.202000\n780.000000\n54.991000\n50.582000\n0.000000\n\n\n25%\n10.900000\n5.800000\n5090.000000\n69.497250\n65.533500\n163.000000\n\n\n50%\n17.545000\n7.163000\n13280.000000\n77.193000\n71.140500\n1288.000000\n\n\n75%\n27.100000\n9.100000\n28360.000000\n80.776500\n76.047500\n7316.000000\n\n\nmax\n46.079000\n15.400000\n123290.000000\n87.700000\n82.300000\n546427.000000\n\n\n\n\n\n\n\n\n# Let's look at the histogram for birthrate \ndf[['birthrate']].plot(kind='hist', ec='black')\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\n\n\n\nTip for reflection: Is the above a unimodal or bimodal distribution? Positive or negative skew - We have worked on this in earlier labs. Try to recollect/revisit and reflect. Can we see/say any better with a kernel density plot?\n\n# Let's create a pairwise plot as we have done many times before to get an idea about the relationship between variables  \nsns.pairplot(data = df.iloc[:,1:])\n\n\n\n\n\n\n\n\nHmmm, some likely positive and negative correlations\n\n# Let's check the relationship between birthrate and deathrate\ndf.plot(x = 'birthrate', y = 'Deathrate', kind='scatter')\n\n&lt;Axes: xlabel='birthrate', ylabel='Deathrate'&gt;\n\n\n\n\n\n\n\n\n\n\n# Let's create a heatmap like we did in Part 1\ncorrMatrix = df.corr(numeric_only=True).round(1) # Again, round(1) so that it's easier to read given number of variables\nsns.heatmap(corrMatrix, annot=True)\nplt.show()",
    "crumbs": [
      "Data Science & Society",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Lab: Poverty and Inequality</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part2.html#how-quickly-are-populations-growing",
    "href": "content/labs/Lab_7/IM939_Lab7-Part2.html#how-quickly-are-populations-growing",
    "title": "33  Lab: Poverty and Inequality",
    "section": "33.3 How quickly are populations growing?",
    "text": "33.3 How quickly are populations growing?\nThis question can be investigated by calculating birth rate minus death rate.\n\n# Let's add a new column to our DataFrame to indicate the rate of population change (assuming the absence of migration)\ndf['pop_change_rate'] = df['birthrate'] - df['Deathrate']\ndf.head()\n\n\n\n\n\n\n\n\nCountry Code\nbirthrate\nDeathrate\nGNI\nLifeexp_female\nLifeexp_male\nNeonatal_death\npop_change_rate\n\n\n\n\n0\nAFG\n32.487\n6.423\n2260.0\n66.026\n63.047\n44503.0\n26.064\n\n\n1\nALB\n11.780\n7.898\n13820.0\n80.167\n76.816\n243.0\n3.882\n\n\n2\nDZA\n24.282\n4.716\n11450.0\n77.938\n75.494\n16407.0\n19.566\n\n\n3\nAND\n7.200\n4.400\nNaN\nNaN\nNaN\n1.0\n2.800\n\n\n4\nAGO\n40.729\n8.190\n6550.0\n63.666\n58.064\n35489.0\n32.539\n\n\n\n\n\n\n\n\n# Let's look at the distribution of our new rate of population change column \ndf.boxplot(column=['pop_change_rate'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nLet’s investigate the values in the column more deeply:\n\npop_change_rate_summary = df['pop_change_rate'].describe()\npop_change_rate_summary\n\ncount    205.000000\nmean      12.063639\nstd       10.477867\nmin       -6.500000\n25%        2.700000\n50%       11.213000\n75%       20.582000\nmax       37.811000\nName: pop_change_rate, dtype: float64\n\n\nThe results range from -6.5 (a decreasing population) to +37.811 (an increasing population). The mean is around 12.06. What does this signify?\n\n\n\n\nRouncefield, Mary. 1995. “The Statistics of Poverty and Inequality.” Journal of Statistics Education 3 (2): 8. https://doi.org/10.1080/10691898.1995.11910491.",
    "crumbs": [
      "Data Science & Society",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Lab: Poverty and Inequality</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part3.html",
    "href": "content/labs/Lab_7/IM939_Lab7-Part3.html",
    "title": "34  Lab: Gender gaps",
    "section": "",
    "text": "34.1 Source (Dataset)\nOffice of the National Statistics Gender Pay Gap ONS Source",
    "crumbs": [
      "Data Science & Society",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Lab: Gender gaps</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part3.html#explanations-from-the-source",
    "href": "content/labs/Lab_7/IM939_Lab7-Part3.html#explanations-from-the-source",
    "title": "34  Lab: Gender gaps",
    "section": "34.2 Explanations (from the source)",
    "text": "34.2 Explanations (from the source)\n\nGender pay gap (GPG) - calculated as the difference between average hourly earnings (excluding overtime) of men and women as a proportion of average hourly earnings (excluding overtime) of men. For example, a 4% GPG denotes that women earn 4% less, on average, than men. Conversely, a -4% GPG denotes that women earn 4% more, on average, than men.\nMean: a measure of the average which is derived by summing the values for a given sample, and then dividing the sum by the number of observations (i.e. jobs) in the sample. In earnings distributions, the mean can be disproportionately influenced by a relatively small number of high-paying jobs.\nMedian: the value below which 50% of jobs fall. It is ONS’s preferred measure of average earnings as it is less affected by a relatively small number of very high earners and the skewed distribution of earnings. It therefore gives a better indication of typical pay than the mean.\n\n\n34.2.1 Coverage and timeliness\nThe Annual Survey of Hours and Earnings (ASHE) covers employee jobs in the United Kingdom. It does not cover the self-employed, nor does it cover employees not paid during the reference period (2023).\nGPG estimates are provided for the pay period that included a specified date in April. They relate to employees on adult rates of pay, whose earnings for the survey pay period were not affected by absence.\nASHE is based on a 1% sample of jobs taken from HM Revenue and Customs’ Pay As You Earn (PAYE) records. Consequently, individuals with more than one job may appear in the sample more than once.",
    "crumbs": [
      "Data Science & Society",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Lab: Gender gaps</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part3.html#reading-the-dataset",
    "href": "content/labs/Lab_7/IM939_Lab7-Part3.html#reading-the-dataset",
    "title": "34  Lab: Gender gaps",
    "section": "34.3 Reading the dataset",
    "text": "34.3 Reading the dataset\n\n%matplotlib inline\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_profession = pd.read_excel('data/genderpaygap.xlsx', sheet_name='All')\ndf_profession_category = pd.read_excel('data/genderpaygap.xlsx', sheet_name='Main')\ndf_age = pd.read_excel('data/genderpaygap.xlsx', sheet_name='Age')\ndf_geography = pd.read_excel('data/genderpaygap.xlsx', sheet_name='Geography')\n\nLet’s have a look at our dataset\n\ndf_profession.tail()\n\n\n\n\n\n\n\n\nDescription\nCode\nGPGmedian\nGPGmean\n\n\n\n\n31\nProcess, plant and machine operatives\n81\n14.0\n14.1\n\n\n32\nTransport and mobile machine drivers and ope...\n82\n10.5\n2.9\n\n\n33\nElementary occupations\n9\n5.8\n8.1\n\n\n34\nElementary trades and related occupations\n91\n7.1\n7.7\n\n\n35\nElementary administration and service occupa...\n92\n5.6\n8.2\n\n\n\n\n\n\n\n\ndf_profession_category.tail()\n\n\n\n\n\n\n\n\nDescription\nCode\nGPGmedian\nGPGmean\n\n\n\n\n5\nSkilled trades occupations\n5\n19.0\n14.5\n\n\n6\nCaring, leisure and other service occupations\n6\n1.5\n2.0\n\n\n7\nSales and customer service occupations\n7\n3.7\n4.5\n\n\n8\nProcess, plant and machine operatives\n8\n14.1\n13.0\n\n\n9\nElementary occupations\n9\n5.8\n8.1\n\n\n\n\n\n\n\n\ndf_age\n\n\n\n\n\n\n\n\nage_group\nGPGmedian\nGPGmean\n\n\n\n\n0\n16-17b\n0.0\n-7.9\n\n\n1\n18-21\n0.8\n10.6\n\n\n2\n22-29\n4.8\n4.3\n\n\n3\n30-39\n11.5\n9.8\n\n\n4\n40-49\n17.0\n15.1\n\n\n5\n50-59\n19.7\n17.9\n\n\n6\n60+\n18.1\n18.2\n\n\n\n\n\n\n\n\ndf_geography.tail()\n\n\n\n\n\n\n\n\nDescription\nCode\nGPGmedian\nGPGmean\n\n\n\n\n385\nSouth Lanarkshire\nS12000029\n6.1\n7.5\n\n\n386\nStirling\nS12000030\n7.4\n21.9\n\n\n387\nWest Dunbartonshire\nS12000039\n17.5\n12.8\n\n\n388\nWest Lothian\nS12000040\n8.3\n9.6\n\n\n389\nNorthern Ireland\nN92000002\n8.1\n9.6\n\n\n\n\n\n\n\nIf you look at the Excel data files, we see that occupations have a main and sub-category. Since we have the main category values in df_profession_category anyway, let’s drop them from ‘df_profession’ to retain the focus on sub-categories only. We can do this based on the values in the Code column since as you can see main category professions have code values &lt; 10 and sub-categories have values greater than 10.\n\nindices_to_drop = df_profession[df_profession['Code'] &lt; 10].index\ndf_profession.drop(indices_to_drop, inplace=True)\ndf_profession\n\n\n\n\n\n\n\n\nDescription\nCode\nGPGmedian\nGPGmean\n\n\n\n\n2\nCorporate managers and directors\n11\n12.4\n12.8\n\n\n3\nOther managers and proprietors\n12\n4.8\n8.6\n\n\n5\nScience, research, engineering and technolog...\n21\n10.2\n9.2\n\n\n6\nHealth professionals\n22\n10.2\n15.2\n\n\n7\nTeaching and other educational professionals\n23\n3.8\n8.9\n\n\n8\nBusiness, media and public service professio...\n24\n7.9\n11\n\n\n10\nScience, engineering and technology associat...\n31\n11.8\n8\n\n\n11\nHealth and social care associate professionals\n32\n4.7\n4.9\n\n\n12\nProtective service occupations\n33\n4.7\n3.4\n\n\n13\nCulture, media and sports occupations\n34\n5.2\nx\n\n\n14\nBusiness and public service associate profes...\n35\n13.9\n18\n\n\n16\nAdministrative occupations\n41\n5.9\n6.4\n\n\n17\nSecretarial and related occupations\n42\n-0.9\n-2.3\n\n\n19\nSkilled agricultural and related trades\n51\n-6.4\n-3.8\n\n\n20\nSkilled metal, electrical and electronic trades\n52\n9.4\n2.3\n\n\n21\nSkilled construction and building trades\n53\n12.6\n6.2\n\n\n22\nTextiles, printing and other skilled trades\n54\n3.4\n4.4\n\n\n24\nCaring personal service occupations\n61\n0.7\n0.7\n\n\n25\nLeisure, travel and related personal service...\n62\n5.3\n7.4\n\n\n26\nCommunity and civil enforcement occupations\n63\n-28.9\n-20.6\n\n\n28\nSales occupations\n71\n1.8\n6.3\n\n\n29\nCustomer service occupations\n72\n0.0\n2.8\n\n\n31\nProcess, plant and machine operatives\n81\n14.0\n14.1\n\n\n32\nTransport and mobile machine drivers and ope...\n82\n10.5\n2.9\n\n\n34\nElementary trades and related occupations\n91\n7.1\n7.7\n\n\n35\nElementary administration and service occupa...\n92\n5.6\n8.2\n\n\n\n\n\n\n\n\n34.3.1 Missing values\nLet’s check our data\n\ndf_profession.info()\ndf_profession_category.info()\ndf_age.info()\ndf_geography.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 26 entries, 2 to 35\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Description  26 non-null     object \n 1   Code         26 non-null     int64  \n 2   GPGmedian    26 non-null     float64\n 3   GPGmean      26 non-null     object \ndtypes: float64(1), int64(1), object(2)\nmemory usage: 1.0+ KB\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10 entries, 0 to 9\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Description  10 non-null     object \n 1   Code         10 non-null     int64  \n 2   GPGmedian    10 non-null     float64\n 3   GPGmean      10 non-null     float64\ndtypes: float64(2), int64(1), object(1)\nmemory usage: 452.0+ bytes\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7 entries, 0 to 6\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   age_group  7 non-null      object \n 1   GPGmedian  7 non-null      float64\n 2   GPGmean    7 non-null      float64\ndtypes: float64(2), object(1)\nmemory usage: 300.0+ bytes\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 390 entries, 0 to 389\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   Description  390 non-null    object\n 1   Code         390 non-null    object\n 2   GPGmedian    390 non-null    object\n 3   GPGmean      390 non-null    object\ndtypes: object(4)\nmemory usage: 12.3+ KB\n\n\n\n# It looks like GPGmean is read as an object (string) in df_profession dataframe. \n# GPGmean and GPGmedian are both objects in df_geography\n# Let's convert the data to float64, so we can create plots later\ndf_profession['GPGmean'] = pd.to_numeric(df_profession['GPGmean'], errors='coerce')\ndf_geography['GPGmean'] = pd.to_numeric(df_geography['GPGmean'], errors='coerce')\ndf_geography['GPGmedian'] = pd.to_numeric(df_geography['GPGmedian'], errors='coerce')\n\n\n# Next, let's check for missing values\ndf_profession.isna().sum()\ndf_profession_category.isna().sum()\ndf_age.isna().sum()\n\nage_group    0\nGPGmedian    0\nGPGmean      0\ndtype: int64\n\n\nAll seems fine - let’s get plotting\n\n# Let's plot the mean and median Gender Pay Gap (GPG)\ndf_profession.boxplot(column=['GPGmedian', 'GPGmean'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nHmmm, there are outliers. Let’s check the descriptive statistics\n\n# Let's look at the distribution of the values in the columns\ndf_profession.describe()\n\n\n\n\n\n\n\n\nCode\nGPGmedian\nGPGmean\n\n\n\n\ncount\n26.000000\n26.000000\n25.00000\n\n\nmean\n47.923077\n4.988462\n5.70800\n\n\nstd\n23.918065\n8.505778\n7.49277\n\n\nmin\n11.000000\n-28.900000\n-20.60000\n\n\n25%\n31.250000\n3.500000\n2.90000\n\n\n50%\n46.500000\n5.450000\n6.40000\n\n\n75%\n62.750000\n10.200000\n8.90000\n\n\nmax\n92.000000\n14.000000\n18.00000\n\n\n\n\n\n\n\n\n# Let's try to visualise what's going on with a histogram - what type of skew do you notice?\ndf_profession[['GPGmedian']].plot(kind='hist', ec='black')\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\n\n\n\nHmmm, there appears to be a lone bin in our histogram. Which might be the profession or professions where women earn more than men?\n\n# Is there one profession or more professions where women earn more? Let's do some investigation through visualisation. \nimport altair as alt\nalt.Chart(df_profession).mark_bar().encode(\n    alt.X(\"GPGmedian:Q\", bin=True, title='GPGmedian'), \n    y=alt.Y('Description:N', sort='-x', title='Professional Category'),  \n    color='Description:N',\n    tooltip=['Description', 'GPGmedian']\n).properties(\n    width=600,\n    height=400\n)\n\n\n\n\n\n\n\nThis plot shows us that Community and civil enforcement occupations, skilled agricultural and related trades, and secretarial and related occupations are the ones where women earn, on average, more than men.\nIf you are wondering what ‘community and civil enforcement occupations’ mean - then this ONS source says it includes police community and parking and civil enforcement officers.\nAre these occupations the ones you suspected women to earn more than men (on average)?\n\n\n\n\n\n\nSidenote\n\n\n\nThe above visualisation is detailed, but it’s busy and cluttered. How about if we try doing this on df_profession_category instead?\n\n\n\n# Is there one profession where Women earn more? Let's do some investigation. \nimport altair as alt\nalt.Chart(df_profession_category).mark_bar().encode(\n    alt.X(\"GPGmedian:Q\", bin=True), \n    y=alt.Y('Description:N', sort='-x'),  \n    color='Description:N',\n    tooltip=['Description', 'GPGmedian']\n).properties(\n    width=600,\n    height=400\n)\n\n\n\n\n\n\n\nIn this, we have lost some of the detail we had in the earlier visualisation, but we get to know that “Caring, leisure and other service occupations” is a ‘main category’ of occcupation where the GPG is low (but women don’t earn more than men).\n\n\n\n\n\n\nSidenote\n\n\n\nWhat does this narrative tell you about women being more likely to do multiple jobs to work around their domestic responsibilities which we spoke about in the lecture (and recordings)?\n\n\n\n# Alternative visualisation (excluding all employees category)\n\n# In which main professional categories is the gap narrow? Let's find out!\ndf_professions_sorted = df_profession_category.sort_values('GPGmedian', ascending=True)\n\n# Let's drop the row corresponding to 'All employees' because we are more interested in looking at the differences across professional categories and sub-categories here \ndf_professions_sorted = df_professions_sorted[df_professions_sorted['Description'] != 'All employees']\n\n# Let's create the bar plot\ndf_professions_sorted.plot.bar(x='Description', y='GPGmedian')\n\n&lt;Axes: xlabel='Description'&gt;\n\n\n\n\n\n\n\n\n\nLet’s look at age-based differences next:\n\ndf_age.sort_values('age_group', ascending=True).plot.bar(x = 'age_group', y = 'GPGmedian')\n\n&lt;Axes: xlabel='age_group'&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nIt seems that GPG increases with age - what does this say about our dicussion during the lectures about GPG increasing for women who take time off from work for a variety of reasons compared to their male and female counterparts who do not take time out of work! What do you think might be the reasons for the minor fall in GPG at 60+?",
    "crumbs": [
      "Data Science & Society",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Lab: Gender gaps</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part3.html#geography",
    "href": "content/labs/Lab_7/IM939_Lab7-Part3.html#geography",
    "title": "34  Lab: Gender gaps",
    "section": "34.4 Geography",
    "text": "34.4 Geography\n\n34.4.0.1 But first:\nSince moving on from our Iris and Wine datasets, the real-world datasets rarely come prepared (ready to use).\n\nIf you download the zip file for the latest 2023 Pay Gap ONS statistics, you will notice that they have color-coded their cells based on the certainty of estimates. On the one hand, this is very good practice - being transparent about the quality of the data. On the other hand, it tells us that we need to be careful about what insights we can draw from the data.\nIf you look at the statistics for one year, you can get a glimpse of what’s happening across various categories (geography, age, profession, etc.) in terms of GPG but it’s a cross-sectional view. But you can collate a longitudinal view should you wish to. E.g., by downloading the zip folders across the desired years and collating the information for desired categories for multiple years. But remember that this will be a ‘simplified approach’ to a longitudinal view and will have limitations. Also, recollect one of the figures Cagatay showed in the earlier lectures - it’s common to spend a lot of time at the start of your Data Science project just collating the necessary information. If you fancy, you can write a script to automate the data collation process!\nWe have geography information as area codes from the ONS source, but wouldn’t it be nice if we are able to visualise GPG by Geography on a map of England (with Levelling Up agenda and all). That’s the data hunt I went on. And the ONS’ Geodata portal provides datasets from the different administrative boundaries, so I downloaded this one:Counties and Unitary Authorities (May 2023) Boundaries UK BUC. Now let’s see what visualisation we can create with it.\n\n\n\n\n\n\n\nTakeaway\n\n\n\nCollating data from multiple sources is a significant, valuable and legitimate part of the Data Science project journey\n\n\n\n# Getting the geospatial polygons for England\nimport geopandas as gpd \nimport altair as alt\n\ngeo_states_england = gpd.read_file('data/Counties_and_Unitary_Authorities_May_2023_UK_BUC_-7406349609691062173.gpkg')\ngeo_states_england.head()\n\n\n\n\n\n\n\n\nCTYUA23CD\nCTYUA23NM\nCTYUA23NMW\nBNG_E\nBNG_N\nLONG\nLAT\nGlobalID\ngeometry\n\n\n\n\n0\nE06000001\nHartlepool\n\n447160\n531474\n-1.27018\n54.676102\n{224B1BB0-27FA-4B44-AD01-F22525CE232E}\nMULTIPOLYGON (((448973.593 536745.277, 448290....\n\n\n1\nE06000002\nMiddlesbrough\n\n451141\n516887\n-1.21099\n54.544701\n{8A06DF87-1F09-4A1C-9D6E-A32D40A0B159}\nMULTIPOLYGON (((451894.299 521145.303, 448410....\n\n\n2\nE06000003\nRedcar and Cleveland\n\n464361\n519597\n-1.00608\n54.567501\n{4A930CE8-4656-4A98-880E-8110EE3D8501}\nMULTIPOLYGON (((478232.568 518788.831, 478074....\n\n\n3\nE06000004\nStockton-on-Tees\n\n444940\n518183\n-1.30664\n54.556900\n{304224A1-E808-4BF2-8F3E-AC43B0368BE8}\nMULTIPOLYGON (((452243.536 526335.188, 451148....\n\n\n4\nE06000005\nDarlington\n\n428029\n515648\n-1.56835\n54.535301\n{F7BBD06A-7E09-4832-90D0-F6CA591D4A1D}\nMULTIPOLYGON (((436388.002 522354.197, 435529....\n\n\n\n\n\n\n\n\nprint(geo_states_england.columns)\n\nIndex(['CTYUA23CD', 'CTYUA23NM', 'CTYUA23NMW', 'BNG_E', 'BNG_N', 'LONG', 'LAT',\n       'GlobalID', 'geometry'],\n      dtype='object')\n\n\n\n# Let's drop the columns we don't need\ngeo_states_england = geo_states_england.drop(['CTYUA23NMW', 'BNG_E', 'BNG_N', 'GlobalID'], axis=1)\n\n\n# Let's check again\ngeo_states_england.head()\n\n\n\n\n\n\n\n\nCTYUA23CD\nCTYUA23NM\nLONG\nLAT\ngeometry\n\n\n\n\n0\nE06000001\nHartlepool\n-1.27018\n54.676102\nMULTIPOLYGON (((448973.593 536745.277, 448290....\n\n\n1\nE06000002\nMiddlesbrough\n-1.21099\n54.544701\nMULTIPOLYGON (((451894.299 521145.303, 448410....\n\n\n2\nE06000003\nRedcar and Cleveland\n-1.00608\n54.567501\nMULTIPOLYGON (((478232.568 518788.831, 478074....\n\n\n3\nE06000004\nStockton-on-Tees\n-1.30664\n54.556900\nMULTIPOLYGON (((452243.536 526335.188, 451148....\n\n\n4\nE06000005\nDarlington\n-1.56835\n54.535301\nMULTIPOLYGON (((436388.002 522354.197, 435529....\n\n\n\n\n\n\n\n\n# Let's create a map of England\npre_GPG_England = alt.Chart(geo_states_england, title='Map of England').mark_geoshape().encode(\n    tooltip=['CTYUA23NM']\n).properties(\n    width=500,\n    height=300\n)\npre_GPG_England\n\n\n\n\n\n\n\nWait, what’s that?! That’s not what we were expecting!\n\n\n\n\n\n\nMap projections\n\n\n\nBecause the Earth is round, and maps are flat, geospatial data needs to be “projected”. There are many types of projecting geospatial data, and all of them come with some tradeoff in terms of distorting area and/or distance (in other words, none of them are perfect). You can read more here.\nNow, the geospatial dataset that we are using for this notebook was downloaded from the Office for National Statistics’ Geoportal and uses a Coordinate Reference System (CRS) known as EPSG:27700 - OSGB36 / British National Grid. Regretfully, Altair works with a different CRS: WGS 84 (also known as epsg:4326), and this is creating the conflict.\nWe have two options: either reproject our data using geopandas, or according to Altair documentation try using the project configuration (type: 'identity', reflectY': True). It draws the geometries without applying a projection.\n\n\n\n# Let's create a map of England\npre_GPG_England = alt.Chart(\n    geo_states_england, title='Map of England'\n).mark_geoshape().encode(\n    tooltip=['CTYUA23NM']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='identity',\n    reflectY=True\n)\npre_GPG_England\n\n\n\n\n\n\n\n\ndf_geography.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 390 entries, 0 to 389\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Description  390 non-null    object \n 1   Code         390 non-null    object \n 2   GPGmedian    386 non-null    float64\n 3   GPGmean      387 non-null    float64\ndtypes: float64(2), object(2)\nmemory usage: 12.3+ KB\n\n\n\ngeo_states_england\n\n\n\n\n\n\n\n\nCTYUA23CD\nCTYUA23NM\nLONG\nLAT\ngeometry\n\n\n\n\n0\nE06000001\nHartlepool\n-1.27018\n54.676102\nMULTIPOLYGON (((448973.593 536745.277, 448290....\n\n\n1\nE06000002\nMiddlesbrough\n-1.21099\n54.544701\nMULTIPOLYGON (((451894.299 521145.303, 448410....\n\n\n2\nE06000003\nRedcar and Cleveland\n-1.00608\n54.567501\nMULTIPOLYGON (((478232.568 518788.831, 478074....\n\n\n3\nE06000004\nStockton-on-Tees\n-1.30664\n54.556900\nMULTIPOLYGON (((452243.536 526335.188, 451148....\n\n\n4\nE06000005\nDarlington\n-1.56835\n54.535301\nMULTIPOLYGON (((436388.002 522354.197, 435529....\n\n\n...\n...\n...\n...\n...\n...\n\n\n213\nW06000020\nTorfaen\n-3.05101\n51.698399\nMULTIPOLYGON (((333723.000 192653.903, 330700....\n\n\n214\nW06000021\nMonmouthshire\n-2.90280\n51.778301\nMULTIPOLYGON (((329597.402 229251.797, 326793....\n\n\n215\nW06000022\nNewport\n-2.89769\n51.582298\nMULTIPOLYGON (((343091.833 184213.309, 342279....\n\n\n216\nW06000023\nPowys\n-3.43531\n52.348598\nMULTIPOLYGON (((322891.550 333139.949, 321104....\n\n\n217\nW06000024\nMerthyr Tydfil\n-3.36425\n51.748600\nMULTIPOLYGON (((308057.304 211036.201, 306294....\n\n\n\n\n218 rows × 5 columns\n\n\n\n\ndf_geography\n\n\n\n\n\n\n\n\nDescription\nCode\nGPGmedian\nGPGmean\n\n\n\n\n0\nDarlington UA\nE06000005\n5.4\n13.3\n\n\n1\nHartlepool UA\nE06000001\n6.2\n8.9\n\n\n2\nMiddlesbrough UA\nE06000002\n14.5\n15.6\n\n\n3\nRedcar and Cleveland UA\nE06000003\n12.8\n12.3\n\n\n4\nStockton-on-Tees UA\nE06000004\n17.1\n16.9\n\n\n...\n...\n...\n...\n...\n\n\n385\nSouth Lanarkshire\nS12000029\n6.1\n7.5\n\n\n386\nStirling\nS12000030\n7.4\n21.9\n\n\n387\nWest Dunbartonshire\nS12000039\n17.5\n12.8\n\n\n388\nWest Lothian\nS12000040\n8.3\n9.6\n\n\n389\nNorthern Ireland\nN92000002\n8.1\n9.6\n\n\n\n\n390 rows × 4 columns\n\n\n\n\n# Add the data\ngeo_states_england_merged = geo_states_england.merge(df_geography, left_on = 'CTYUA23CD', right_on = 'Code')\n\n\n# Check the merged data\ngeo_states_england_merged.head(10)\n\n\n\n\n\n\n\n\nCTYUA23CD\nCTYUA23NM\nLONG\nLAT\ngeometry\nDescription\nCode\nGPGmedian\nGPGmean\n\n\n\n\n0\nE06000001\nHartlepool\n-1.27018\n54.676102\nMULTIPOLYGON (((448973.593 536745.277, 448290....\nHartlepool UA\nE06000001\n6.2\n8.9\n\n\n1\nE06000002\nMiddlesbrough\n-1.21099\n54.544701\nMULTIPOLYGON (((451894.299 521145.303, 448410....\nMiddlesbrough UA\nE06000002\n14.5\n15.6\n\n\n2\nE06000003\nRedcar and Cleveland\n-1.00608\n54.567501\nMULTIPOLYGON (((478232.568 518788.831, 478074....\nRedcar and Cleveland UA\nE06000003\n12.8\n12.3\n\n\n3\nE06000004\nStockton-on-Tees\n-1.30664\n54.556900\nMULTIPOLYGON (((452243.536 526335.188, 451148....\nStockton-on-Tees UA\nE06000004\n17.1\n16.9\n\n\n4\nE06000005\nDarlington\n-1.56835\n54.535301\nMULTIPOLYGON (((436388.002 522354.197, 435529....\nDarlington UA\nE06000005\n5.4\n13.3\n\n\n5\nE06000006\nHalton\n-2.68853\n53.334202\nMULTIPOLYGON (((358131.901 385425.802, 355191....\nHalton UA\nE06000006\n3.4\n4.6\n\n\n6\nE06000007\nWarrington\n-2.56167\n53.391602\nMULTIPOLYGON (((367582.201 396058.199, 367158....\nWarrington UA\nE06000007\n12.8\n14.6\n\n\n7\nE06000008\nBlackburn with Darwen\n-2.46360\n53.700802\nMULTIPOLYGON (((372966.498 423266.501, 371465....\nBlackburn with Darwen UA\nE06000008\n22.3\n15.3\n\n\n8\nE06000009\nBlackpool\n-3.02199\n53.821602\nMULTIPOLYGON (((333572.799 437130.702, 333041....\nBlackpool UA\nE06000009\n4.4\n3.5\n\n\n9\nE06000010\nKingston upon Hull, City of\n-0.30382\n53.769199\nMULTIPOLYGON (((515429.592 427689.472, 516047....\nKingston upon Hull UA\nE06000010\n16.1\n7.9\n\n\n\n\n\n\n\n\n# Let's plot the GPG by geography now\npost_GPG_England = alt.Chart(geo_states_england_merged, title='GPG by region - England').mark_geoshape().encode(\n    color='GPGmedian',\n    tooltip=['Description', 'GPGmedian']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='identity',\n    reflectY=True\n)\npost_GPG_England\n\n\n\n\n\n\n\n\n# side by side view\nGPG_England = pre_GPG_England | post_GPG_England\nGPG_England\n\n\n\n\n\n\n\nHow do the results in this workbook compare to the visualisation we saw during the lecture, for example, for the UK in Information is Beautiful But remember the earnings across the two might be for different years - do remember to check the metadata!",
    "crumbs": [
      "Data Science & Society",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Lab: Gender gaps</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-08.html",
    "href": "content/sessions/session-08.html",
    "title": "35  Session 1: Design Thinking in Data Science",
    "section": "",
    "text": "35.1 Supportive Reading & Resources",
    "crumbs": [
      "Data Science Workshop",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Session 1: Design Thinking in Data Science</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-08.html#supportive-reading-resources",
    "href": "content/sessions/session-08.html#supportive-reading-resources",
    "title": "35  Session 1: Design Thinking in Data Science",
    "section": "",
    "text": "A good reference from Design Council on their human centred design process where bits of the methodology could frame approaching Data Science projects – https://www.designcouncil.org.uk/news-opinion/what-framework-innovation-design-councils-evolved-double-diamond\nIDEO’s The Field Guide to Human-Centered Design is a broader and detailed guide that can provide some additional methods to approach for the design of DS solutions: https://www.designkit.org/resources/1\nA light practical article titled “Integrating Personas in User-Centered ML Model Development”: https://towardsdatascience.com/integrating-personas-in-user-centered-ml-model-development-afb593741c49",
    "crumbs": [
      "Data Science Workshop",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Session 1: Design Thinking in Data Science</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-08.html#further-resources",
    "href": "content/sessions/session-08.html#further-resources",
    "title": "35  Session 1: Design Thinking in Data Science",
    "section": "35.2 Further resources",
    "text": "35.2 Further resources\n\nAn interesting project that looks into developing some basic templates to organise data science projects – https://drivendata.github.io/cookiecutter-data-science/\nAnd a further ethics checklist from the same team – https://deon.drivendata.org/ [Do you see any gaps/issues in their handling of this checklist?]",
    "crumbs": [
      "Data Science Workshop",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Session 1: Design Thinking in Data Science</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-08_WorkshopBrief.html",
    "href": "content/sessions/session-08_WorkshopBrief.html",
    "title": "36  Brief: Analysing Health Indicators",
    "section": "",
    "text": "36.1 Data\nFrom Health Profiles Programme:\nIn this case, we are lucky that the authorities have nicely structured the data and made sure that it is easily possible to join the different data sets. We will be using the information collected and made available through the Health Profiles Portal.\nThe data is for each local authority in England and presented in the form of indicators that have been carefully processed and made available for analysis (e.g., grouped under categories such as “Our communities”, “Disease and poor health”, etc.).\nThrough this link, you can investigate the data visually and download data on a single indicator in either CSV or Excel format. Each file starts with a meta-data section where the indicator is described so these brief information should be sufficient to understand the contents of the file. However, if you want to get a deeper understanding of the indicators and read about how they have been collected, you can have a look at this detailed page about the indicators. Note that the actual indicator values are under the column “Value” in the joint files.\nThe indicators are available from the above link but you can also download a csv with all the indicators in a zipped file here.",
    "crumbs": [
      "Data Science Workshop",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Brief: Analysing Health Indicators</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-08_WorkshopBrief.html#tasks",
    "href": "content/sessions/session-08_WorkshopBrief.html#tasks",
    "title": "36  Brief: Analysing Health Indicators",
    "section": "36.2 Tasks",
    "text": "36.2 Tasks\nAs also described above, the expectation to utilise the relation between the health indicators and socio-demographics information.\n\n36.2.1 Step-1: Sketch a hypothetical scenario and a stakeholder\n\nIdentify a stakeholder and define the setting. Think about the personas and think about the context. Think about questions such as:\n\nWhat are the needs of the people? What do they know and what data is available?\nWhat do they need to know?\nWhat are some sensitivities?\n\n\n\n36.2.1.1 Potential stakeholder involved could be:\nPublic Health Officer, Local Authority Officer, Patient-facing Clinician (GP), Clinical researcher, Patient…\n\n\n36.2.1.2 Potential situations could be:\nConsider yourself/your team to be a data scientists aiming to support:\n\nA local authority\nClinical researcher talking to a public health officer – how can research outcome inform policy?\nPatient looking for self help – how to visualise information to inform them?\nGP giving advice to a patient – how to visualise information to support their conversation?\n\n\n\n\n\n\n\nTasks to complete\n\n\n\n\nT1: Explore the datasets through the webpages above to get yourself familiar.\nT2: Identify a stakeholder and agree on a situation\nT3: Use this persona template from servicedesigntools.org [link to template page] to help outline a stakeholder persona. Consider how you can help your stakeholder through data when responding to these challenges.\n\n\n\n\n\n\n36.2.2 Step-2: Identify analytical questions and data\n\nIdentify a number of analytical questions that you can address using the data available and using the analytical tools that we have available.\nIdentify the data fields that fit with your scenario\nChoose features that are relevant and speculate what you can achieve with them\nConsider if you can make use of any design thinking tools, some examples here: https://www.designkit.org/methods, for instance, you can reflect on this brainstorming guidance to help with your discussions as a group: https://www.designkit.org/methods/brainstorm\n\n\n\n\n\n\n\nTasks to complete\n\n\n\n\nT4: Develop one or two analytical questions that you want to address\nT5: Clarify which data variables you will be using in your analysis in order to answer your research question\n\n\n\n\n\n36.2.3 Step-3: Plan an analysis approach/strategy\n\nDecide on an analysis strategy, which methods will you use and how they can help you respond to your question.\n\nTry to decide on some high level strategies, e.g., “will develop a regression model predicting XXX using variables associated with YYY”, or “will apply some clustering algorithms on variables associated with XXX to look for some emerging groups”\n\nAlso identify which data variables could be used with which technique.\nYou can already use a Jupyter Notebook document as a template. Set some headers to indicate the different stages of your analysis.\n\n\n\n\n\n\n\nTasks to complete\n\n\n\n\nT6: Develop an analysis plan on which techniques you want to use and which data variables you will incorporate.\n\n\n\n\n\n36.2.4 Step-4: Conduct your analysis\n\nConduct the analysis following your plan. You might find yourself reflecting back on your decisions in the earlier steps and doing things differently to your plan.\nMake good use of exploratory analysis, you might find out that data is different to what you were expecting it to be or new patterns might emerge.\nThink of how you can visualise the data and the results from the tools you use\nMake use of existing Jupyter Notebooks and pieces of code from the labs\n\n\n\n\n\n\n\nTasks to complete\n\n\n\n\nT7: Conduct your analysis using Jupyter Notebook by documenting your decisions as you go along.\n\n\n\n\n\n36.2.5 Step-5: Presenting the results (Note: To be presented on Week-10)\n\nThink about how you will present your results, extract insights, and extract some key visualisations\nIf you need more time to work as a group on your analysis, arrange some time to meet and finalise some bits\nStart a slide-deck and prepare max three slides to:\n\nIntroduce your persona/context\nList your analytical questions\nPresent some results from your analysis\n\n\n\n\n\n\n\n\nTasks to complete\n\n\n\n\nT8: Prepare to present your results\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe’ll tell you how to get your slides to us",
    "crumbs": [
      "Data Science Workshop",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Brief: Analysing Health Indicators</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-08_WorkshopBrief.html#workshop-timeline",
    "href": "content/sessions/session-08_WorkshopBrief.html#workshop-timeline",
    "title": "36  Brief: Analysing Health Indicators",
    "section": "36.3 Workshop Timeline",
    "text": "36.3 Workshop Timeline\nWe have a total of 2 hours for the session, so roughly 120 minutes.\n\n15 min: Introduction and breakout into groups\n20 min: Step-1, Sketch a hypothetical scenario and a stakeholder\n15 min: Step 2, Identify analytical questions and data\n15 min: Step-3, Plan an analysis approach/strategy\n10 min: Break / Reflect\n40 min: Step-4, Conduct your analysis (if you have the time, plan and move to Step-5)\n5 min: Groups reconvene, debrief\n+ Week-10 reporting back, 2-3 min. from each group",
    "crumbs": [
      "Data Science Workshop",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Brief: Analysing Health Indicators</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-09.html",
    "href": "content/sessions/session-09.html",
    "title": "37  Session 2: Bring Your Own Data (BYOD)",
    "section": "",
    "text": "37.1 The workshop session\nIn the workshop session, we would like to hear from you on your ideas for your second coursework. You will also be able to use this session to start working on the dataset(s) that you have decided (or still considering) to analyse as part of your second assessment and discuss with your peers and with the staff members.",
    "crumbs": [
      "Data Science Workshop",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Session 2: Bring Your Own Data (BYOD)</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-09.html#the-workshop-session",
    "href": "content/sessions/session-09.html#the-workshop-session",
    "title": "37  Session 2: Bring Your Own Data (BYOD)",
    "section": "",
    "text": "37.1.1 Mini progress presentation\n\n\n\n\n\n\nImportant\n\n\n\nThis requires some pre work before the session.\n\n\nWe will visit each and every one of you during the session and hear from you and discuss briefly your ideas on what you want to do for your assessment. We are not expecting you to have made much progress yet but we would also like to give you the chance to get feedback from us on your early ideas. For this brief discussion, prepare to present and discuss:\n\nYour ideas for a topic/question/problem and suitable data sets for your second assessment\nThe objectives of your analysis for the topic/question/problem you identified\nAnd if you have started thinking about it – your methodology (i.e., what sort of analysis you plan to do)\n\n\n\n\n\n\n\nNote\n\n\n\nYou’ll notice that these are roughly the outcomes from P1 - P4 in the process we suggested in the coursework brief.\n\n\nWhen the teaching staff joins your group, be prepared to talk about these questions for 2-3 minutes to your coding group where you will share your screen to talk over some of the notes/slides/material you prepared. We also provided you a Jupyter Notebook template on Moodle so that you can both present your ideas and if you have done already, present your initial steps on the coding side. You can find the JupyterNotebook template on Moodle on this link.\n\n\n37.1.2 Hands-on work on your own dataset(s)\nUse the rest of the session to start working on the datasets you identified to be relevant/interesting for your assessment. If you haven’t decided yet, this will also be a good occasion to “get your hands dirty” with the potential datasets, see if they can prove to be useful in what you plan to do. Staff will be “around” to give you feedback on any early issues you are facing and discuss your data plans.\nAs a team, you can have an open discussion on your project ideas, your datasets, and get feedback and guidance from your peers. Alternatively, you can choose your group to be a “silent” working group where you work silently on your own unless you want to discuss with others – looks like some people are finding it helpful (see an article on The Guardian on silent Zoom groups).",
    "crumbs": [
      "Data Science Workshop",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Session 2: Bring Your Own Data (BYOD)</span>"
    ]
  },
  {
    "objectID": "content/sessions/session-09.html#reading-lists-resources",
    "href": "content/sessions/session-09.html#reading-lists-resources",
    "title": "37  Session 2: Bring Your Own Data (BYOD)",
    "section": "37.2 Reading lists & Resources",
    "text": "37.2 Reading lists & Resources\nThere is not necessarily any further reading we would like to suggest this week due to the hands-on nature of the sessions. But since you are working in teams last and this week, this two articles can provide some good reflections and recent research on working collaboratively within data science teams:\n\nA very interesting read to help you think about a “data science” team and what different roles involve: Crisan, A., Fiore-Gartland, B. and Tory, M., 2020. Passing the Data Baton: A Retrospective Analysis on Data Science Work and Workers. IEEE Transactions on Visualization and Computer Graphics. [paper link – you can login with your institutional account]\nResult of a survey with 183 data scientists. Take a look at Table-2 for the various tools people use. And the Discussion section contains some useful guidance: Zhang, A.X., Muller, M. and Wang, D., 2020. How do data science workers collaborate? roles, workflows, and tools. Proceedings of the ACM on Human-Computer Interaction, 4(CSCW1), pp.1-23. [link to pdf]",
    "crumbs": [
      "Data Science Workshop",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Session 2: Bring Your Own Data (BYOD)</span>"
    ]
  },
  {
    "objectID": "content/files-and-folders.html",
    "href": "content/files-and-folders.html",
    "title": "Appendix B — Working with files and directories",
    "section": "",
    "text": "This is a placeholder\n# SETTING THE WORKING DIRECTORY: if you have not downloaded both the .ipynb file and the Data folder into the same \n# location, then put the path to the folder containing all of your data files (necessary for this lab) inside \n# the os.chdir() function as a STRING\nos.chdir(os.path.join(os.getcwd(), 'data')) \n# READ COMMENT ABOVE or Download the Data folder as well into the same folder as this notebook",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Working with files and directories</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Across Disciplines",
    "section": "",
    "text": "Welcome\nThis is the handbook for IM939: Data Science Across Disciplines, taught at the Centre for Interdisciplinary Methodologies at the University of Warwick and taught1 by Cagatay Turkay (Module leader), Kavin Narasimhan, Carlos Cámara-Menoyo and Busola Oronti.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Data Science Across Disciplines",
    "section": "",
    "text": "1 For a comprehensive list of past and current staff members, refer to Present and former staff\n\n\nIM939 Logo",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#what-this-module-is-about",
    "href": "index.html#what-this-module-is-about",
    "title": "Data Science Across Disciplines",
    "section": "What this module is about?",
    "text": "What this module is about?\nThis module introduces students to the fundamental techniques, concepts and contemporary discussions across the broad field of data science. With data and data related artefacts becoming ubiquitous in all aspects of social life, data science gains access to new sources of data, is taken up across an expanding range of research fields and disciplines, and increasingly engages with societal challenges. The module provides an advanced introduction to the theoretical and scientific frameworks of data science, and to the fundamental techniques for working with data using appropriate procedures, algorithms and visualisation.\nStudents learn how to critically approach data and data-driven artefacts, and engage with and critically reflect on contemporary discussions around the practice of data science, its compatibility with different analytics frameworks and disciplinary, and its relation to on-going digital transformations of society. As well as lectures discussing the theoretical, scientific and ethical frameworks of data science, the module features coding labs and workshops that expose students to the practice of working effectively with data, algorithms, and analytical techniques, as well as providing a platform for reflective and critical discussions on data science practices, resulting data artefacts and how they can be interpreted, actioned and influence society.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#course-contents",
    "href": "index.html#course-contents",
    "title": "Data Science Across Disciplines",
    "section": "Course contents",
    "text": "Course contents\n\nIntroduction and historical perspectives (Chapter 1  Introduction)\nThis week discusses data science as a field that cuts across disciplines and provides a historical perspective on the subject. We discuss the terms Data Science and Data Scientists, reflect on examples of Data Science projects, and discuss the research process at a methodological level.\n\n\nThinking Data: Theoretical and Practical Concerns (Chapter 5  Introduction)\nThis week explores the cultural, ethical, and critical challenges posed by data artefacts and data-intensive scientific processes. Engaging with Critical Data Studies, we discuss issues around data capture, curation, data quality, inclusion/exclusion and representativeness. The session also discusses the different kinds of data that one can encounter across disciplines, the underlying characteristics of data and how we can analytically and practically approach data quality issues and the challenge of identifying and curating appropriate data sets.\n\n\nAbstractions and Models (Chapter 10  Introduction)\nThis week discusses ways of abstracting data. We start by visiting statistics as a means of representing data and its inherent characteristics. The session moves on to discuss the notion of a “model” and visit the different schools of thought within model-ing, as well as a tour of fundamental statistical models that help abstract data and its inherent relations.\n\n\nStructures and Spaces (Chapter 14  Introduction)\nThis week explores the notion of structures and how data science can enable the extraction of “hidden” underlying groups – clusters – and hierarchical structures from data. We discuss the different techniques to surface and generate artificial boundaries and how the resulting artefacts can be interpreted. This session then investigates how artificial and abstract spaces can be constructed through different “projection” techniques, and how these spaces help us navigate data that are high-dimensional in nature and apply analytic frameworks to them.\n\n\nMulti-model Thinking and Rigour in Data Science (Chapter 19  Introduction)\nThis week we focus on multi-model approaches as a way of thinking and how critical, pluralistic thinking can improve our understanding of the underlying phenomena implicit in data. We also discuss how to adopt a comprehensive approach to the data science process, and investigate indicators of rigour in data science.\n\n\nRecognising and Avoiding Traps (Chapter 25  Introduction)\nData analysis and statistical routines and procedures are ingrained with several pitfalls and limitations – these range from methodological pitfalls in the processes and data that once can use, to cognitive and behavioural pitfalls that one can come across in making inferences from data and data artefacts. This week we discuss such theoretical and practical traps and pitfalls, how we can be aware of them and what approaches we have to avoid them.\n\n\nData Science and Society (Chapter 31  Introduction)\nWe will engage with academic and practices discourse on the social, cultural and ethical aspects of data science, and discuss around how one can responsibly carry out data science research on social phenomena, whether data science can be a transformative power in society, and what ethical and social frameworks can help us to critically approach data science practices and its effects on society, and what are ethical practices for data scientists.\n\n\nData Science Workshop 1: Design Thinking in Data Science (Chapter 35  Session 1: Design Thinking in Data Science)\nThis week explores the question “Can we approach data science as a design problem?” and discusses how one can embrace a user-centred approach to design appropriate data science processes. We will do this through hands-on practical where we go through the data science process over an applied case.\n\n\nData Science Workshop 2: Bring Your Own Data (Chapter 37  Session 2: Bring Your Own Data (BYOD))\nThis workshop week will involve you working hands-on towards your final assessments individually but in small coding groups. In the workshop session, we would like to hear from you on your ideas for your second coursework. You will also be able to use this session to start working on the dataset(s) that you have decided (or still considering) to analyse as part of your second assessment and discuss with your peers and with the staff members.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Data Science Across Disciplines",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis handbook has been created by Carlos Cámara-Menoyo and Cagatay Turkay, based on the materials from previous years created by Cagatay Turkay, James Tripp and Zofia Bednarowska-Michaiel.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "Data Science Across Disciplines",
    "section": "Licence",
    "text": "Licence",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "content/about/teaching_materials.html",
    "href": "content/about/teaching_materials.html",
    "title": "Teaching materials",
    "section": "",
    "text": "Moodle\nThis is the primary source of information and communication. As a source of information, Moodle contains all links to every teaching materials that we will be used in this module. Materials are organised by units/weeks and can be of different types:\nAs a communication tool, Moodle provides contains an Announcement board, where teaching staff can post any important updates about the module1, and a Discussion Forum, where anyone can ask (and answer!) questions, expand on some topic or discuss anything related to the module.",
    "crumbs": [
      "About",
      "Teaching materials"
    ]
  },
  {
    "objectID": "content/about/teaching_materials.html#moodle",
    "href": "content/about/teaching_materials.html#moodle",
    "title": "Teaching materials",
    "section": "",
    "text": "Pre-recorded videos\nLabs’ materials\nHandbook units\nReadings\n…\n\n\n1 Every time a new message is created on the announcements’ board, Students will receive a copy in their email inboxes.\n\n\nIM939 Moodle course",
    "crumbs": [
      "About",
      "Teaching materials"
    ]
  },
  {
    "objectID": "content/about/teaching_materials.html#microsoft-teams",
    "href": "content/about/teaching_materials.html#microsoft-teams",
    "title": "Teaching materials",
    "section": "Microsoft Teams",
    "text": "Microsoft Teams\nThis is where the real-time communication takes place. IM939 has a dedicated team channel for students and staff. You can use it to ask questions, make comments, react… or even share materials with the rest of the class on real time. Please note that every student will have access to what it is said or shared, so treat this channel as if it was public, and never share private information.\n\n\n\nIM939’s team on Microsoft Teams",
    "crumbs": [
      "About",
      "Teaching materials"
    ]
  },
  {
    "objectID": "content/about/teaching_materials.html#online-handbook",
    "href": "content/about/teaching_materials.html#online-handbook",
    "title": "Teaching materials",
    "section": "Online Handbook",
    "text": "Online Handbook\nThe online book that you are reading now is the companion handbook for the module2. It contains the contents you’ll need to follow the coding labs and the hands-on parts of the module presented in an read-friendly format and organised around sections (one per week). Each session consists of information about the session, one or more “labs” which are narrated computational artefacts and some exercises that encourage you to tackle specific questions.\n2 To avoid confusions with the Module’s PGT handbook, we will use online handbook to refer to the online book that can be accessed on https://warwickcim.github.io/IM939_handbook/Additionally, every page combines detailed explanations, code blocks showing code and output (see Figure 1), bibliographical references, side notes… as well as clear sign posting and search features to help you find the information you are looking for and to make the learning process, easier.\n\n\n\n\n\n\nCode blocks\n\n\n\n\n\nCode blocks represent what you would need to type in Python’s terminal, script or notebook. You can copy their content and paste it in your IDE to run the code or copy and paste and try to modify it to experiment with it and learn how the code works.\n\n# This is a comment in a code block.\nimport pandas as pd \n\nurl = \"https://raw.githubusercontent.com/WarwickCIM/IM939_handbook/main/content/labs/Lab_1/data/office_ratings.csv\"\n\ndf = pd.read_csv(url)\n\n# Show data frame.\ndf.head()\n\n\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n\n\n\n\n\n\n\nFigure 1: An example of code block and its output. In this case, a dataframe.\n\n\n\n\n\n\n\nYou can use this handbook as a regular book you can read prior to, during or after the teaching sessions to either prepare, follow or refresh key concepts in the coding labs for every unit.\nBut you can also use this handbook as an invitation to experiment: you can copy the code in the examples, pasting it into your notebook and even changing some parameters to see how the output changes.\n\n\n\n\n\n\nWork in progress\n\n\n\nThis first edition of the handbook is a live document. That means that teaching staff will be updating the materials on a weekly basis, making sure that materials for the current week are updated. This means you may want to check for changes on the handbook every week.\n\n\nFrom a technical standpoint, this book has been created with quarto and most of the chapters (i.e., all the labs) are actually the same jupyter notebooks that you will be using during the labs’ sessions. You can see the code in this Github Repository and even try to reproduce the entire book! (see below)",
    "crumbs": [
      "About",
      "Teaching materials"
    ]
  },
  {
    "objectID": "content/about/teaching_materials.html#labs-notebooks",
    "href": "content/about/teaching_materials.html#labs-notebooks",
    "title": "Teaching materials",
    "section": "Labs’ notebooks",
    "text": "Labs’ notebooks\nLabs notebooks are jupyter notebooks that will be using during the weekly face to face workshops3. They contain explanations and code that students will have to work during the session to reproduced, modify and understand the code and concepts for every unit. Strictly speaking, each of those noteooks is exactly the same as every chapter in the book prefixed as “Labs:”.\n3 As you will see in the course, jupyter notebooks are a special type of files with the extension *.ipynb that combine rich-text format\n\n\n\n\n\nSetting up your machine\n\n\n\nTo follow the Labs’ sessions, basides Python, you will need to install the required software and packages that we will be using. Please refer to Appendix A — Setting up your machine for instructions on how to do it.",
    "crumbs": [
      "About",
      "Teaching materials"
    ]
  },
  {
    "objectID": "content/about/teaching_materials.html#github-repository",
    "href": "content/about/teaching_materials.html#github-repository",
    "title": "Teaching materials",
    "section": "GitHub repository",
    "text": "GitHub repository\nThis is where the source code for the online handbook (and therefore, the labs and exercises) reside. Strictly speaking you do not need to do anything with this, but if you are familiar with Git, you may want to get yourself a copy and learn a complementary set of valuable skills by reproducing its contents.\n\n\n\nGitHub Repository containing the handbook /teaching materials’ source code",
    "crumbs": [
      "About",
      "Teaching materials"
    ]
  },
  {
    "objectID": "content/about/conventions.html",
    "href": "content/about/conventions.html",
    "title": "Book Conventions",
    "section": "",
    "text": "Text format\nitalics will be used for these cases:\nSometimes you will see text written in monospace. This refers to something related to code. It can be:",
    "crumbs": [
      "About",
      "Book Conventions"
    ]
  },
  {
    "objectID": "content/about/conventions.html#text-format",
    "href": "content/about/conventions.html#text-format",
    "title": "Book Conventions",
    "section": "",
    "text": "Highlight some technical terms\nBook titles\n\n\n\na function: i.e. help() (functions always end with ())\na file following the pattern script.py, notebook.ipynb or something more generic like *.py (filenames always have a name, a ., and a file extension)\na folder, like folder_name/. Folders always have a trailing / and don’t have a . and an extension.\na package or library, like Python’s pandas\nany character or characters that need to be copied.\n\n\nPlaceholder texts\nSometimes, instead of using an exact name that needs to be copied in your script, we will be using a placeholder that may change in certain circumstances but they will always follow a certain structure. Placeholders are always written between &lt;&gt; like this: pd.read_csv(&lt;filename&gt;.py) this means that when you run your code, you will have to replace the placeholder &lt;filename&gt; with the actual name of your file, which may differ from one person to another. After replacing the placeholder, it could be something like this: pd.read_csv(my_fancy_file.csv)",
    "crumbs": [
      "About",
      "Book Conventions"
    ]
  },
  {
    "objectID": "content/about/conventions.html#sec-code-blocks",
    "href": "content/about/conventions.html#sec-code-blocks",
    "title": "Book Conventions",
    "section": "Code blocks",
    "text": "Code blocks\nCode blocks represent what you would need to type in your programming language’s terminal or script. You can copy their content and paste it in your IDE to run the code or copy and paste and try to modify it to experiment with it and learn how the code works.\n\n\n\n\n\n\nCopying code block’s content\n\n\n\nFor your ease of use, you can copy the whole content of the code block by hovering over them and clicking on the icon on the top right.\n\n\nThe following is a code block:\n\n# This is a comment in a code block.\nimport pandas as pd \n\nurl = \"https://raw.githubusercontent.com/WarwickCIM/IM939_handbook/main/content/labs/Lab_1/data/office_ratings.csv\"\n\ndf = pd.read_csv(url)\n\nSome blocks have the resulting code below, exactly as it would appear if we ran it in a terminal using R or Python1.\n1 In fact, this whole book has been created using literate programming, which combines text and code. The code displayed in these codeblocks has actually been run when creating the book, in exactly the same way it would have happened in a regular script.\n# This is a code block with resulting code.\nprint(\"This is the output resulting from the code above\")\n\nThis is the output resulting from the code above\n\n\nOutput can be a simple line (like above) or something more complex, like this:\n\n# We are displaying the first rows (head) of a dataframe.\ndf.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n\n\n\n\n\n\n\nor even interactive figures!\n\n\n\n\n\n\n\n\n\n\n\nFigure 1",
    "crumbs": [
      "About",
      "Book Conventions"
    ]
  },
  {
    "objectID": "content/about/conventions.html#callouts",
    "href": "content/about/conventions.html#callouts",
    "title": "Book Conventions",
    "section": "Callouts",
    "text": "Callouts\nCallouts are texts that outstand from the regular content for some of these reasons:\n\n\n\n\n\n\nNote\n\n\n\nThis is a note, used for…\n\n\n\n\n\n\n\n\nPro-Tip\n\n\n\nThis is a tip that will make your life easier.\n\n\n\n\n\n\n\n\nThis is Important\n\n\n\nIf you were to remember just some bits of what you’ve been reading, this text should be one of those important bits.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDanger, if you do not pay attention to this, things may not work as expected or not work at all!\n\n\n\n\n\n\n\n\nAction needed\n\n\n\nThis is prompting you to do something, either answering a question or doing some exercise.\n\n\nAdditionally, some callouts can be expanded to see their content. This can be either because:\n\ntheir contents are optional, and they are collapsed by default not to overwhelm you with unnecessary information.\n\n\n\n\n\n\n\nExpand to learn more\n\n\n\n\n\nAlthough strictly speaking, you might not need to learn this, it will make your life easier.\n\n\n\n\nthey give a solution to a question\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nThis is an answer",
    "crumbs": [
      "About",
      "Book Conventions"
    ]
  },
  {
    "objectID": "content/about/conventions.html#bibliographical-references",
    "href": "content/about/conventions.html#bibliographical-references",
    "title": "Book Conventions",
    "section": "Bibliographical references",
    "text": "Bibliographical references\nSometimes you’ll see texts with bliographical references such as: “See Knuth (1984) for additional discussion of literate programming.”\nYou can click over the reference to see the full details or visit chapter References\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "About",
      "Book Conventions"
    ]
  },
  {
    "objectID": "content/labs/Lab_5/week5_recap.html",
    "href": "content/labs/Lab_5/week5_recap.html",
    "title": "20  Recap",
    "section": "",
    "text": "20.1 Workflow\nOur labs have focused on data analysis. The goal here is to try and understand informative patterns in our data. These patterns allow us to answer questions.\nTo do this we:\nThese are not mutually exclusive processes and are not exhaustive. One may review our data after cleaning, load in more data, carry out additional analysis and/or fit multiple models, tweak data summaries or adopt new techniques. Reflecting on the patterns are in our data can give way to additional analysis and processing.",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Recap</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_5/week5_recap.html#workflow",
    "href": "content/labs/Lab_5/week5_recap.html#workflow",
    "title": "20  Recap",
    "section": "",
    "text": "Read data into Python.\nLook at our data.\nWrangling our data. Often exploring raw data and dealing with missing values (imputation techniques), transformaing, normalising, standardising, outliers or reshaping.\nCarry out a series of analysis to better understand our data via clustering, regressions analysis, dimension reduction, and many other techniques.\nReflect on what the patterns in our data can tell us.",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Recap</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_5/week5_recap.html#so-far",
    "href": "content/labs/Lab_5/week5_recap.html#so-far",
    "title": "20  Recap",
    "section": "20.2 So far",
    "text": "20.2 So far\nA quick reminder, our toolkit comprises of:\n\nPandas - table like data structures. Packed with methods for summarising and manipulating data. Documentation. Cheat sheet.\nSeaborn - helping us create statistical data visualisations. Tutorials.\nScikit-learn - An accessible collection of functions and object for analysing data. These analysis include dimension reduction, clustering, regressions and evaluating our models. Examples.\n\nThese tools comprise some of the core Python data science stack and allow us to tackle many of the elements from each week.\n\n20.2.1 Week 2\nTidy data, data types, wrangling data, imputation (missing data), transformations.\n\n\n20.2.2 Week 3\nDescriptive statistics, distributions, models (e.g., regression).\n\n\n20.2.3 Week 4\nFeature selection, dimension reduction (e.g., Principle Component Analysis, Multidimensional scaling, Linear Discriminant Analysis, t-SNE, Correspondance Analysis), clustering (e.g., Hierarchical Clustering, Partioning-based clustering such as K-means).\nWe have also encountered two dataset sources.\n\nsklearn example datasets\nUCI Machine Learning Repository\n\nYou can learn a lot by picking a dataset, choosing a possible research question and carrying a series of analysis. I encourage you to do so outside of the session. It certainly forces one to read the documentation and explore the wonderful possabilities.",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Recap</span>"
    ]
  },
  {
    "objectID": "content/labs/Lab_5/week5_recap.html#this-week",
    "href": "content/labs/Lab_5/week5_recap.html#this-week",
    "title": "20  Recap",
    "section": "20.3 This week",
    "text": "20.3 This week\nTrying to understand patterns in data often requires us to fit multiple models. We need to consider how well a given model (a kmeans cluster, a linear regression, dimension reduction, etc.) performs.\nSpecifically, we will look at:\n\nComparing clusters to the ‘ground truth’ - the wine dataset\nCross validation of linear regression - the crime dataset\nInvestigating multidimensional scaling - the london borough dataset\nVisualising the overlap in clustering results",
    "crumbs": [
      "Multi-model thinking and rigour",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Recap</span>"
    ]
  },
  {
    "objectID": "content/references.html",
    "href": "content/references.html",
    "title": "References",
    "section": "",
    "text": "Calero Valdez, André, Martina Ziefle, and Michael Sedlmair. 2018.\n“Studying Biases in Visualization\nResearch: Framework and\nMethods.” In Cognitive Biases in\nVisualizations, edited by Geoffrey Ellis, 13–27. Cham:\nSpringer International Publishing. https://doi.org/10.1007/978-3-319-95831-6_2.\n\n\nCortez, Paulo, A Cerdeira, F Almeida, T Matos, and J. Reis. 2009.\n“Wine Quality.” UCI Machine Learning\nRepository. https://doi.org/10.24432/C56S3T.\n\n\nFisher, R. A. 1936. “Iris.” UCI Machine Learning\nRepository. https://doi.org/10.24432/C56C76.\n\n\nIgual, Laura, and Santi Seguí. 2017. “Regression\nAnalysis.” In Introduction to Data\nScience: A Python\nApproach to Concepts, Techniques\nand Applications, 97–114. Cham: Springer International\nPublishing. https://doi.org/10.1007/978-3-319-50017-1_6.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nPage, Scott E. 2018. “Why ‘Many-Model\nThinkers’ Make Better Decisions.”\nHarvard Business Review, November. https://hbr.org/2018/11/why-many-model-thinkers-make-better-decisions.\n\n\nRedmond, Michael. 2009. “Communities and\nCrime.” UCI Machine Learning Repository. https://doi.org/10.24432/C53W3X.\n\n\nRouncefield, Mary. 1995. “The Statistics of\nPoverty and Inequality.” Journal of\nStatistics Education 3 (2): 8. https://doi.org/10.1080/10691898.1995.11910491.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "content/labs/labs_setup.html",
    "href": "content/labs/labs_setup.html",
    "title": "Appendix A — Setting up your machine",
    "section": "",
    "text": "A.1 Install Anaconda\nYou will need to install Anaconda distribution your machine if it is not already installed. You can run the following command to see if it is already present in your system:\nIf you get a command not found error, you will need to install Anaconda. You can do so by:\nOnce you’ve done it, Anaconda, Python, some popular libraries and a software called Anaconda Navigator will be installed in your system. We will be using Anaconda Navigator as an interface (GUI) to Anaconda.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Setting up your machine</span>"
    ]
  },
  {
    "objectID": "content/labs/labs_setup.html#install-anaconda",
    "href": "content/labs/labs_setup.html#install-anaconda",
    "title": "Appendix A — Setting up your machine",
    "section": "",
    "text": "Terminal\n\nconda --help\n\n\n\nDownload it from the official website: https://www.anaconda.com/download\nExecute the binary you downloaded in previous steps and follow instructions\n\n\n\n\n\n\n\nFigure A.1: Download the right version for your system from Anaconda’s website",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Setting up your machine</span>"
    ]
  },
  {
    "objectID": "content/labs/labs_setup.html#create-a-virtual-environment",
    "href": "content/labs/labs_setup.html#create-a-virtual-environment",
    "title": "Appendix A — Setting up your machine",
    "section": "A.2 Create a virtual environment",
    "text": "A.2 Create a virtual environment\n\n\n\n\n\n\nImportant\n\n\n\nYou will only need to do this step once per machine. After the virtual environment is created, you will not need to create it again, unless you delete it or you want to use it in a different machine.\n\n\n\nDownload the file environment.yml in your laptop\n\nRight click on environment.yml and select “Save Link As…”\nCreate a folder dedicated to IM939 (where you will be adding your notebooks and exercises) and save it in there without changing its name.\n\nOpen Anaconda Navigator\n\nClick on Environments’ tab\nClick on Import\n\n\n\n\n\n\n\n\nFigure A.2: Import environment from Anaconda Navigator\n\n\n\n\nSelect the file environment.yml you downloaded on the previous step.\nWrite IM939 as the environment’s name\n\n\n\n\n\n\n\nFigure A.3: Select the file\n\n\n\n\nAfter downloading the required packages and installing them (this can take time), an environment called IM939 will be created\n\n\n\n\n\n\n\nFigure A.4: Anaconda Navigator displaying the newly created environment as the active environment\n\n\n\n\n\n\n\n\n\nCommand Line alternative\n\n\n\nInstead of using anaconda navigator, you can also do the same from the command line.\nOpen your terminal and type the following command:\n\n\nTerminal\n\nconda env create -f environment.yml",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Setting up your machine</span>"
    ]
  },
  {
    "objectID": "content/labs/labs_setup.html#activate-the-virtual-environment",
    "href": "content/labs/labs_setup.html#activate-the-virtual-environment",
    "title": "Appendix A — Setting up your machine",
    "section": "A.3 Activate the virtual environment",
    "text": "A.3 Activate the virtual environment\n\n\n\n\n\n\nImportant\n\n\n\nYou will need to do this every time you restart your session. Prior to every lab session, you will need to make sure that the IM939 environment has been activated.\n\n\nTo activate an existing environment you need to:\n\nOpen Anaconda Navigator\nClick on Environments’ tab\nClick on IM939 to activate it. After a brief lapse of time, you will see a green border and arrow next to it, indicating that the environment has been activated.\n\n\n\n\n\n\n\nFigure A.5: Anaconda Navigator showing IM939 as the active environment, and a listing of every library and versions installed.\n\n\n\n\n\n\n\n\n\nCommand Line alternative\n\n\n\nInstead of using anaconda navigator, you can also do the same from the command line.\nOpen your terminal and type the following command:\n\n\nTerminal\n\nconda activate IM939\n\nYou will see that the environment is active because you will see its name in the command line.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you do not activate the environment, chances are that the code will not work due to missing libraries or using different versions of some libraries. Image below shows how the same command (import altar as alt) yields an error on Figure A.6 (a) because the environment has not been activated, whereas Figure A.6 (b) shows that the same code just works after activating the environment using the command conda activate IM939.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Error caused by not loading the environment\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) IM939 activated does not trigger any error\n\n\n\n\n\n\n\nFigure A.6: Running the same command yields different results depending on whether the environment has not been activated (A) or it has been activated (B).\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDid you notice that the terminal on Figure A.6 (a) shows (base) next to the command prompt, whereas Figure A.6 (b) shows (IM939) instead after having activated the environment?\nYou’ve guessed right: this is how the terminal will tell us which is our current environment.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Setting up your machine</span>"
    ]
  },
  {
    "objectID": "content/labs/labs_setup.html#working-as-usual",
    "href": "content/labs/labs_setup.html#working-as-usual",
    "title": "Appendix A — Setting up your machine",
    "section": "A.4 Working as usual",
    "text": "A.4 Working as usual\nNow that the environment has been created and activated, you can use your jupyter notebooks using your favourite IDE, like you would do normally. For your convenience, the environment includes Jupyter Lab (and some extensions).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Setting up your machine</span>"
    ]
  }
]